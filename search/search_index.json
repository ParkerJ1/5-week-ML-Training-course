{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"5-Week Machine Learning &amp; Deep Learning Training Program","text":"<p>Jonathan Parker Discipline of Electrical, Electronic and Computer Engineering University of KwaZulu-Natal Last Updated: November 2025 </p>"},{"location":"#start-here-welcome-video","title":"\ud83c\udfa5 START HERE: Welcome Video","text":"<p>Before diving into the program, please watch this welcome video:</p> <p>\u25b6\ufe0f Watch Welcome &amp; Program Overview (15 min)</p> <p>This video covers: - Program structure and daily expectations - How to navigate the lesson plans - Tips for success over the 5 weeks - Communication and check-in schedule</p>"},{"location":"#program-overview","title":"Program Overview","text":"<p>This comprehensive training program covers the fundamentals of machine learning and deep learning, from Python basics through advanced neural network architectures, complete project development, and optional production deployment.</p> <p>Core Duration: 5 weeks | Daily Commitment: 8 hours | Total Hours: 200</p>"},{"location":"#week-1-python-ml-fundamentals","title":"Week 1: Python &amp; ML Fundamentals","text":"<p>\ud83d\udccb Week 1 Overview</p> <ul> <li>Day 1: NumPy &amp; Arrays</li> <li>Day 2: Matplotlib &amp; Data Visualization</li> <li>Day 3: Introduction to Machine Learning</li> <li>Day 4: Classification and Logistic Regression</li> <li>Day 5: Scikit-learn &amp; Titanic Project</li> </ul> <p>Key Project: Titanic Survival Prediction (&gt;75% accuracy)</p>"},{"location":"#week-2-neural-networks-foundations","title":"Week 2: Neural Networks Foundations","text":"<p>\ud83d\udccb Week 2 Overview</p> <ul> <li>Day 6: Neural Network Theory</li> <li>Day 7: Backpropagation and Training</li> <li>Day 8: Introduction to PyTorch</li> <li>Day 9: Building Neural Networks in PyTorch</li> <li>Day 10: MNIST Digit Classification Project</li> </ul> <p>Key Project: MNIST Digit Classification (&gt;95% accuracy)</p>"},{"location":"#week-3-deep-learning-cnns","title":"Week 3: Deep Learning &amp; CNNs","text":"<p>\ud83d\udccb Week 3 Overview</p> <ul> <li>Day 11: CNN Theory &amp; Convolutions</li> <li>Day 12: LeNet &amp; AlexNet Architectures</li> <li>Day 13: Modern Architectures (ResNet, VGG)</li> <li>Day 14: Transfer Learning &amp; Data Augmentation</li> <li>Day 15: CIFAR-10 Classification Project</li> </ul> <p>Key Project: CIFAR-10 Image Classification (&gt;85% accuracy)</p>"},{"location":"#week-4-advanced-architectures","title":"Week 4: Advanced Architectures","text":"<p>\ud83d\udccb Week 4 Overview</p> <ul> <li>Day 16: RNNs and LSTMs</li> <li>Day 17: Attention Mechanisms and Transformers</li> <li>Day 18: NLP Fundamentals</li> <li>Day 19: Generative Adversarial Networks</li> <li>Day 20: Sentiment Analysis Project</li> </ul> <p>Key Project: Movie Review Sentiment Analysis (&gt;85% accuracy)</p>"},{"location":"#week-5-capstone-project-development","title":"Week 5: Capstone Project Development","text":"<p>\ud83d\udccb Week 5 Overview</p> <ul> <li>Day 21: Project Planning &amp; Setup</li> <li>Day 22: Data Pipeline &amp; Baseline Model</li> <li>Day 23: Model Development &amp; Iteration</li> <li>Day 24: Testing &amp; Refinement</li> <li>Day 25: Documentation &amp; Presentation</li> </ul> <p>Key Project: Independent Capstone Project (Choose from 3 tracks)</p>"},{"location":"#project-tracks","title":"Project Tracks:","text":"<ol> <li>Medical Image Classification - Pneumonia detection from chest X-rays</li> <li>NLP Sentiment Analysis - IMDB movie review classification</li> <li>Time Series Forecasting - Stock price movement prediction</li> </ol> <p>Deliverable: Portfolio-ready project with professional documentation</p>"},{"location":"#week-6-production-machine-learning-optional-coming-soon","title":"Week 6: Production Machine Learning (OPTIONAL) (COMING SOON)","text":"<p>\ud83d\udccb Week 6 Overview</p> <ul> <li>Day 26: Model Optimization - Quantization, Pruning, ONNX</li> <li>Day 27: Model Deployment - FastAPI, Docker, Cloud</li> <li>Day 28: MLOps Fundamentals - Tracking, Versioning, Pipelines</li> <li>Day 29: Best Practices - Testing, Documentation, Code Quality</li> <li>Day 30: Production Project - Deploy Your Week 5 Project</li> </ul> <p>Key Project: Production-ready ML API with monitoring</p> <p>Deliverable: Deployed ML system with API, Docker, and monitoring</p>"},{"location":"#program-structure-summary","title":"Program Structure Summary","text":""},{"location":"#core-program-weeks-1-5-required","title":"Core Program (Weeks 1-5) - REQUIRED","text":"<ul> <li>Week 1: Python &amp; ML Fundamentals</li> <li>Week 2: Neural Networks</li> <li>Week 3: Computer Vision &amp; CNNs</li> <li>Week 4: Advanced Architectures (RNN, Transformer, GAN)</li> <li>Week 5: Independent Capstone Project</li> </ul>"},{"location":"#optional-extension-week-6","title":"Optional Extension (Week 6)","text":"<ul> <li>Week 6: Production ML Deployment</li> </ul>"},{"location":"#how-to-use-these-materials","title":"How to Use These Materials","text":""},{"location":"#daily-workflow","title":"Daily Workflow","text":"<ol> <li>Read the Overview - Start each week by reading the weekly overview</li> <li>Follow the Daily Plan - Each day has a structured 8-hour schedule</li> <li>Watch Videos - All video links are embedded with durations</li> <li>Complete Exercises - Hands-on coding is essential for learning</li> <li>Daily Reflection - Document your learning and questions</li> <li>Check-ins - Monday, Wednesday, Friday with instructor (Weeks 1-4)</li> </ol>"},{"location":"#coding-vs-copying","title":"Coding vs Copying","text":"<p>There is a lot of coding in these exercises. There is value in retyping code yourself - it will help solidify the concepts being impelmented. Once you feel comfortable with a certain part of the code, feel free to copy it instead of retyping it.  </p> <p>Focus on the code that implements: - the datasets and dataloaders,  - the models - the training and testing loops  </p> <p>Copy the code that implements: - visualisation - in-code comparisons designed to produce commentary once executed</p> <p>CAREFUL: Some exercise require the code from previous exercises in the day</p>"},{"location":"#genai-assistance","title":"GenAI assistance","text":"<p>Feel free to use a generative AI platform (e.g. ChatGPT) as a tutor - ie paste a code snippet and ask it to explain what it does.</p>"},{"location":"#resources-needed","title":"Resources Needed","text":"<p>Core Program: - Google Colab account (free) - Stable internet connection - 8 hours per day for focused learning - Microsoft Teams for communication - Notebook for reflections</p> <p>Week 6 (Optional): - Docker installed locally - Basic command line familiarity - Cloud account (AWS/GCP/Azure) for deployment (optional)</p>"},{"location":"#assessment","title":"Assessment","text":"<p>Completion of exercises and projects with bona fide effort receives full credit for vacation work.</p> <p>Week 5 Capstone: Must demonstrate working project with: - Trained model (&gt;80% accuracy target) - Professional documentation - Results visualization - Presentation readiness</p>"},{"location":"#key-learning-outcomes","title":"Key Learning Outcomes","text":""},{"location":"#by-end-of-week-5-core-program","title":"By End of Week 5 (Core Program):","text":"<p>\u2705 Master Python libraries (NumPy, Matplotlib, PyTorch) \u2705 Implement ML algorithms from scratch \u2705 Build and train neural networks \u2705 Apply CNNs to image classification \u2705 Understand modern architectures (ResNet, RNN, Transformer, GAN) \u2705 Complete end-to-end ML projects independently \u2705 Document work professionally \u2705 Present technical projects effectively  </p>"},{"location":"#by-end-of-week-6-optional","title":"By End of Week 6 (Optional):","text":"<p>\u2705 Optimize models for production (4x size reduction) \u2705 Deploy models as REST APIs \u2705 Containerize applications with Docker \u2705 Implement monitoring and logging \u2705 Apply MLOps best practices \u2705 Write production-quality code with tests  </p>"},{"location":"#support-communication","title":"Support &amp; Communication","text":"<ul> <li>Microsoft Teams: Group for questions and peer support</li> <li>Check-ins: Monday, Wednesday, Friday </li> <li>Email: For individual concerns</li> <li>Peer Learning: Collaborate and learn from classmates</li> </ul>"},{"location":"#portfolio-projects","title":"Portfolio Projects","text":"<p>By the end of the program, you will have completed:</p> <ol> <li>Titanic Survival Prediction (Week 1) - Binary classification</li> <li>MNIST Digit Classification (Week 2) - Neural networks from scratch</li> <li>CIFAR-10 Image Classification (Week 3) - CNNs and transfer learning</li> <li>Sentiment Analysis (Week 4) - RNNs/LSTMs for NLP</li> <li>Independent Capstone (Week 5) - Complete ML system</li> <li>Production API (Week 6, Optional) - Deployed ML system</li> </ol> <p>All projects are portfolio-ready with professional documentation!</p>"},{"location":"#ready-to-begin","title":"Ready to Begin?","text":""},{"location":"#start-here","title":"Start Here:","text":"<ol> <li>Read Week 1 Overview</li> <li>Begin with Day 1: NumPy &amp; Arrays</li> <li>Join Microsoft Teams group</li> <li>Set up Google Colab account</li> </ol> <p>Good luck with your machine learning journey!</p> <p>Last Updated: November 2025 Jonathan Parker Discipline of Electrical, Electronic and Computer Engineering University of KwaZulu-Natal </p>"},{"location":"Week1_Day1/","title":"Week 1, Day 1: Python Crash Course - NumPy &amp; Arrays","text":""},{"location":"Week1_Day1/#daily-goals","title":"Daily Goals","text":"<ul> <li>Set up Python environment (Anaconda/Colab)</li> <li>Master NumPy array operations</li> <li>Understand vectorization benefits</li> </ul>"},{"location":"Week1_Day1/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week1_Day1/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week1_Day1/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: NumPy Tutorial for Beginners by freeCodeCamp (1 hour)</p> <p>\u2610 Watch: Python NumPy Tutorial for Beginners by Keith Galli - focus on 0:00-15:00 for basics and 30:00-45:00 for array operations (30 min)</p>"},{"location":"Week1_Day1/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: NumPy Quickstart Tutorial - Sections on \"The Basics\" and \"Array Creation\"</p> <p>\u2610 Bookmark: NumPy Reference Documentation for afternoon use</p>"},{"location":"Week1_Day1/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week1_Day1/#setup-15-min","title":"Setup (15 min)","text":"<p>\u2610 Create a Google Colab notebook titled \"Day1_NumPy_Practice\"</p> <p>\u2610 Import NumPy: <code>import numpy as np</code></p> <p>\u2610 Test installation: <code>print(np.__version__)</code></p>"},{"location":"Week1_Day1/#exercise-1-array-creation-30-min","title":"Exercise 1: Array Creation (30 min)","text":"<p>Create the following arrays and print their shape, dtype, and contents:</p> <ol> <li> <p>A 1D array with integers from 0 to 9</p> </li> <li> <p>Hint: Use <code>np.arange()</code></p> </li> <li> <p>A 2D array (3x3) filled with zeros</p> </li> <li> <p>Hint: Use <code>np.zeros()</code></p> </li> <li> <p>A 3x3 identity matrix</p> </li> <li> <p>Hint: Use <code>np.eye()</code></p> </li> <li> <p>A 1D array with 10 evenly spaced values between 0 and 1</p> </li> <li> <p>Hint: Use <code>np.linspace()</code></p> </li> <li> <p>A 2D array (5x5) with random values between 0 and 1</p> </li> <li> <p>Hint: Use <code>np.random.rand()</code></p> </li> <li> <p>A 2D array (3x4) with sequential values from 0 to 11</p> </li> <li> <p>Hint: Use <code>np.arange()</code> then <code>.reshape()</code></p> </li> </ol>"},{"location":"Week1_Day1/#exercise-2-array-indexing-and-slicing-40-min","title":"Exercise 2: Array Indexing and Slicing (40 min)","text":"<p>Given this array:</p> <pre><code>arr = np.arange(0, 100).reshape(10, 10)\n</code></pre> <p>Complete these tasks:</p> <ol> <li> <p>Extract the element at row 3, column 5</p> </li> <li> <p>Expected output: 35</p> </li> <li> <p>Extract the entire 5th row - remember Python starts at index 0</p> </li> <li> <p>Expected output: array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])</p> </li> <li> <p>Extract the top-left 3x3 subarray</p> </li> <li> <p>Hint: Use slicing <code>arr[start:end, start:end]</code></p> </li> <li> <p>Extract every other row</p> </li> <li> <p>Hint: Use step in slicing <code>arr[::2]</code></p> </li> <li> <p>Extract the bottom-right 2x2 subarray</p> </li> <li> <p>Hint: Use negative indices</p> </li> <li> <p>Extract all elements greater than 50 (returns 1D array)</p> </li> <li> <p>Hint: Use boolean indexing <code>arr[arr &gt; 50]</code></p> </li> </ol>"},{"location":"Week1_Day1/#exercise-3-basic-array-operations-35-min","title":"Exercise 3: Basic Array Operations (35 min)","text":"<p>Create two arrays:</p> <pre><code>a = np.array([1, 2, 3, 4, 5])\nb = np.array([10, 20, 30, 40, 50])\n</code></pre> <p>Perform and print results:</p> <ol> <li> <p>Element-wise addition (a + b)</p> </li> <li> <p>Expected output: [11, 22, 33, 44, 55]</p> </li> <li> <p>Element-wise multiplication (a * b)</p> </li> <li> <p>Expected output: [10, 40, 90, 160, 250]</p> </li> <li> <p>Square each element of array a</p> </li> <li> <p>Hint: Use <code>a ** 2</code></p> </li> <li> <p>Calculate the dot product of a and b</p> </li> <li> <p>Hint: Use <code>np.dot()</code> or <code>@</code> operator</p> </li> <li> <p>Expected output: 550</p> </li> <li> <p>Find the sum, mean, and standard deviation of array a</p> </li> <li> <p>Hint: Use <code>np.sum()</code>, <code>np.mean()</code>, <code>np.std()</code></p> </li> <li> <p>Find the maximum value in b and its index</p> </li> <li> <p>Hint: Use <code>np.max()</code> and <code>np.argmax()</code></p> </li> </ol>"},{"location":"Week1_Day1/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week1_Day1/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: NumPy Broadcasting by Algorithmic Simplicity (15 min)</p> <p>\u2610 Watch: Vectorization in Python first half (15 min)</p>"},{"location":"Week1_Day1/#hands-on-coding-part-2-3-hours","title":"Hands-on Coding - Part 2 (3 hours)","text":""},{"location":"Week1_Day1/#exercise-4-statistics-without-loops-50-min","title":"Exercise 4: Statistics Without Loops (50 min)","text":"<p>Implement these functions using only NumPy operations (no Python loops):</p> <p>1. normalize_array(arr): Takes a 1D array and returns it normalized to have mean=0 and std=1</p> <pre><code>def normalize_array(arr):\n    # Your code here\n    pass\n\n# Test\narr = np.array([1, 2, 3, 4, 5])\nresult = normalize_array(arr)\nprint(f\"Mean: {result.mean():.10f}, Std: {result.std():.10f}\")\n# Expected: Mean \u2248 0, Std \u2248 1\n</code></pre> <p>Hint: normalized = (arr - mean) / std</p> <p>2. moving_average(arr, window_size): Calculate moving average with given window size</p> <pre><code>def moving_average(arr, window_size):\n    # Your code here\n    pass\n\n# Test\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nresult = moving_average(arr, 3)\nprint(result)\n# Expected output: [2., 3., 4., 5., 6., 7., 8., 9.]\n</code></pre> <p>Hint: Use <code>np.convolve()</code> with <code>mode='valid'</code></p> <p>3. find_outliers(arr, threshold): Return indices where values deviate from the mean by more than threshold * std</p> <pre><code>def find_outliers(arr, threshold):\n    # Your code here\n    pass\n\n# Test\narr = np.array([1, 2, 3, 4, 5, 100, 6, 7, 8])\noutliers = find_outliers(arr, 2)\nprint(f\"Outlier indices: {outliers}\")\nprint(f\"Outlier values: {arr[outliers]}\")\n# Expected: Index 5 (value 100) is an outlier\n</code></pre> <p>Hint: Calculate z-scores: <code>(arr - mean) / std</code>, then find where <code>abs(z_scores) &gt; threshold</code></p>"},{"location":"Week1_Day1/#exercise-5-broadcasting-practice-40-min","title":"Exercise 5: Broadcasting Practice (40 min)","text":"<p>Solve these without loops:</p> <ol> <li>Add a 1D array <code>[1, 2, 3]</code> to each row of a 4x3 matrix</li> </ol> <p><code>python    matrix = np.array([[1, 1, 1],                    [2, 2, 2],                    [3, 3, 3],                    [4, 4, 4]])    to_add = np.array([1, 2, 3])    # Your code here    # Expected output:    # [[2, 3, 4],    #  [3, 4, 5],    #  [4, 5, 6],    #  [5, 6, 7]]</code></p> <p>Hint: Broadcasting works automatically with <code>matrix + to_add</code></p> <ol> <li>Create a 10x10 multiplication table using broadcasting</li> </ol> <p><code>python    # Your code here    # Expected: Element (i,j) should be i*j    # Row 5 should be [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]</code></p> <p>Hint: Create two arrays with shapes (10, 1) and (1, 10), then multiply</p> <ol> <li>Normalize each column of a matrix independently to range [0, 1]</li> </ol> <p><code>python    matrix = np.random.rand(5, 3) * 100    # Your code here: normalize each column so min=0, max=1    # Verify: Check that each column's min is 0 and max is 1</code></p> <p>Hint: For each column: <code>(col - col.min()) / (col.max() - col.min())</code> Use broadcasting with <code>axis=0</code> in min/max functions</p>"},{"location":"Week1_Day1/#mini-challenge-image-manipulation-90-min","title":"Mini-Challenge: Image Manipulation (90 min)","text":"<p>You'll work with a grayscale image represented as a 2D NumPy array. First, create a test image:</p> <pre><code># Create a simple test pattern\nimage = np.zeros((100, 100))\nimage[25:75, 25:75] = 1.0  # White square in center\nimage[40:60, 40:60] = 0.5  # Gray square in middle\n\n# Visualize (you'll need this throughout)\nimport matplotlib.pyplot as plt\nplt.imshow(image, cmap='gray')\nplt.colorbar()\nplt.show()\n</code></pre> <p>Implement these image transformations:</p> <p>1. rotate_90(image): Rotate image 90 degrees clockwise</p> <pre><code>def rotate_90(image):\n    # Your code here\n    pass\n\nresult = rotate_90(image)\nplt.imshow(result, cmap='gray')\nplt.title('Rotated 90\u00b0 clockwise')\nplt.show()\n</code></pre> <p>Hint: Use <code>np.rot90()</code> with appropriate k value</p> <p>2. flip_horizontal(image): Flip image horizontally (mirror left-right)</p> <pre><code>def flip_horizontal(image):\n    # Your code here\n    pass\n\nresult = flip_horizontal(image)\nplt.imshow(result, cmap='gray')\nplt.title('Flipped Horizontally')\nplt.show()\n</code></pre> <p>Hint: Use <code>np.fliplr()</code> or slicing with <code>::-1</code></p> <p>3. crop_center(image, size): Crop a square of given size from the center</p> <pre><code>def crop_center(image, size):\n    # Your code here\n    # Example: if image is 100x100 and size=50, extract the central 50x50 region\n    pass\n\nresult = crop_center(image, 50)\nprint(f\"Cropped shape: {result.shape}\")  # Should be (50, 50)\nplt.imshow(result, cmap='gray')\nplt.title('Center Crop (50x50)')\nplt.show()\n</code></pre> <p>Hint: Calculate center indices, then slice appropriately</p> <p>4. adjust_brightness(image, factor): Multiply all pixels by factor and clip to [0, 1]</p> <pre><code>def adjust_brightness(image, factor):\n    # Your code here\n    pass\n\n# Test with darkening (factor=0.5) and brightening (factor=1.5)\ndark = adjust_brightness(image, 0.5)\nbright = adjust_brightness(image, 1.5)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Original')\naxes[1].imshow(dark, cmap='gray')\naxes[1].set_title('Darkened (0.5x)')\naxes[2].imshow(bright, cmap='gray')\naxes[2].set_title('Brightened (1.5x)')\nplt.show()\n</code></pre> <p>Hint: Use <code>np.clip(image * factor, 0, 1)</code></p> <p>5. apply_threshold(image, threshold): Convert to binary (0 or 1) based on threshold</p> <pre><code>def apply_threshold(image, threshold):\n    # Your code here\n    # Pixels &gt;= threshold become 1, others become 0\n    pass\n\nresult = apply_threshold(image, 0.5)\nplt.imshow(result, cmap='gray')\nplt.title('Thresholded at 0.5')\nplt.show()\n</code></pre> <p>Hint: Use boolean comparison and <code>.astype(float)</code></p>"},{"location":"Week1_Day1/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review all code you wrote today</p> <p>\u2610 Clean up your notebook with comments and markdown cells</p> <p>\u2610 Write your daily reflection in a separate document (choose 2-3 prompts below)</p> <p>\u2610 List any questions for the Monday check-in email</p>"},{"location":"Week1_Day1/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>What challenged you the most? How did you approach it?</li> <li>What connections did you make between today's content and previous learning?</li> <li>What questions do you still have?</li> <li>How does today's learning relate to real-world applications?</li> <li>What would you do differently if you repeated today?</li> </ul> <p>Next: Day 2 - Matplotlib &amp; Data Visualization</p>"},{"location":"Week1_Day2/","title":"Week 1, Day 2: Python Crash Course - Matplotlib &amp; Data Visualization","text":""},{"location":"Week1_Day2/#daily-goals","title":"Daily Goals","text":"<ul> <li>Master basic plotting with Matplotlib</li> <li>Create various plot types (line, scatter, histogram, heatmap)</li> <li>Understand when to use different visualizations</li> </ul>"},{"location":"Week1_Day2/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week1_Day2/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week1_Day2/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Matplotlib Tutorial for Beginners by Corey Schafer (1 hour)</p> <p>\u2610 Watch: Matplotlib Crash Course by Tech With Tim - focus on 0:00-30:00 (30 min)</p>"},{"location":"Week1_Day2/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: Matplotlib Quick Start Guide</p> <p>\u2610 Bookmark: Matplotlib Gallery for examples</p>"},{"location":"Week1_Day2/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week1_Day2/#setup-10-min","title":"Setup (10 min)","text":"<p>\u2610 Create a new Colab notebook titled \"Day2_Matplotlib_Practice\"</p> <p>\u2610 Import libraries:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Week1_Day2/#exercise-1-basic-line-plots-40-min","title":"Exercise 1: Basic Line Plots (40 min)","text":"<p>1. Simple line plot: Plot y = x\u00b2 for x values from -10 to 10</p> <pre><code>x = np.linspace(-10, 10, 100)\n# Your code here\n</code></pre> <p>Hint: Use <code>plt.plot()</code> then <code>plt.show()</code> Expected: A parabola centered at origin</p> <p>2. Multiple lines: Plot sin(x), cos(x), and tan(x) for x from 0 to 2\u03c0 on the same graph</p> <pre><code>x = np.linspace(0, 2*np.pi, 100)\n# Your code here\n# Add labels, legend, title, and grid\n</code></pre> <p>Hint: Call <code>plt.plot()</code> multiple times before <code>plt.show()</code> Use <code>plt.legend()</code>, <code>plt.xlabel()</code>, <code>plt.ylabel()</code>, <code>plt.title()</code>, <code>plt.grid()</code></p> <p>3. Customized plot: Create a line plot with:</p> <ul> <li>Custom colors (use 'red', 'blue', etc.)</li> <li>Different line styles (solid, dashed, dotted)</li> <li>Markers at data points</li> <li>Custom line widths</li> </ul> <pre><code>x = np.linspace(0, 10, 20)\ny1 = x\ny2 = x**2\n# Your code here\n</code></pre> <p>Hint: Use parameters like <code>color='red'</code>, <code>linestyle='--'</code>, <code>marker='o'</code>, <code>linewidth=2</code></p>"},{"location":"Week1_Day2/#exercise-2-scatter-plots-35-min","title":"Exercise 2: Scatter Plots (35 min)","text":"<p>1. Basic scatter plot: Create random data and visualize</p> <pre><code>np.random.seed(42)\nx = np.random.randn(100)\ny = 2*x + np.random.randn(100)*0.5\n# Create scatter plot\n# Add title and labels\n</code></pre> <p>Hint: Use <code>plt.scatter()</code> Expected: Points roughly following a line with some noise</p> <p>2. Colored scatter plot: Create a scatter plot where point colors represent a third variable</p> <pre><code>x = np.random.rand(50)\ny = np.random.rand(50)\ncolors = x + y  # Color based on sum of coordinates\nsizes = (x * 100) ** 2  # Size based on x value\n# Create scatter plot with colors and varying sizes\n# Add a colorbar\n</code></pre> <p>Hint: Use <code>c=colors</code>, <code>s=sizes</code> parameters in <code>plt.scatter()</code>, then <code>plt.colorbar()</code></p>"},{"location":"Week1_Day2/#exercise-3-subplots-35-min","title":"Exercise 3: Subplots (35 min)","text":"<p>Create a 2x2 grid of subplots showing different mathematical functions:</p> <pre><code>x = np.linspace(-5, 5, 100)\n\n# Create 2x2 subplot layout\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Top-left: y = x\u00b2\n# Top-right: y = e^x\n# Bottom-left: y = sin(x)\n# Bottom-right: y = e^x * sin(x)\n\n# Add titles to each subplot\n# Add a main title to the figure\n</code></pre> <p>Hint: Access subplots using <code>axes[row, col].plot()</code> Use <code>axes[row, col].set_title()</code> for individual titles Use <code>fig.suptitle()</code> for main title Expected: Four distinct plots in a grid layout</p>"},{"location":"Week1_Day2/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week1_Day2/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: Matplotlib Histograms and Bar Charts by Corey Schafer (15 min)</p> <p>\u2610 Watch: Seaborn Heatmaps first 15 minutes (15 min)</p>"},{"location":"Week1_Day2/#hands-on-coding-part-2-3-hours","title":"Hands-on Coding - Part 2 (3 hours)","text":""},{"location":"Week1_Day2/#exercise-4-histograms-40-min","title":"Exercise 4: Histograms (40 min)","text":"<p>1. Single histogram: Generate and visualize normal distribution</p> <pre><code>data = np.random.randn(1000)\n# Create histogram with 30 bins\n# Add labels and title\n</code></pre> <p>Hint: Use <code>plt.hist(data, bins=30)</code> Expected: Bell curve shape</p> <p>2. Overlapping histograms: Compare two distributions</p> <pre><code>data1 = np.random.randn(1000)\ndata2 = np.random.randn(1000) + 2  # Shifted distribution\n# Plot both histograms with transparency\n# Add legend\n</code></pre> <p>Hint: Use <code>alpha=0.5</code> for transparency Use different colors for each histogram</p> <p>3. Histogram analysis: Generate data and analyze with histogram</p> <pre><code># Generate data from mixed distributions\ndata = np.concatenate([\n    np.random.randn(500),\n    np.random.randn(500) + 5\n])\n# Create histogram\n# What pattern do you observe?\n</code></pre> <p>Expected: Bimodal distribution (two peaks)</p>"},{"location":"Week1_Day2/#exercise-5-heatmaps-45-min","title":"Exercise 5: Heatmaps (45 min)","text":"<p>1. Correlation matrix: Create and visualize a correlation matrix</p> <pre><code># Generate correlated data\nnp.random.seed(42)\ndata = np.random.randn(100, 5)\n# Calculate correlation matrix\ncorr_matrix = np.corrcoef(data.T)\n\n# Create heatmap\n# Add colorbar and labels\n</code></pre> <p>Hint: Use <code>plt.imshow()</code> with <code>cmap='coolwarm'</code> Use <code>plt.colorbar()</code> to show scale</p> <p>2. 2D function visualization: Visualize z = sin(x) * cos(y)</p> <pre><code>x = np.linspace(-np.pi, np.pi, 100)\ny = np.linspace(-np.pi, np.pi, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(X) * np.cos(Y)\n\n# Create heatmap\n# Add colorbar, labels, and title\n</code></pre> <p>Hint: Use <code>plt.imshow()</code> or <code>plt.contourf()</code> Expected: Symmetric pattern with positive and negative regions</p>"},{"location":"Week1_Day2/#exercise-6-bar-charts-35-min","title":"Exercise 6: Bar Charts (35 min)","text":"<p>1. Simple bar chart: Visualize category data</p> <pre><code>categories = ['Category A', 'Category B', 'Category C', 'Category D']\nvalues = [23, 45, 56, 78]\n# Create bar chart\n# Rotate x-labels if needed\n</code></pre> <p>Hint: Use <code>plt.bar()</code> Use <code>plt.xticks(rotation=45)</code> if labels overlap</p> <p>2. Grouped bar chart: Compare multiple series</p> <pre><code>categories = ['Q1', 'Q2', 'Q3', 'Q4']\nproduct_A = [50, 60, 70, 80]\nproduct_B = [45, 55, 65, 85]\n\n# Create grouped bar chart showing both products\n# Add legend\n</code></pre> <p>Hint: Use <code>x</code> positions and offset bars with width parameter Example: <code>plt.bar(x - width/2, product_A)</code> and <code>plt.bar(x + width/2, product_B)</code></p>"},{"location":"Week1_Day2/#mini-challenge-data-visualization-dashboard-60-min","title":"Mini-Challenge: Data Visualization Dashboard (60 min)","text":"<p>Create a comprehensive visualization dashboard for a synthetic dataset:</p> <pre><code># Generate synthetic student performance data\nnp.random.seed(42)\nn_students = 200\n\nstudy_hours = np.random.uniform(0, 10, n_students)\nattendance = np.random.uniform(50, 100, n_students)\nexam_scores = (study_hours * 5 + attendance * 0.3 + \n               np.random.randn(n_students) * 5)\nexam_scores = np.clip(exam_scores, 0, 100)\n</code></pre> <p>Create a figure with 6 subplots (2 rows \u00d7 3 columns) showing:</p> <ol> <li> <p>Scatter plot: Study hours vs exam scores (with trendline)    Hint: Use <code>np.polyfit()</code> and <code>np.poly1d()</code> for trendline</p> </li> <li> <p>Scatter plot: Attendance vs exam scores (colored by study hours)    Hint: Use <code>c=study_hours</code> and add colorbar</p> </li> <li> <p>Histogram: Distribution of exam scores</p> </li> <li> <p>Mark mean and median with vertical lines      Hint: Use <code>plt.axvline()</code> for vertical lines</p> </li> <li> <p>Histogram: Distribution of study hours</p> </li> <li> <p>Box plot: Exam scores grouped by study hour ranges (0-3, 3-6, 6-10 hours)</p> </li> </ol> <p><code>python    # Categorize students    low_study = exam_scores[study_hours &lt; 3]    med_study = exam_scores[(study_hours &gt;= 3) &amp; (study_hours &lt; 6)]    high_study = exam_scores[study_hours &gt;= 6]</code></p> <p>Hint: Use <code>plt.boxplot([low_study, med_study, high_study])</code></p> <ol> <li>Heatmap: 2D histogram (study hours vs attendance), colored by average exam score    Hint: Use <code>plt.hist2d()</code> or bin the data manually</li> </ol> <p>Requirements:</p> <ul> <li>All subplots should have appropriate titles, labels, and legends where needed</li> <li>Use a consistent color scheme</li> <li>Add a main title to the entire figure</li> <li>Make sure the figure is large enough to see all details clearly</li> </ul>"},{"location":"Week1_Day2/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review all visualizations you created today</p> <p>\u2610 Experiment with different color schemes and styles</p> <p>\u2610 Write your daily reflection (choose 2-3 prompts below)</p> <p>\u2610 Note any questions for the check-in email</p>"},{"location":"Week1_Day2/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>What challenged you the most? How did you approach it?</li> <li>What connections did you make between yesterday's NumPy work and today's visualization?</li> <li>What questions do you still have?</li> <li>How could these visualization techniques be useful in understanding data?</li> <li>What would you do differently if you repeated today?</li> </ul> <p>Next: Day 3 - Introduction to Machine Learning</p>"},{"location":"Week1_Day3/","title":"Week 1, Day 3: Introduction to Machine Learning Concepts","text":""},{"location":"Week1_Day3/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand supervised vs unsupervised learning</li> <li>Learn about train/validation/test splits</li> <li>Understand overfitting and underfitting</li> <li>Grasp regression concepts and gradient descent</li> <li>Implement linear regression from scratch</li> </ul>"},{"location":"Week1_Day3/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week1_Day3/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week1_Day3/#video-learning-2-hours","title":"Video Learning (2 hours)","text":"<p>\u2610 Watch: Machine Learning Basics by StatQuest (10 min)</p> <p>\u2610 Watch: Supervised Learning by StatQuest (9 min)</p> <p>\u2610 Watch: Train, Validation, and Test Sets by StatQuest (10 min)</p> <p>\u2610 Watch: Overfitting and Underfitting by StatQuest (20 min)</p> <p>\u2610 Watch: Linear Regression by StatQuest (27 min)</p> <p>\u2610 Watch: Gradient Descent by StatQuest (20 min)</p>"},{"location":"Week1_Day3/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: Dive into Deep Learning - Chapter 3.1-3.2</p>"},{"location":"Week1_Day3/#hands-on-coding-part-1-15-hours","title":"Hands-on Coding - Part 1 (1.5 hours)","text":""},{"location":"Week1_Day3/#exercise-1-data-splits-30-min","title":"Exercise 1: Data Splits (30 min)","text":"<p>Create train/validation/test splits:</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nimport numpy as np\n\n# Generate data\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n\n# Split into train (60%), val (20%), test (20%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n</code></pre> <p>Expected: 60, 20, 20 samples</p>"},{"location":"Week1_Day3/#exercise-2-linear-regression-from-scratch-60-min","title":"Exercise 2: Linear Regression from Scratch (60 min)","text":"<p>Implement gradient descent:</p> <pre><code>def train_linear_regression(X, y, learning_rate=0.01, epochs=100):\n    w, b = 0.0, 0.0\n    losses = []\n\n    y = y.reshape(-1,1) # This converts y from a vector of size (100, ) to a matrix of size (100, 1)\n\n    for epoch in range(epochs):\n        # Forward pass\n        y_pred = w * X + b\n\n        # Compute loss (MSE)\n        loss = np.mean((y - y_pred) ** 2)\n        losses.append(loss)\n\n        # Compute gradients\n        dw = -2 * np.mean(X * (y - y_pred))\n        db = -2 * np.mean(y - y_pred)\n\n        # Update parameters\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n\n    return w, b, losses\n\nw, b, losses = train_linear_regression(X_train, y_train, learning_rate=0.01, epochs=100)\n</code></pre> <p>Expected: Loss should decrease over epochs. Try play around with the learning rate and observe the effect</p>"},{"location":"Week1_Day3/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week1_Day3/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week1_Day3/#exercise-3-polynomial-regression-overfitting-60-min","title":"Exercise 3: Polynomial Regression &amp; Overfitting (60 min)","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndegrees = [1, 2, 5, 10, 15]\ntrain_errors, val_errors = [], []\n\nfor degree in degrees:\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_val_poly = poly.transform(X_val)\n\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n\n    train_errors.append(mean_squared_error(y_train, model.predict(X_train_poly)))\n    val_errors.append(mean_squared_error(y_val, model.predict(X_val_poly)))\n\n# Plot training vs validation error\nplt.plot(degrees, train_errors, label='Train')\nplt.plot(degrees, val_errors, label='Validation')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('MSE')\nplt.legend()\nplt.show()\n</code></pre> <p>Expected: Validation error increases for high degrees (overfitting)</p>"},{"location":"Week1_Day3/#exercise-4-regularization-50-min","title":"Exercise 4: Regularization (50 min)","text":"<p>Compare Ridge and Lasso:</p> <pre><code>from sklearn.linear_model import Ridge, Lasso\n\n# Using degree 10 polynomial features\npoly = PolynomialFeatures(degree=10)\nX_train_poly = poly.fit_transform(X_train)\nX_val_poly = poly.transform(X_val)\n\n# Compare models\nmodels = {\n    'Linear': LinearRegression(),\n    'Ridge': Ridge(alpha=1.0),\n    'Lasso': Lasso(alpha=0.1)\n}\n\nfor name, model in models.items():\n    model.fit(X_train_poly, y_train)\n    val_pred = model.predict(X_val_poly)\n    val_error = mean_squared_error(y_val, val_pred)\n    print(f\"{name}: Val MSE = {val_error:.4f}\")\n</code></pre> <p>Expected: Ridge/Lasso may have better validation performance</p>"},{"location":"Week1_Day3/#mini-challenge-california-housing-90-min","title":"Mini-Challenge: California Housing (90 min)","text":"<p>Complete end-to-end regression project:</p> <pre><code>from sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split data (60-20-20)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# Train and compare models\nfrom sklearn.metrics import r2_score\n\nfor alpha in [0.1, 1.0, 10.0]:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_scaled, y_train)\n\n    val_pred = ridge.predict(X_val_scaled)\n    val_mse = mean_squared_error(y_val, val_pred)\n    val_r2 = r2_score(y_val, val_pred)\n\n    print(f\"Ridge (\u03b1={alpha}): MSE={val_mse:.4f}, R\u00b2={val_r2:.4f}\")\n\n# Final evaluation on test set with best model\nbest_model = Ridge(alpha=1.0)\nbest_model.fit(X_train_scaled, y_train)\ntest_pred = best_model.predict(X_test_scaled)\nprint(f\"\\nTest R\u00b2: {r2_score(y_test, test_pred):.4f}\")\n\n# Visualize predictions vs actual\nplt.scatter(y_test, test_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nplt.title('Test Set Predictions')\nplt.show()\n</code></pre>"},{"location":"Week1_Day3/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review gradient descent and loss functions \u2610 Understand train/val/test splits \u2610 Write daily reflection (choose 2-3 prompts below) \u2610 List questions for check-in</p>"},{"location":"Week1_Day3/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does gradient descent help models learn?</li> <li>What is the purpose of validation sets?</li> <li>What did you observe about overfitting?</li> <li>How does regularization prevent overfitting?</li> <li>What questions do you still have?</li> </ul> <p>Next: Day 4 - Classification and Logistic Regression</p>"},{"location":"Week1_Day4/","title":"Week 1, Day 4: Classification and Logistic Regression","text":""},{"location":"Week1_Day4/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand classification problems and metrics</li> <li>Learn logistic regression and sigmoid function</li> <li>Explore multi-class classification</li> <li>Practice with real classification datasets</li> <li>Master evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)</li> </ul>"},{"location":"Week1_Day4/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week1_Day4/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week1_Day4/#video-learning-2-hours","title":"Video Learning (2 hours)","text":"<p>\u2610 Watch: Logistic Regression by StatQuest (15 min)</p> <p>\u2610 Watch: Logistic Regression Details Pt 1 by StatQuest (9 min)</p> <p>\u2610 Watch: Logistic Regression Details Pt 2 by StatQuest (11 min)</p> <p>\u2610 Watch: ROC and AUC by StatQuest (16 min)</p> <p>\u2610 Watch: Confusion Matrix by StatQuest (8 min)</p> <p>\u2610 Watch: Sensitivity and Specificity by StatQuest (12 min)</p> <p>\u2610 Watch: Cross Validation by StatQuest (6 min)</p>"},{"location":"Week1_Day4/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: Dive into Deep Learning - Chapter 4.1 - Softmax Regression</p> <p>\u2610 Bookmark: Scikit-learn Classification Metrics</p>"},{"location":"Week1_Day4/#hands-on-coding-part-1-15-hours","title":"Hands-on Coding - Part 1 (1.5 hours)","text":""},{"location":"Week1_Day4/#exercise-1-binary-classification-basics-45-min","title":"Exercise 1: Binary Classification Basics (45 min)","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate binary classification data\nX, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n                          n_informative=2, n_clusters_per_class=1, random_state=42)\n\n# Split data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train logistic regression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm}\")\n\n# Visualize decision boundary\nfrom matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(X, y, model):\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']), \n                edgecolors='black')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Decision Boundary')\n    plt.show()\n\nplot_decision_boundary(X_train, y_train, model)\n</code></pre> <p>Expected: Clear separation between classes with decision boundary</p>"},{"location":"Week1_Day4/#exercise-2-probability-predictions-45-min","title":"Exercise 2: Probability Predictions (45 min)","text":"<pre><code># Get probability predictions\ny_proba = model.predict_proba(X_test)\n\nprint(\"First 10 predictions:\")\nprint(\"True | Pred | P(class=0) | P(class=1)\")\nprint(\"-\" * 45)\nfor i in range(10):\n    print(f\"{y_test[i]:4d} | {y_pred[i]:4d} | {y_proba[i,0]:10.4f} | {y_proba[i,1]:10.4f}\")\n\n# Plot probability histogram\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(y_proba[y_test == 0, 1], bins=20, alpha=0.7, label='Class 0', edgecolor='black')\nplt.hist(y_proba[y_test == 1, 1], bins=20, alpha=0.7, label='Class 1', edgecolor='black')\nplt.xlabel('Predicted Probability of Class 1')\nplt.ylabel('Count')\nplt.title('Probability Distribution by True Class')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# ROC curve\nfrom sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.subplot(1, 2, 2)\nplt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nAUC Score: {roc_auc:.4f}\")\n</code></pre> <p>Expected: AUC should be &gt; 0.9 for this dataset</p>"},{"location":"Week1_Day4/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week1_Day4/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week1_Day4/#exercise-3-multi-class-classification-50-min","title":"Exercise 3: Multi-Class Classification (50 min)","text":"<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n# Load Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train multi-class logistic regression\nmodel = LogisticRegression(max_iter=200, multi_class='multinomial')\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Detailed evaluation\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n# Confusion matrix visualization\nfig, ax = plt.subplots(figsize=(8, 6))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, \n                                        display_labels=iris.target_names,\n                                        cmap='Blues', ax=ax)\nplt.title('Confusion Matrix - Iris Dataset')\nplt.show()\n\n# Feature importance visualization\nfeature_importance = np.abs(model.coef_).mean(axis=0)\nplt.figure(figsize=(10, 6))\nplt.barh(iris.feature_names, feature_importance)\nplt.xlabel('Average Absolute Coefficient')\nplt.title('Feature Importance')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Expected: High accuracy (&gt;0.95) on Iris dataset</p>"},{"location":"Week1_Day4/#exercise-4-cross-validation-40-min","title":"Exercise 4: Cross-Validation (40 min)","text":"<pre><code>from sklearn.model_selection import cross_val_score, cross_validate\n\n# Simple cross-validation\nscores = cross_val_score(model, X, y, cv=5)\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Mean accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n\n# Detailed cross-validation with multiple metrics\nscoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\ncv_results = cross_validate(model, X, y, cv=5, scoring=scoring)\n\nprint(\"\\nDetailed Cross-Validation Results:\")\nfor metric in scoring:\n    scores = cv_results[f'test_{metric}']\n    print(f\"{metric:20s}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n\n# Visualize CV results\nplt.figure(figsize=(10, 6))\nplt.boxplot([cv_results[f'test_{metric}'] for metric in scoring], \n            labels=scoring)\nplt.ylabel('Score')\nplt.title('Cross-Validation Performance')\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Week1_Day4/#mini-challenge-breast-cancer-classification-90-min","title":"Mini-Challenge: Breast Cancer Classification (90 min)","text":"<p>Complete end-to-end binary classification project:</p> <pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n# Load dataset\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\n\nprint(f\"Dataset: {cancer.data.shape[0]} samples, {cancer.data.shape[1]} features\")\nprint(f\"Classes: {cancer.target_names}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\n\n# Split data\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# Hyperparameter tuning with GridSearchCV\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear']\n}\n\ngrid_search = GridSearchCV(LogisticRegression(max_iter=1000), \n                           param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(f\"\\nBest parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation F1: {grid_search.best_score_:.4f}\")\n\n# Train best model\nbest_model = grid_search.best_estimator_\n\n# Comprehensive evaluation\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ny_val_pred = best_model.predict(X_val_scaled)\ny_val_proba = best_model.predict_proba(X_val_scaled)[:, 1]\n\nprint(\"\\nValidation Set Performance:\")\nprint(f\"Accuracy:  {accuracy_score(y_val, y_val_pred):.4f}\")\nprint(f\"Precision: {precision_score(y_val, y_val_pred):.4f}\")\nprint(f\"Recall:    {recall_score(y_val, y_val_pred):.4f}\")\nprint(f\"F1 Score:  {f1_score(y_val, y_val_pred):.4f}\")\n\n# Final test set evaluation\ny_test_pred = best_model.predict(X_test_scaled)\ny_test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"\\nTest Set Performance:\")\nprint(classification_report(y_test, y_test_pred, target_names=cancer.target_names))\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n                                        display_labels=cancer.target_names,\n                                        cmap='Blues', ax=axes[0, 0])\naxes[0, 0].set_title('Confusion Matrix')\n\n# ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_test_proba)\nroc_auc = auc(fpr, tpr)\naxes[0, 1].plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc:.3f}')\naxes[0, 1].plot([0, 1], [0, 1], 'k--')\naxes[0, 1].set_xlabel('False Positive Rate')\naxes[0, 1].set_ylabel('True Positive Rate')\naxes[0, 1].set_title('ROC Curve')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Feature importance (top 10)\nfeature_importance = np.abs(best_model.coef_[0])\ntop_features_idx = np.argsort(feature_importance)[-10:]\naxes[1, 0].barh(range(10), feature_importance[top_features_idx])\naxes[1, 0].set_yticks(range(10))\naxes[1, 0].set_yticklabels([cancer.feature_names[i] for i in top_features_idx])\naxes[1, 0].set_xlabel('Absolute Coefficient')\naxes[1, 0].set_title('Top 10 Important Features')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Probability distribution\naxes[1, 1].hist(y_test_proba[y_test == 0], bins=20, alpha=0.7, \n                label=cancer.target_names[0], edgecolor='black')\naxes[1, 1].hist(y_test_proba[y_test == 1], bins=20, alpha=0.7,\n                label=cancer.target_names[1], edgecolor='black')\naxes[1, 1].set_xlabel('Predicted Probability of Malignant')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title('Prediction Probabilities')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Week1_Day4/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review classification metrics \u2610 Understand ROC curves and AUC \u2610 Write daily reflection (choose 2-3 prompts) \u2610 Prepare questions for Friday check-in</p>"},{"location":"Week1_Day4/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does logistic regression differ from linear regression?</li> <li>What is the difference between accuracy, precision, and recall?</li> <li>When would you prioritize precision over recall, or vice versa?</li> <li>What does the AUC score tell you about a model?</li> <li>What surprised you about the breast cancer classification results?</li> </ul> <p>Next: Day 5 - Scikit-learn &amp; Titanic Project</p>"},{"location":"Week1_Day5/","title":"Week 1, Day 5: Scikit-learn &amp; Titanic Project","text":""},{"location":"Week1_Day5/#daily-goals","title":"Daily Goals","text":"<ul> <li>Master scikit-learn pipelines and workflows</li> <li>Understand various ML algorithms (Decision Trees, Random Forests, SVM, KNN)</li> <li>Learn feature engineering and preprocessing</li> <li>Complete end-to-end Titanic survival prediction project</li> <li>Consolidate Week 1 learning</li> </ul>"},{"location":"Week1_Day5/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week1_Day5/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week1_Day5/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Decision Trees by StatQuest (17 min)</p> <p>\u2610 Watch: Decision Trees Part 2 - Feature Selection by StatQuest (5 min)</p> <p>\u2610 Watch: Random Forests Part 1 by StatQuest (10 min)</p> <p>\u2610 Watch: Random Forests Part 2 by StatQuest (14 min)</p> <p>\u2610 Watch: K-Nearest Neighbors by StatQuest (6 min)</p> <p>\u2610 Watch: Support Vector Machines Part 1 by StatQuest (20 min)</p> <p>\u2610 Watch: Principal Component Analysis (PCA) by StatQuest (20 min)</p>"},{"location":"Week1_Day5/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: Scikit-learn User Guide - Sections 1.1-1.4</p> <p>\u2610 Bookmark: Scikit-learn Algorithm Cheat Sheet</p>"},{"location":"Week1_Day5/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week1_Day5/#exercise-1-comparing-multiple-algorithms-60-min","title":"Exercise 1: Comparing Multiple Algorithms (60 min)","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                          n_redundant=5, random_state=42)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define models\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'KNN': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Compare models\nresults = {}\n\nprint(\"Model Comparison:\")\nprint(\"-\" * 60)\n\nfor name, model in models.items():\n    # Cross-validation scores\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n\n    # Train and test\n    model.fit(X_train_scaled, y_train)\n    train_score = model.score(X_train_scaled, y_train)\n    test_score = model.score(X_test_scaled, y_test)\n\n    results[name] = {\n        'cv_mean': cv_scores.mean(),\n        'cv_std': cv_scores.std(),\n        'train': train_score,\n        'test': test_score\n    }\n\n    print(f\"{name:20s}: CV={cv_scores.mean():.4f}(\u00b1{cv_scores.std():.4f}) \"\n          f\"Train={train_score:.4f} Test={test_score:.4f}\")\n\n# Visualize results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# CV scores comparison\nmodel_names = list(results.keys())\ncv_means = [results[m]['cv_mean'] for m in model_names]\ncv_stds = [results[m]['cv_std'] for m in model_names]\n\naxes[0].bar(range(len(model_names)), cv_means, yerr=cv_stds, capsize=5)\naxes[0].set_xticks(range(len(model_names)))\naxes[0].set_xticklabels(model_names, rotation=45, ha='right')\naxes[0].set_ylabel('Cross-Validation Score')\naxes[0].set_title('Model Comparison - Cross-Validation')\naxes[0].grid(True, alpha=0.3)\n\n# Train vs Test scores\ntrain_scores = [results[m]['train'] for m in model_names]\ntest_scores = [results[m]['test'] for m in model_names]\n\nx = np.arange(len(model_names))\nwidth = 0.35\n\naxes[1].bar(x - width/2, train_scores, width, label='Train', alpha=0.8)\naxes[1].bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(model_names, rotation=45, ha='right')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Train vs Test Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Identify best model\nbest_model_name = max(results, key=lambda x: results[x]['test'])\nprint(f\"\\nBest performing model: {best_model_name}\")\n</code></pre> <p>Expected: Random Forest and SVM typically perform well</p>"},{"location":"Week1_Day5/#exercise-2-scikit-learn-pipelines-and-gridsearchcv-60-min","title":"Exercise 2: Scikit-learn Pipelines and GridSearchCV (60 min)","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Create pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Define parameter grid\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [5, 10, 15, None],\n    'classifier__min_samples_split': [2, 5, 10]\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, \n                          scoring='accuracy', n_jobs=-1, verbose=1)\n\nprint(\"Running Grid Search...\")\ngrid_search.fit(X_train, y_train)\n\nprint(f\"\\nBest parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {grid_search.best_score_:.4f}\")\nprint(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")\n\n# Visualize grid search results\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Plot mean test scores for different n_estimators\nimport pandas as pd\n\nfor max_depth in [5, 10, 15, None]:\n    subset = results_df[results_df['param_classifier__max_depth'] == max_depth]\n    if len(subset) &gt; 0:\n        estimators = subset['param_classifier__n_estimators']\n        scores = subset['mean_test_score']\n        plt.plot(estimators, scores, 'o-', label=f'max_depth={max_depth}')\n\nplt.xlabel('Number of Estimators')\nplt.ylabel('Mean CV Score')\nplt.title('Grid Search Results')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Expected: Performance improves with more estimators, but plateaus</p>"},{"location":"Week1_Day5/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week1_Day5/#titanic-survival-prediction-project-35-hours","title":"Titanic Survival Prediction Project (3.5 hours)","text":"<p>Complete end-to-end machine learning project with the famous Titanic dataset.</p>"},{"location":"Week1_Day5/#phase-1-data-exploration-30-min","title":"Phase 1: Data Exploration (30 min)","text":"<pre><code>import pandas as pd\nimport seaborn as sns\n\n# Load Titanic dataset\ntitanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(titanic_url)\n\nprint(\"Titanic Dataset Overview:\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nColumn Info:\")\nprint(df.info())\n\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\n\nprint(f\"\\nSurvival rate: {df['Survived'].mean():.2%}\")\nprint(f\"\\nMissing values:\")\nprint(df.isnull().sum())\n\n# Exploratory visualizations\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# Survival distribution\ndf['Survived'].value_counts().plot(kind='bar', ax=axes[0,0])\naxes[0,0].set_title('Survival Distribution')\naxes[0,0].set_xticklabels(['Died', 'Survived'], rotation=0)\naxes[0,0].set_ylabel('Count')\n\n# Survival by class\npd.crosstab(df['Pclass'], df['Survived']).plot(kind='bar', ax=axes[0,1])\naxes[0,1].set_title('Survival by Class')\naxes[0,1].set_xlabel('Class')\naxes[0,1].set_xticklabels(['1st', '2nd', '3rd'], rotation=0)\naxes[0,1].legend(['Died', 'Survived'])\n\n# Survival by sex\npd.crosstab(df['Sex'], df['Survived']).plot(kind='bar', ax=axes[0,2])\naxes[0,2].set_title('Survival by Sex')\naxes[0,2].set_xticklabels(['Female', 'Male'], rotation=0)\naxes[0,2].legend(['Died', 'Survived'])\n\n# Age distribution\ndf['Age'].hist(bins=30, ax=axes[1,0], edgecolor='black')\naxes[1,0].set_title('Age Distribution')\naxes[1,0].set_xlabel('Age')\n\n# Fare distribution\ndf['Fare'].hist(bins=30, ax=axes[1,1], edgecolor='black')\naxes[1,1].set_title('Fare Distribution')\naxes[1,1].set_xlabel('Fare')\n\n# Correlation heatmap (numeric features only)\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\ncorrelation = df[numeric_cols].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1,2])\naxes[1,2].set_title('Feature Correlations')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Week1_Day5/#phase-2-data-preprocessing-feature-engineering-45-min","title":"Phase 2: Data Preprocessing &amp; Feature Engineering (45 min)","text":"<pre><code># Create a copy for preprocessing\ndf_processed = df.copy()\n\n# Feature engineering\nprint(\"Creating new features...\")\n\n# 1. Family size\ndf_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1\n\n# 2. Is alone\ndf_processed['IsAlone'] = (df_processed['FamilySize'] == 1).astype(int)\n\n# 3. Title from name\ndf_processed['Title'] = df_processed['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Group rare titles\ntitle_mapping = {\n    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n    'Mlle': 'Miss', 'Mme': 'Mrs', 'Don': 'Rare', 'Dona': 'Rare',\n    'Lady': 'Rare', 'Countess': 'Rare', 'Jonkheer': 'Rare', 'Sir': 'Rare',\n    'Capt': 'Rare', 'Ms': 'Miss'\n}\ndf_processed['Title'] = df_processed['Title'].map(title_mapping)\ndf_processed['Title'].fillna('Rare', inplace=True)\n\n# 4. Fare per person\ndf_processed['FarePerPerson'] = df_processed['Fare'] / df_processed['FamilySize']\n\n# Handle missing values\nprint(\"\\nHandling missing values...\")\n\n# Note: For simplicity, we are preprocessing before splitting. \n# In production, parameters must be derived from training data only to avoid leakage.\n\n# Age: Fill with median by title and class\nfor title in df_processed['Title'].unique():\n    for pclass in df_processed['Pclass'].unique():\n        mask = (df_processed['Title'] == title) &amp; (df_processed['Pclass'] == pclass)\n        median_age = df_processed[mask]['Age'].median()\n        df_processed.loc[mask &amp; df_processed['Age'].isnull(), 'Age'] = median_age\n\n# Embarked: Fill with mode\ndf_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)\n\n# Fare: Fill with median\ndf_processed['Fare'].fillna(df_processed['Fare'].median(), inplace=True)\ndf_processed['FarePerPerson'].fillna(df_processed['FarePerPerson'].median(), inplace=True)\n\n# Drop unnecessary columns\ncolumns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\ndf_processed.drop(columns_to_drop, axis=1, inplace=True)\n\nprint(f\"\\nMissing values after preprocessing:\")\nprint(df_processed.isnull().sum())\n\n# Encode categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\nle_sex = LabelEncoder()\nle_embarked = LabelEncoder()\nle_title = LabelEncoder()\n\ndf_processed['Sex'] = le_sex.fit_transform(df_processed['Sex'])\ndf_processed['Embarked'] = le_embarked.fit_transform(df_processed['Embarked'])\ndf_processed['Title'] = le_title.fit_transform(df_processed['Title'])\n\nprint(f\"\\nProcessed dataset shape: {df_processed.shape}\")\nprint(f\"Features: {df_processed.columns.tolist()}\")\n</code></pre>"},{"location":"Week1_Day5/#phase-3-model-training-comparison-45-min","title":"Phase 3: Model Training &amp; Comparison (45 min)","text":"<pre><code># Separate features and target\nX = df_processed.drop('Survived', axis=1)\ny = df_processed['Survived']\n\n# Split data (60-20-20)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# Train multiple models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nmodels_titanic = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n    'KNN': KNeighborsClassifier(n_neighbors=7)\n}\n\nresults_titanic = []\n\nprint(\"\\nModel Performance on Titanic Dataset:\")\nprint(\"=\" * 80)\n\nfor name, model in models_titanic.items():\n    # Train\n    model.fit(X_train_scaled, y_train)\n\n    # Predict on validation set\n    y_val_pred = model.predict(X_val_scaled)\n    y_val_proba = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_val, y_val_pred)\n    precision = precision_score(y_val, y_val_pred)\n    recall = recall_score(y_val, y_val_pred)\n    f1 = f1_score(y_val, y_val_pred)\n    roc_auc = roc_auc_score(y_val, y_val_proba) if y_val_proba is not None else None\n\n    results_titanic.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1': f1,\n        'ROC-AUC': roc_auc\n    })\n\n    print(f\"{name:20s}: Acc={accuracy:.4f}, Prec={precision:.4f}, \"\n          f\"Rec={recall:.4f}, F1={f1:.4f}, AUC={roc_auc:.4f if roc_auc else 'N/A'}\")\n\n# Convert to DataFrame\nresults_df = pd.DataFrame(results_titanic)\nprint(\"\\n\" + results_df.to_string(index=False))\n</code></pre>"},{"location":"Week1_Day5/#phase-4-hyperparameter-optimization-45-min","title":"Phase 4: Hyperparameter Optimization (45 min)","text":"<pre><code># Select Random Forest for optimization\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    'n_estimators': [50, 100, 200, 300],\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'max_features': ['sqrt', 'log2', None]\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    rf, param_distributions=param_dist,\n    n_iter=20, cv=5, scoring='roc_auc',\n    random_state=42, n_jobs=-1, verbose=1\n)\n\nprint(\"Running Randomized Search...\")\nrandom_search.fit(X_train_scaled, y_train)\n\nprint(f\"\\nBest parameters: {random_search.best_params_}\")\nprint(f\"Best CV ROC-AUC: {random_search.best_score_:.4f}\")\n\n# Evaluate best model on validation set\nbest_rf = random_search.best_estimator_\ny_val_pred_best = best_rf.predict(X_val_scaled)\ny_val_proba_best = best_rf.predict_proba(X_val_scaled)[:, 1]\n\nprint(f\"\\nValidation Performance (Optimized Model):\")\nprint(f\"Accuracy:  {accuracy_score(y_val, y_val_pred_best):.4f}\")\nprint(f\"Precision: {precision_score(y_val, y_val_pred_best):.4f}\")\nprint(f\"Recall:    {recall_score(y_val, y_val_pred_best):.4f}\")\nprint(f\"F1 Score:  {f1_score(y_val, y_val_pred_best):.4f}\")\nprint(f\"ROC-AUC:   {roc_auc_score(y_val, y_val_proba_best):.4f}\")\n</code></pre>"},{"location":"Week1_Day5/#phase-5-final-evaluation-analysis-45-min","title":"Phase 5: Final Evaluation &amp; Analysis (45 min)","text":"<pre><code># Final evaluation on test set\ny_test_pred = best_rf.predict(X_test_scaled)\ny_test_proba = best_rf.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL TEST SET PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_test_pred):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\nprint(f\"Recall:    {recall_score(y_test, y_test_pred):.4f}\")\nprint(f\"F1 Score:  {f1_score(y_test, y_test_pred):.4f}\")\nprint(f\"ROC-AUC:   {roc_auc_score(y_test, y_test_proba):.4f}\")\n\n# Comprehensive visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Confusion Matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n                                        display_labels=['Died', 'Survived'],\n                                        cmap='Blues', ax=axes[0, 0])\naxes[0, 0].set_title('Confusion Matrix - Test Set')\n\n# ROC Curve\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_test, y_test_proba)\nroc_auc_val = auc(fpr, tpr)\naxes[0, 1].plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc_val:.3f}')\naxes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\naxes[0, 1].set_xlabel('False Positive Rate')\naxes[0, 1].set_ylabel('True Positive Rate')\naxes[0, 1].set_title('ROC Curve')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Feature Importance\nfeature_importance = best_rf.feature_importances_\nfeature_names = X.columns\nsorted_idx = np.argsort(feature_importance)[-10:]  # Top 10\n\naxes[1, 0].barh(range(len(sorted_idx)), feature_importance[sorted_idx])\naxes[1, 0].set_yticks(range(len(sorted_idx)))\naxes[1, 0].set_yticklabels([feature_names[i] for i in sorted_idx])\naxes[1, 0].set_xlabel('Importance')\naxes[1, 0].set_title('Top 10 Feature Importance')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Prediction Probability Distribution\naxes[1, 1].hist(y_test_proba[y_test == 0], bins=20, alpha=0.7, \n                label='Died', edgecolor='black')\naxes[1, 1].hist(y_test_proba[y_test == 1], bins=20, alpha=0.7,\n                label='Survived', edgecolor='black')\naxes[1, 1].set_xlabel('Predicted Probability of Survival')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title('Prediction Probabilities by True Class')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Detailed feature importance analysis\nprint(\"\\nFeature Importance Ranking:\")\nprint(\"=\"*60)\nfeature_imp_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importance\n}).sort_values('Importance', ascending=False)\n\nfor idx, row in feature_imp_df.iterrows():\n    print(f\"{row['Feature']:20s}: {row['Importance']:.4f}\")\n\n# Error analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"ERROR ANALYSIS\")\nprint(\"=\"*60)\n\n# Find misclassified samples\nmisclassified_idx = np.where(y_test_pred != y_test)[0]\nprint(f\"Number of misclassified samples: {len(misclassified_idx)}\")\n\nif len(misclassified_idx) &gt; 0:\n    # Show a few examples\n    print(\"\\nSample Misclassifications:\")\n    for i in misclassified_idx[:5]:\n        true_label = 'Survived' if y_test.iloc[i] == 1 else 'Died'\n        pred_label = 'Survived' if y_test_pred[i] == 1 else 'Died'\n        confidence = y_test_proba[i] if y_test_pred[i] == 1 else 1 - y_test_proba[i]\n\n        print(f\"\\nSample {i}:\")\n        print(f\"  True: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.2%}\")\n        print(f\"  Features: {X_test.iloc[i].to_dict()}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PROJECT COMPLETE!\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"Week1_Day5/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review entire Week 1 journey \u2610 Document your Titanic project results \u2610 Write comprehensive reflection (address all prompts) \u2610 Prepare for Week 1 review and Week 2 preview</p>"},{"location":"Week1_Day5/#daily-reflection-prompts-address-all","title":"Daily Reflection Prompts (Address All):","text":"<ul> <li>What was the most important concept you learned this week?</li> <li>How did the Titanic project bring together all Week 1 concepts?</li> <li>What was your biggest challenge this week? How did you overcome it?</li> <li>Which machine learning algorithm performed best on Titanic? Why?</li> <li>What surprised you most about feature engineering?</li> <li>Review Days 1-5: What are the key takeaways from each day?</li> <li>How confident do you feel about starting Week 2?</li> <li>What topics from Week 1 need more practice?</li> <li>What are you most excited to learn in Week 2?</li> </ul>"},{"location":"Week1_Day5/#week-1-complete","title":"Week 1 Complete!","text":"<p>Congratulations on completing Week 1! You now understand:</p> <ul> <li>NumPy for numerical computing</li> <li>Matplotlib for visualization</li> <li>Core ML concepts (supervised learning, overfitting, regularization)</li> <li>Linear and logistic regression</li> <li>Multiple ML algorithms (Decision Trees, Random Forests, SVM, KNN)</li> <li>Scikit-learn pipelines and workflows</li> <li>End-to-end ML project execution</li> </ul> <p>Weekend Activities:</p> <ul> <li>Review your Week 1 notes</li> <li>Revisit challenging exercises</li> <li>Share your Titanic results with peers</li> <li>Preview Week 2 topics</li> </ul> <p>Next Week: Neural Networks and Deep Learning with PyTorch!</p> <p>Next: Week 2 Overview</p>"},{"location":"Week1_Overview/","title":"Week 1 Overview: Python &amp; ML Fundamentals","text":""},{"location":"Week1_Overview/#introduction","title":"Introduction","text":"<p>Week 1 establishes the foundation for your machine learning journey. You will master essential Python libraries, understand fundamental ML concepts, and complete your first end-to-end machine learning project.</p>"},{"location":"Week1_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Master NumPy for numerical computing and array operations</li> <li>Learn Matplotlib for data visualization and result presentation</li> <li>Understand core ML concepts: supervised learning, train/val/test splits, overfitting</li> <li>Implement linear regression and logistic classification from scratch</li> <li>Use scikit-learn for practical machine learning workflows</li> <li>Complete Titanic survival prediction project</li> </ul>"},{"location":"Week1_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 1: Python Crash Course - NumPy &amp; Arrays</li> <li>Day 2: Python Crash Course - Matplotlib &amp; Data Visualization</li> <li>Day 3: Introduction to Machine Learning Concepts</li> <li>Day 4: Classification and Logistic Regression</li> <li>Day 5: Scikit-learn Ecosystem and Mini-Project</li> </ul>"},{"location":"Week1_Overview/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>By the end of Week 1, you will be able to:</p> <ul> <li>Manipulate multi-dimensional arrays efficiently using NumPy</li> <li>Create professional visualizations to communicate insights</li> <li>Explain the difference between training, validation, and test sets</li> <li>Implement gradient descent and understand how models learn</li> <li>Evaluate classification models using appropriate metrics</li> <li>Build complete ML pipelines using scikit-learn</li> <li>Work independently on structured ML projects</li> </ul>"},{"location":"Week1_Overview/#tips-for-success","title":"Tips for Success","text":"<p>To get the most out of Week 1:</p> <ul> <li>Watch all video tutorials completely before starting exercises</li> <li>Type out code examples rather than copying and pasting - except for print statements, plotting statements etc I recommend that you should write out every line of code, unless you understand it completely.</li> <li>Complete daily reflections - they help consolidate learning</li> <li>Ask questions during check-ins if concepts are unclear</li> <li>Compare your solutions with peers to learn different approaches</li> </ul>"},{"location":"Week1_Overview/#daily-schedule-format","title":"Daily Schedule Format","text":"<p>Each day follows this structure:</p>"},{"location":"Week1_Overview/#morning-session-4-hours","title":"Morning Session (4 hours)","text":"<ul> <li>Optional: Daily check-in with peers on Teams (15 min)</li> <li>Video Learning (60-120 min)</li> <li>Reference Material (30 min)</li> <li>Hands-on Coding - Part 1 (remainder)</li> </ul>"},{"location":"Week1_Overview/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":"<ul> <li>Video Learning (30 min, if applicable)</li> <li>Hands-on Coding - Part 2 (3-3.5 hours)</li> </ul>"},{"location":"Week1_Overview/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<ul> <li>Review key concepts</li> <li>Write daily reflection</li> <li>List questions for check-in</li> </ul>"},{"location":"Week1_Overview/#assessment","title":"Assessment","text":"<p>As long as you demonstrate a bona fide effort in completing exercises and projects, you will receive credit for the vac work. Focus on learning rather than perfection.</p>"},{"location":"Week1_Overview/#communication","title":"Communication","text":"<ul> <li>Microsoft Teams: Use the group for questions, discussions, and peer support</li> <li>Check-ins: Monday, Wednesday, Friday (approximately 1 hour each)</li> <li>Email: For individual questions or concerns</li> </ul> <p>Ready to begin? Start with Day 1: NumPy &amp; Arrays</p>"},{"location":"Week2_Day10/","title":"Week 2, Day 10: MNIST Project - Digit Classification","text":""},{"location":"Week2_Day10/#daily-goals","title":"Daily Goals","text":"<ul> <li>Complete end-to-end MNIST digit classification project</li> <li>Achieve &gt;95% accuracy (target: &gt;98%)</li> <li>Apply all Week 2 concepts in integrated project</li> <li>Create professional documentation and visualizations</li> <li>Build portfolio-ready project</li> </ul>"},{"location":"Week2_Day10/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week2_Day10/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week2_Day10/#video-learning-45-min","title":"Video Learning (45 min)","text":"<p>\u2610 Watch: MNIST with PyTorch by Sentdex (20 min) Complete walkthrough of MNIST project</p> <p>\u2610 Watch: Saving &amp; Loading Models by Python Engineer (10 min) How to save and load your trained models</p> <p>\u2610 Optional Review: Any Week 2 videos as needed (15 min)</p>"},{"location":"Week2_Day10/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: PyTorch Save and Load</p> <p>\u2610 Review: D2L Chapter 5 as needed</p>"},{"location":"Week2_Day10/#project-briefing-30-min","title":"Project Briefing (30 min)","text":"<p>Read this entire project structure before starting:</p> <pre><code>\"\"\"\nMNIST DIGIT CLASSIFICATION PROJECT\n\nGoal: Build a neural network that achieves &gt;95% accuracy on MNIST digit classification\n\nProject Structure:\n1. Phase 1: Data Exploration (30 min)\n2. Phase 2: Baseline Model (45 min)\n3. Phase 3: Improved Model (60 min)\n4. Phase 4: Analysis &amp; Visualization (45 min)\n5. Phase 5: Documentation (30 min)\n\nTotal Time: ~3.5 hours\n\nSuccess Criteria:\n- Minimum: &gt;90% test accuracy\n- Target: &gt;95% test accuracy\n- Stretch: &gt;98% test accuracy\n- Code is well-organized and documented\n- Visualizations are clear and informative\n- Write-up explains methodology and results\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\n# Set random seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"=\"*70)\nprint(\"MNIST DIGIT CLASSIFICATION PROJECT\")\nprint(\"Week 2, Day 10 Capstone\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"Week2_Day10/#phase-1-data-exploration-30-min","title":"Phase 1: Data Exploration (30 min)","text":"<p>Understand the dataset thoroughly:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: DATA EXPLORATION\")\nprint(\"=\"*70)\n\n# Load MNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\nprint(f\"Image shape: {train_dataset[0][0].shape}\")\nprint(f\"Number of classes: {len(train_dataset.classes)}\")\n\n# Split training data into train/validation\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_data, val_data = random_split(train_dataset, [train_size, val_size])\n\nprint(f\"\\nSplit:\")\nprint(f\"Training: {len(train_data)}\")\nprint(f\"Validation: {len(val_data)}\")\nprint(f\"Test: {len(test_dataset)}\")\n\n# Visualize samples\nfig, axes = plt.subplots(4, 10, figsize=(16, 7))\naxes = axes.flatten()\n\nfor i in range(40):\n    img, label = train_dataset[i]\n    axes[i].imshow(img.squeeze(), cmap='gray')\n    axes[i].set_title(f'{label}', fontsize=10)\n    axes[i].axis('off')\n\nplt.suptitle('MNIST Dataset Samples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Class distribution\nlabels = [train_dataset[i][1] for i in range(len(train_dataset))]\nunique, counts = np.unique(labels, return_counts=True)\n\nplt.figure(figsize=(10, 6))\nplt.bar(unique, counts, color='steelblue', edgecolor='black')\nplt.xlabel('Digit Class')\nplt.ylabel('Count')\nplt.title('Class Distribution in Training Set')\nplt.xticks(unique)\nplt.grid(True, alpha=0.3, axis='y')\nfor i, (digit, count) in enumerate(zip(unique, counts)):\n    plt.text(digit, count + 50, str(count), ha='center', fontsize=10)\nplt.show()\n\nprint(f\"\\nClass distribution:\")\nfor digit, count in zip(unique, counts):\n    print(f\"  Digit {digit}: {count} samples ({count/len(labels)*100:.1f}%)\")\n\nprint(\"\\n\u2713 Data exploration complete\")\n</code></pre>"},{"location":"Week2_Day10/#phase-2-baseline-model-45-min","title":"Phase 2: Baseline Model (45 min)","text":"<p>Build a simple model to establish baseline:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: BASELINE MODEL\")\nprint(\"=\"*70)\n\n# Simple baseline network\nclass BaselineNet(nn.Module):\n    def __init__(self):\n        super(BaselineNet, self).__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Create data loaders\nbatch_size = 128\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Initialize model\nbaseline_model = BaselineNet()\nprint(f\"\\nBaseline Model:\")\nprint(baseline_model)\n\ntotal_params = sum(p.numel() for p in baseline_model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Training function\ndef train_epoch(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    return running_loss / len(train_loader), correct / total\n\ndef validate(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return running_loss / len(val_loader), correct / total\n\n# Train baseline model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n\nprint(\"\\nTraining baseline model...\")\nepochs = 10\nbaseline_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\nfor epoch in range(epochs):\n    train_loss, train_acc = train_epoch(baseline_model, train_loader, criterion, optimizer)\n    val_loss, val_acc = validate(baseline_model, val_loader, criterion)\n\n    baseline_history['train_loss'].append(train_loss)\n    baseline_history['val_loss'].append(val_loss)\n    baseline_history['train_acc'].append(train_acc)\n    baseline_history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n# Test baseline\ntest_loss, test_acc = validate(baseline_model, test_loader, criterion)\nprint(f\"\\n\ud83c\udfaf Baseline Test Accuracy: {test_acc:.4f}\")\n\nprint(\"\\n\u2713 Baseline model complete\")\n</code></pre>"},{"location":"Week2_Day10/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week2_Day10/#phase-3-improved-model-90-min","title":"Phase 3: Improved Model (90 min)","text":"<p>Build a better model with what you've learned:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: IMPROVED MODEL\")\nprint(\"=\"*70)\n\nclass ImprovedNet(nn.Module):\n    def __init__(self, dropout_prob=0.3):\n        super(ImprovedNet, self).__init__()\n        self.flatten = nn.Flatten()\n\n        # Deeper network with batch norm and dropout\n        self.fc1 = nn.Linear(28*28, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(dropout_prob)\n\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(dropout_prob)\n\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.dropout3 = nn.Dropout(dropout_prob)\n\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.dropout1(x)\n\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = self.dropout2(x)\n\n        x = self.fc3(x)\n        x = self.bn3(x)\n        x = torch.relu(x)\n        x = self.dropout3(x)\n\n        x = self.fc4(x)\n        return x\n\n# Create improved model\nimproved_model = ImprovedNet(dropout_prob=0.25)\nprint(f\"\\nImproved Model:\")\nprint(improved_model)\n\ntotal_params = sum(p.numel() for p in improved_model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Train improved model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(improved_model.parameters(), lr=0.001, weight_decay=1e-4)\n\nprint(\"\\nTraining improved model...\")\nepochs = 15\nimproved_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\nbest_val_acc = 0\npatience = 5\npatience_counter = 0\n\nfor epoch in range(epochs):\n    train_loss, train_acc = train_epoch(improved_model, train_loader, criterion, optimizer)\n    val_loss, val_acc = validate(improved_model, val_loader, criterion)\n\n    improved_history['train_loss'].append(train_loss)\n    improved_history['val_loss'].append(val_loss)\n    improved_history['train_acc'].append(train_acc)\n    improved_history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    # Early stopping\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n        # Save best model\n        torch.save(improved_model.state_dict(), 'best_mnist_model.pth')\n    else:\n        patience_counter += 1\n\n    if patience_counter &gt;= patience:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\n# Load best model\nimproved_model.load_state_dict(torch.load('best_mnist_model.pth'))\n\n# Test improved model\ntest_loss, test_acc = validate(improved_model, test_loader, criterion)\nprint(f\"\\n\ud83c\udfaf Improved Test Accuracy: {test_acc:.4f}\")\n\n# Compare models\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(f\"Baseline Test Accuracy:  {baseline_history['val_acc'][-1]:.4f}\")\nprint(f\"Improved Test Accuracy:  {test_acc:.4f}\")\nprint(f\"Improvement: +{(test_acc - baseline_history['val_acc'][-1]):.4f}\")\n\nprint(\"\\n\u2713 Improved model complete\")\n</code></pre>"},{"location":"Week2_Day10/#phase-4-analysis-visualization-60-min","title":"Phase 4: Analysis &amp; Visualization (60 min)","text":"<p>Comprehensive evaluation of the best model:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: ANALYSIS &amp; VISUALIZATION\")\nprint(\"=\"*70)\n\n# 1. Training curves comparison\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\naxes[0, 0].plot(baseline_history['train_loss'], label='Train', alpha=0.7)\naxes[0, 0].plot(baseline_history['val_loss'], label='Val', alpha=0.7)\naxes[0, 0].set_title('Baseline - Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(improved_history['train_loss'], label='Train', alpha=0.7)\naxes[0, 1].plot(improved_history['val_loss'], label='Val', alpha=0.7)\naxes[0, 1].set_title('Improved - Loss')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[1, 0].plot(baseline_history['train_acc'], label='Train', alpha=0.7)\naxes[1, 0].plot(baseline_history['val_acc'], label='Val', alpha=0.7)\naxes[1, 0].set_title('Baseline - Accuracy')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Accuracy')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(improved_history['train_acc'], label='Train', alpha=0.7)\naxes[1, 1].plot(improved_history['val_acc'], label='Val', alpha=0.7)\naxes[1, 1].set_title('Improved - Accuracy')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Accuracy')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.suptitle('Training Comparison', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# 2. Confusion matrix\nprint(\"\\nGenerating confusion matrix...\")\nimproved_model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = improved_model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Confusion Matrix - Test Accuracy: {test_acc:.4f}')\nplt.show()\n\n# 3. Per-class accuracy\nprint(\"\\nPer-class Performance:\")\nprint(\"-\" * 50)\nfor i in range(10):\n    class_correct = cm[i, i]\n    class_total = cm[i, :].sum()\n    class_acc = class_correct / class_total\n    print(f\"Digit {i}: {class_acc:.4f} ({class_correct}/{class_total})\")\n\n# 4. Visualize mistakes\nprint(\"\\nFinding misclassified examples...\")\nimproved_model.eval()\nmistakes = {'images': [], 'true': [], 'pred': [], 'conf': []}\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = improved_model(images)\n        probs = torch.softmax(outputs, dim=1)\n        confidences, predicted = torch.max(probs, 1)\n\n        # Find mistakes\n        mask = predicted != labels\n        if mask.sum() &gt; 0:\n            mistakes['images'].extend(images[mask])\n            mistakes['true'].extend(labels[mask])\n            mistakes['pred'].extend(predicted[mask])\n            mistakes['conf'].extend(confidences[mask])\n\n        if len(mistakes['images']) &gt;= 20:\n            break\n\n# Plot mistakes\nfig, axes = plt.subplots(4, 5, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i in range(min(20, len(mistakes['images']))):\n    img = mistakes['images'][i].squeeze()\n    true_label = mistakes['true'][i].item()\n    pred_label = mistakes['pred'][i].item()\n    conf = mistakes['conf'][i].item()\n\n    axes[i].imshow(img, cmap='gray')\n    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({conf:.2f})', fontsize=9)\n    axes[i].axis('off')\n\nplt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Analysis complete\")\n</code></pre>"},{"location":"Week2_Day10/#phase-5-documentation-30-min","title":"Phase 5: Documentation (30 min)","text":"<p>Create professional project documentation:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: DOCUMENTATION\")\nprint(\"=\"*70)\n\n# Generate project report\nreport = f\"\"\"\n{'='*70}\nMNIST DIGIT CLASSIFICATION - PROJECT REPORT\n{'='*70}\n\nPROJECT OVERVIEW\n----------------\nGoal: Build a neural network to classify handwritten digits (MNIST dataset)\nTarget Accuracy: &gt;95%\nAchieved Accuracy: {test_acc:.4f}\nStatus: {'\u2713 SUCCESS' if test_acc &gt; 0.95 else '\u2717 TARGET NOT MET'}\n\nDATASET\n-------\nTraining samples: {len(train_data):,}\nValidation samples: {len(val_data):,}\nTest samples: {len(test_dataset):,}\nImage size: 28x28 pixels\nClasses: 10 (digits 0-9)\nPreprocessing: Normalized to mean=0.1307, std=0.3081\n\nMODELS DEVELOPED\n----------------\n\n1. Baseline Model\n   - Architecture: Simple 2-layer network (784 \u2192 128 \u2192 10)\n   - Parameters: {sum(p.numel() for p in baseline_model.parameters()):,}\n   - Test Accuracy: {baseline_history['val_acc'][-1]:.4f}\n\n2. Improved Model\n   - Architecture: Deep network with regularization\n     * 4 fully connected layers (784 \u2192 512 \u2192 256 \u2192 128 \u2192 10)\n     * Batch normalization after each hidden layer\n     * Dropout (p=0.25) for regularization\n     * ReLU activation functions\n   - Parameters: {sum(p.numel() for p in improved_model.parameters()):,}\n   - Test Accuracy: {test_acc:.4f}\n   - Improvement: +{(test_acc - baseline_history['val_acc'][-1]):.4f}\n\nTRAINING DETAILS\n----------------\nOptimizer: Adam\nLearning Rate: 0.001\nWeight Decay: 1e-4\nBatch Size: 128\nEpochs: {len(improved_history['train_acc'])} (with early stopping)\nLoss Function: Cross Entropy Loss\n\nRESULTS SUMMARY\n---------------\nBest Validation Accuracy: {best_val_acc:.4f}\nFinal Test Accuracy: {test_acc:.4f}\nAverage per-class accuracy: {np.diag(cm).sum() / cm.sum():.4f}\n\nMost confused pairs:\n\"\"\"\n\n# Find most confused digit pairs\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nnp.fill_diagonal(cm_normalized, 0)\nconfused_pairs = []\n\nfor i in range(10):\n    for j in range(10):\n        if i != j and cm_normalized[i, j] &gt; 0.05:\n            confused_pairs.append((i, j, cm_normalized[i, j]))\n\nconfused_pairs.sort(key=lambda x: x[2], reverse=True)\n\nfor i, (true_digit, pred_digit, conf_rate) in enumerate(confused_pairs[:5]):\n    report += f\"  {i+1}. Digit {true_digit} classified as {pred_digit}: {conf_rate:.2%}\\n\"\n\nreport += f\"\"\"\n\nKEY INSIGHTS\n------------\n1. Batch normalization stabilized training\n2. Dropout helped prevent overfitting\n3. Deeper network improved accuracy significantly\n4. Early stopping prevented overtraining\n\nFUTURE IMPROVEMENTS\n-------------------\n1. Implement Convolutional Neural Networks (Week 3!)\n2. Data augmentation (rotation, scaling)\n3. Ensemble methods\n4. Hyperparameter tuning with grid search\n\nCONCLUSION\n----------\nSuccessfully built a neural network achieving {test_acc:.4f} test accuracy\non MNIST digit classification. This demonstrates understanding of:\n- Neural network architecture design\n- PyTorch implementation\n- Training with regularization\n- Model evaluation and analysis\n\n{'='*70}\nWeek 2 Capstone Project Complete! \ud83c\udf89\n{'='*70}\n\"\"\"\n\nprint(report)\n\n# Save report to file\nwith open('mnist_project_report.txt', 'w') as f:\n    f.write(report)\n\nprint(\"\\n\ud83d\udcc4 Report saved to: mnist_project_report.txt\")\nprint(\"\ud83d\udcbe Model saved to: best_mnist_model.pth\")\n\nprint(\"\\n\u2713 Documentation complete\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83c\udf89 CONGRATULATIONS! PROJECT COMPLETE!\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"Week2_Day10/#reflection-week-review-30-min","title":"Reflection &amp; Week Review (30 min)","text":"<p>\u2610 Review entire Week 2 journey \u2610 Document key learnings \u2610 Celebrate achievements \u2610 Prepare for Week 3</p>"},{"location":"Week2_Day10/#week-2-reflection-prompts-address-all","title":"Week 2 Reflection Prompts (Address All):","text":"<ul> <li>What was the most valuable thing you learned this week?</li> <li>How has your understanding of neural networks evolved?</li> <li>What was your biggest challenge? How did you overcome it?</li> <li>How does your final MNIST accuracy compare to your expectations?</li> <li>What connections did you make between Days 6-10?</li> <li>How confident do you feel about neural networks now?</li> <li>What are you most excited to learn in Week 3 (CNNs)?</li> <li>What from Week 2 needs more practice?</li> </ul>"},{"location":"Week2_Day10/#week-2-achievement-checklist","title":"Week 2 Achievement Checklist:","text":"<p>\u2610 Understood neural network architecture \u2610 Implemented forward propagation from scratch \u2610 Implemented backpropagation from scratch \u2610 Solved XOR problem \u2610 Learned PyTorch fundamentals \u2610 Built models with nn.Module \u2610 Used Dataset and DataLoader \u2610 Applied dropout and batch normalization \u2610 Completed MNIST project with &gt;95% accuracy \u2610 Created professional documentation</p>"},{"location":"Week2_Day10/#week-2-complete","title":"\ud83c\udf89 Week 2 Complete!","text":"<p>Achievements Unlocked: - \u2705 Neural networks from first principles - \u2705 Backpropagation mastery - \u2705 PyTorch proficiency - \u2705 Production-ready training pipelines - \u2705 Portfolio project complete</p> <p>Next Week Preview: Week 3 introduces Convolutional Neural Networks (CNNs) - the backbone of computer vision. You'll learn about convolutions, pooling, modern architectures (ResNet, VGG), and complete CIFAR-10 classification!</p> <p>Weekend Recommendations: - Review your Week 2 notes - Share your MNIST results with classmates - Optional: Try improving your MNIST model further - Rest and prepare for CNNs!</p> <p>Next: Week 3 Overview</p>"},{"location":"Week2_Day6/","title":"Week 2, Day 6: Neural Network Theory and Forward Propagation","text":""},{"location":"Week2_Day6/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand neural network architecture (layers, nodes, connections)</li> <li>Learn about activation functions and their purposes</li> <li>Implement forward propagation from scratch in NumPy</li> <li>Grasp the intuition behind how neural networks transform data</li> <li>Solve the XOR problem with a neural network</li> </ul>"},{"location":"Week2_Day6/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week2_Day6/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week2_Day6/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: But what is a neural network? by 3Blue1Brown (19 min) This is THE best introduction to neural networks. Watch it carefully.</p> <p>\u2610 Watch: Gradient descent, how neural networks learn by 3Blue1Brown (21 min) Builds intuition for the learning process</p> <p>\u2610 Watch: Neural Networks Part 1: Setup by StatQuest (20 min) Alternative perspective - good for reinforcement</p> <p>\u2610 Watch: Activation Functions by StatQuest (9 min) Understand why we need non-linearity</p> <p>\u2610 Watch: Neural Network Architectures by StatQuest (8 min) Different ways to structure networks</p>"},{"location":"Week2_Day6/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 5.1 - Multilayer Perceptrons Mathematical foundation for neural networks</p> <p>\u2610 Read: D2L Chapter 5.2 - Implementation from Scratch See a complete implementation before building your own</p>"},{"location":"Week2_Day6/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week2_Day6/#setup-10-min","title":"Setup (10 min)","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n</code></pre>"},{"location":"Week2_Day6/#exercise-1-activation-functions-40-min","title":"Exercise 1: Activation Functions (40 min)","text":"<p>Implement and visualize the key activation functions:</p> <p>1. Sigmoid</p> <pre><code>def sigmoid(x):\n    \"\"\"\n    Sigmoid activation: \u03c3(x) = 1 / (1 + e^(-x))\n    Maps input to (0, 1)\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    \"\"\"\n    Derivative: \u03c3'(x) = \u03c3(x) * (1 - \u03c3(x))\n    \"\"\"\n    s = sigmoid(x)\n    return s * (1 - s)\n\n# Test\nx = np.linspace(-10, 10, 100)\ny = sigmoid(x)\ny_prime = sigmoid_derivative(x)\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, label='sigmoid(x)', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('sigmoid(x)')\nplt.title('Sigmoid Function')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y_prime, label=\"sigmoid'(x)\", color='orange', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel(\"sigmoid'(x)\")\nplt.title('Sigmoid Derivative')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Expected: S-curve for sigmoid, bell curve for derivative</p> <p>2. Tanh</p> <pre><code>def tanh(x):\n    \"\"\"\n    Hyperbolic tangent: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n    Maps input to (-1, 1)\n    \"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"\n    Derivative: tanh'(x) = 1 - tanh\u00b2(x)\n    \"\"\"\n    t = tanh(x)\n    return 1 - t**2\n\n# Visualize\ny_tanh = tanh(x)\ny_tanh_prime = tanh_derivative(x)\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y_tanh, label='tanh(x)', color='green', linewidth=2)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.title('Tanh Function')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y_tanh_prime, label=\"tanh'(x)\", color='red', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.title('Tanh Derivative')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Expected: Similar to sigmoid but centered at zero</p> <p>3. ReLU (Rectified Linear Unit)</p> <pre><code>def relu(x):\n    \"\"\"\n    ReLU: max(0, x)\n    Most popular activation for hidden layers\n    \"\"\"\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    \"\"\"\n    Derivative: 1 if x &gt; 0, else 0\n    \"\"\"\n    return (x &gt; 0).astype(float)\n\n# Visualize\ny_relu = relu(x)\ny_relu_prime = relu_derivative(x)\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y_relu, label='ReLU(x)', color='purple', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.title('ReLU Function')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y_relu_prime, label=\"ReLU'(x)\", color='brown', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.title('ReLU Derivative')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Expected: Straight line for x&gt;0, flat for x&lt;0</p> <p>4. Compare All</p> <pre><code>plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(x, sigmoid(x), label='Sigmoid', linewidth=2)\nplt.plot(x, tanh(x), label='Tanh', linewidth=2)\nplt.plot(x, relu(x), label='ReLU', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Activation Functions Comparison')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, sigmoid_derivative(x), label=\"Sigmoid'\", linewidth=2)\nplt.plot(x, tanh_derivative(x), label=\"Tanh'\", linewidth=2)\nplt.plot(x, relu_derivative(x), label=\"ReLU'\", linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.xlabel('x')\nplt.ylabel(\"f'(x)\")\nplt.title('Derivatives Comparison')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key observations:\")\nprint(\"1. Sigmoid: outputs (0,1), derivative peaks at 0\")\nprint(\"2. Tanh: outputs (-1,1), similar to sigmoid but centered\")\nprint(\"3. ReLU: simple, efficient, derivative is 0 or 1\")\nprint(\"4. ReLU avoids vanishing gradient problem for x &gt; 0\")\n</code></pre>"},{"location":"Week2_Day6/#exercise-2-forward-propagation-manual-calculation-30-min","title":"Exercise 2: Forward Propagation - Manual Calculation (30 min)","text":"<p>Work through forward propagation by hand first:</p> <pre><code># Simple network: 2 inputs -&gt; 2 hidden -&gt; 1 output\n# Let's trace through an example\n\nprint(\"=\" * 60)\nprint(\"MANUAL FORWARD PROPAGATION EXAMPLE\")\nprint(\"=\" * 60)\n\n# Network architecture\nprint(\"\\nArchitecture: 2 inputs \u2192 2 hidden neurons \u2192 1 output\")\n\n# Input\nX = np.array([0.5, 0.8])\nprint(f\"\\nInput: X = {X}\")\n\n# Weights and biases (initialized randomly for now)\nW1 = np.array([[0.2, 0.5],   # weights from input to hidden\n               [0.3, 0.4]])  \nb1 = np.array([0.1, 0.2])     # biases for hidden layer\n\nW2 = np.array([[0.6],          # weights from hidden to output\n               [0.7]])\nb2 = np.array([0.3])           # bias for output\n\nprint(f\"\\nW1 (input\u2192hidden):\\n{W1}\")\nprint(f\"b1 (hidden biases): {b1}\")\nprint(f\"\\nW2 (hidden\u2192output):\\n{W2}\")\nprint(f\"b2 (output bias): {b2}\")\n\n# Hidden layer computation\nprint(\"\\n\" + \"=\" * 60)\nprint(\"HIDDEN LAYER\")\nprint(\"=\" * 60)\n\n# Pre-activation (linear combination)\nz1 = np.dot(X, W1) + b1\nprint(f\"\\nz1 = X\u00b7W1 + b1\")\nprint(f\"z1 = {X} \u00b7 {W1.T} + {b1}\")\nprint(f\"z1 = {z1}\")\n\n# Activation\na1 = sigmoid(z1)\nprint(f\"\\na1 = sigmoid(z1)\")\nprint(f\"a1 = {a1}\")\n\n# Output layer computation\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OUTPUT LAYER\")\nprint(\"=\" * 60)\n\n# Pre-activation\nz2 = np.dot(a1, W2) + b2\nprint(f\"\\nz2 = a1\u00b7W2 + b2\")\nprint(f\"z2 = {a1} \u00b7 {W2.T} + {b2}\")\nprint(f\"z2 = {z2}\")\n\n# Activation\na2 = sigmoid(z2)\nprint(f\"\\na2 = sigmoid(z2)\")\nprint(f\"a2 = {a2}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FINAL OUTPUT: {a2[0]:.4f}\")\nprint(f\"{'='*60}\")\n</code></pre> <p>Expected: Follow the computation step-by-step, verify numbers</p>"},{"location":"Week2_Day6/#exercise-3-implement-neural-network-class-40-min","title":"Exercise 3: Implement Neural Network Class (40 min)","text":"<p>Build a complete neural network from scratch:</p> <pre><code>class NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initialize a 2-layer neural network\n\n        Args:\n            input_size: number of input features\n            hidden_size: number of neurons in hidden layer\n            output_size: number of output neurons\n        \"\"\"\n        # Initialize weights with small random values\n        # He initialization for better training\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n        self.b1 = np.zeros((1, hidden_size))\n\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n        self.b2 = np.zeros((1, output_size))\n\n        print(f\"Network initialized:\")\n        print(f\"  Input size: {input_size}\")\n        print(f\"  Hidden size: {hidden_size}\")\n        print(f\"  Output size: {output_size}\")\n        print(f\"  Total parameters: {self.count_parameters()}\")\n\n    def count_parameters(self):\n        \"\"\"Count total number of parameters\"\"\"\n        return (self.W1.size + self.b1.size + \n                self.W2.size + self.b2.size)\n\n    def forward(self, X):\n        \"\"\"\n        Forward propagation\n\n        Args:\n            X: input data (n_samples, n_features)\n\n        Returns:\n            output: predictions (n_samples, n_outputs)\n        \"\"\"\n        # Hidden layer\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = sigmoid(self.z1)\n\n        # Output layer\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = sigmoid(self.z2)\n\n        return self.a2\n\n    def __repr__(self):\n        return (f\"NeuralNetwork(\\n\"\n                f\"  W1: {self.W1.shape},\\n\"\n                f\"  b1: {self.b1.shape},\\n\"\n                f\"  W2: {self.W2.shape},\\n\"\n                f\"  b2: {self.b2.shape}\\n\"\n                f\")\")\n\n# Test the network\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING NEURAL NETWORK CLASS\")\nprint(\"=\"*60)\n\n# Create network for XOR problem (2 inputs, 2 hidden, 1 output)\nnn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\nprint(f\"\\n{nn}\")\n\n# Test forward propagation\nX_test = np.array([[0, 0],\n                   [0, 1],\n                   [1, 0],\n                   [1, 1]])\n\nprint(\"\\nForward propagation test (random weights):\")\npredictions = nn.forward(X_test)\n\nfor i, (x, pred) in enumerate(zip(X_test, predictions)):\n    print(f\"Input: {x} \u2192 Output: {pred[0]:.4f}\")\n\nprint(\"\\nNote: These are random outputs since we haven't trained yet!\")\n</code></pre> <p>Expected: Network runs without errors, produces random outputs</p>"},{"location":"Week2_Day6/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week2_Day6/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: Why Neural Networks Can Learn Almost Anything by 3Blue1Brown-style (15 min)</p> <p>\u2610 Review: Replay key sections from morning videos as needed (15 min)</p>"},{"location":"Week2_Day6/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week2_Day6/#exercise-4-understanding-network-capacity-50-min","title":"Exercise 4: Understanding Network Capacity (50 min)","text":"<p>Explore how hidden layer size affects learning capacity:</p> <pre><code>def visualize_network_capacity():\n    \"\"\"\n    Create networks of different sizes and visualize their decision boundaries\n    \"\"\"\n    # Generate simple 2D classification data\n    np.random.seed(42)\n    n_samples = 200\n\n    # Create two circular clusters\n    theta = np.random.uniform(0, 2*np.pi, n_samples//2)\n    r1 = np.random.normal(1, 0.1, n_samples//2)\n    r2 = np.random.normal(2, 0.1, n_samples//2)\n\n    X_class0 = np.column_stack([r1 * np.cos(theta), r1 * np.sin(theta)])\n    X_class1 = np.column_stack([r2 * np.cos(theta), r2 * np.sin(theta)])\n\n    X = np.vstack([X_class0, X_class1])\n    y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)]).reshape(-1, 1)\n\n    # Try different hidden layer sizes\n    hidden_sizes = [2, 5, 10, 20]\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    axes = axes.flatten()\n\n    for idx, hidden_size in enumerate(hidden_sizes):\n        # Create network\n        nn = NeuralNetwork(input_size=2, hidden_size=hidden_size, output_size=1)\n\n        # Create decision boundary plot\n        h = 0.1\n        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n\n        # Get predictions for mesh\n        mesh_predictions = nn.forward(np.c_[xx.ravel(), yy.ravel()])\n        mesh_predictions = mesh_predictions.reshape(xx.shape)\n\n        # Plot\n        axes[idx].contourf(xx, yy, mesh_predictions, alpha=0.4, cmap='RdYlBu', levels=20)\n        axes[idx].scatter(X[y.flatten()==0, 0], X[y.flatten()==0, 1], \n                         c='red', edgecolors='k', label='Class 0', alpha=0.6)\n        axes[idx].scatter(X[y.flatten()==1, 0], X[y.flatten()==1, 1],\n                         c='blue', edgecolors='k', label='Class 1', alpha=0.6)\n        axes[idx].set_title(f'Hidden Size = {hidden_size} ({nn.count_parameters()} params)')\n        axes[idx].set_xlabel('Feature 1')\n        axes[idx].set_ylabel('Feature 2')\n        axes[idx].legend()\n        axes[idx].grid(True, alpha=0.3)\n\n    plt.suptitle('Network Capacity: Decision Boundaries with Random Weights', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nObservations:\")\n    print(\"- Larger networks (more parameters) have more flexible decision boundaries\")\n    print(\"- Even untrained, you can see complexity differences\")\n    print(\"- With training, larger networks can fit more complex patterns\")\n    print(\"- But too large = overfitting risk!\")\n\nvisualize_network_capacity()\n</code></pre>"},{"location":"Week2_Day6/#exercise-5-xor-problem-the-classic-test-60-min","title":"Exercise 5: XOR Problem - The Classic Test (60 min)","text":"<p>Implement the XOR problem to test your network:</p> <pre><code>print(\"=\" * 60)\nprint(\"XOR PROBLEM - The Neural Network Classic\")\nprint(\"=\" * 60)\n\n# XOR truth table\nX_xor = np.array([[0, 0],\n                  [0, 1],\n                  [1, 0],\n                  [1, 1]])\ny_xor = np.array([[0],\n                  [1],\n                  [1],\n                  [0]])\n\nprint(\"\\nXOR Truth Table:\")\nprint(\"Input  | Target\")\nprint(\"-------|-------\")\nfor x, y in zip(X_xor, y_xor):\n    print(f\"{x}  |   {y[0]}\")\n\n# Why XOR is important\nprint(\"\\nWhy XOR matters:\")\nprint(\"- XOR is NOT linearly separable\")\nprint(\"- Single perceptron CANNOT solve it\")\nprint(\"- Need hidden layer (non-linearity) to solve\")\nprint(\"- Historically significant (ended first AI winter)\")\n\n# Visualize XOR\nplt.figure(figsize=(8, 6))\ncolors = ['red' if y[0] == 0 else 'blue' for y in y_xor]\nplt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='k', linewidth=2)\n\nfor i, (x, y) in enumerate(zip(X_xor, y_xor)):\n    plt.annotate(f'{x} \u2192 {y[0]}', xy=x, xytext=(5, 5), textcoords='offset points')\n\nplt.xlabel('Input 1')\nplt.ylabel('Input 2')\nplt.title('XOR Problem Visualization')\nplt.grid(True, alpha=0.3)\nplt.xlim(-0.5, 1.5)\nplt.ylim(-0.5, 1.5)\nplt.show()\n\n# Test with our neural network (untrained)\nnn_xor = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n\nprint(\"\\nUntrained network predictions on XOR:\")\npredictions = nn_xor.forward(X_xor)\n\nfor x, y_true, y_pred in zip(X_xor, y_xor, predictions):\n    print(f\"Input: {x} | Target: {y_true[0]} | Prediction: {y_pred[0]:.4f}\")\n\nprint(\"\\nThese are random because we haven't trained yet!\")\nprint(\"Tomorrow (Day 7) we'll implement backpropagation to train this network!\")\n</code></pre>"},{"location":"Week2_Day6/#mini-challenge-network-visualization-tool-90-min","title":"Mini-Challenge: Network Visualization Tool (90 min)","text":"<p>Create comprehensive visualizations for neural networks:</p> <pre><code>def visualize_network_architecture(nn, title=\"Neural Network Architecture\"):\n    \"\"\"\n    Visualize network architecture with weights\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n    # 1. Architecture diagram (simplified - you can elaborate)\n    ax = axes[0]\n    ax.text(0.1, 0.9, 'Input\\nLayer', ha='center', va='center', fontsize=12, \n            bbox=dict(boxstyle='round', facecolor='lightblue'))\n    ax.text(0.5, 0.9, 'Hidden\\nLayer', ha='center', va='center', fontsize=12,\n            bbox=dict(boxstyle='round', facecolor='lightgreen'))\n    ax.text(0.9, 0.9, 'Output\\nLayer', ha='center', va='center', fontsize=12,\n            bbox=dict(boxstyle='round', facecolor='lightcoral'))\n\n    ax.annotate('', xy=(0.45, 0.9), xytext=(0.15, 0.9),\n                arrowprops=dict(arrowstyle='-&gt;', lw=2))\n    ax.annotate('', xy=(0.85, 0.9), xytext=(0.55, 0.9),\n                arrowprops=dict(arrowstyle='-&gt;', lw=2))\n\n    ax.text(0.5, 0.5, f'Parameters: {nn.count_parameters()}', ha='center', fontsize=11)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.set_title('Network Structure')\n\n    # 2. Weight matrix W1 (input to hidden)\n    ax = axes[1]\n    im = ax.imshow(nn.W1, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n    ax.set_title('W1: Input \u2192 Hidden Weights')\n    ax.set_xlabel('Hidden Neurons')\n    ax.set_ylabel('Input Features')\n    plt.colorbar(im, ax=ax)\n\n    # 3. Weight matrix W2 (hidden to output)\n    ax = axes[2]\n    im = ax.imshow(nn.W2, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n    ax.set_title('W2: Hidden \u2192 Output Weights')\n    ax.set_xlabel('Output Neurons')\n    ax.set_ylabel('Hidden Neurons')\n    plt.colorbar(im, ax=ax)\n\n    plt.suptitle(title, fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# Test the visualization\nnn_vis = NeuralNetwork(input_size=4, hidden_size=6, output_size=2)\nvisualize_network_architecture(nn_vis, \"Example Neural Network\")\n\ndef visualize_activations(nn, X, title=\"Activation Flow\"):\n    \"\"\"\n    Visualize how activations flow through the network\n    \"\"\"\n    # Get activations\n    output = nn.forward(X)\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n    # Input\n    ax = axes[0]\n    ax.imshow(X.T, cmap='viridis', aspect='auto')\n    ax.set_title(f'Input\\n({X.shape[0]} samples, {X.shape[1]} features)')\n    ax.set_xlabel('Sample')\n    ax.set_ylabel('Feature')\n    plt.colorbar(ax.images[0], ax=ax)\n\n    # Hidden activations\n    ax = axes[1]\n    ax.imshow(nn.a1.T, cmap='viridis', aspect='auto')\n    ax.set_title(f'Hidden Layer Activations\\n({nn.a1.shape[1]} neurons)')\n    ax.set_xlabel('Sample')\n    ax.set_ylabel('Neuron')\n    plt.colorbar(ax.images[0], ax=ax)\n\n    # Output\n    ax = axes[2]\n    ax.imshow(output.T, cmap='viridis', aspect='auto')\n    ax.set_title(f'Output\\n({output.shape[1]} values)')\n    ax.set_xlabel('Sample')\n    ax.set_ylabel('Output')\n    plt.colorbar(ax.images[0], ax=ax)\n\n    plt.suptitle(title, fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# Test activation visualization\nX_test = np.random.randn(10, 4)\nvisualize_activations(nn_vis, X_test, \"Activation Flow Through Network\")\n</code></pre>"},{"location":"Week2_Day6/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review forward propagation steps thoroughly \u2610 Ensure you understand activation functions \u2610 Write daily reflection (choose 2-3 prompts below) \u2610 List questions for Wednesday check-in</p>"},{"location":"Week2_Day6/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do activation functions enable neural networks to learn complex patterns?</li> <li>What is the purpose of having multiple layers?</li> <li>Why can't a single-layer network solve XOR?</li> <li>What surprised you about forward propagation?</li> <li>What questions do you still have about neural networks?</li> </ul> <p>Next: Day 7 - Backpropagation and Training</p>"},{"location":"Week2_Day7/","title":"Week 2, Day 7: Backpropagation and Training","text":""},{"location":"Week2_Day7/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand backpropagation and the chain rule</li> <li>Compute gradients manually for simple networks</li> <li>Implement backpropagation from scratch</li> <li>Build complete training loop with gradient descent</li> <li>Successfully train XOR network</li> <li>Visualize learning process</li> </ul>"},{"location":"Week2_Day7/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week2_Day7/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week2_Day7/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: What is backpropagation really doing? by 3Blue1Brown (14 min) THE essential video for understanding backpropagation intuitively</p> <p>\u2610 Watch: Backpropagation calculus by 3Blue1Brown (10 min) Mathematical details - watch after the intuition video</p> <p>\u2610 Watch: Backpropagation main ideas by StatQuest (14 min) Different perspective, reinforces concepts</p> <p>\u2610 Watch: Chain Rule by StatQuest (18 min) Foundation for backpropagation mathematics</p> <p>\u2610 Watch: Backpropagation Details Pt 1 by StatQuest (13 min)</p> <p>\u2610 Watch: Backpropagation Details Pt 2 by StatQuest (11 min)</p>"},{"location":"Week2_Day7/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 5.3 - Forward and Backward Propagation</p> <p>\u2610 Optional: Michael Nielsen Chapter 2 - Backpropagation details</p>"},{"location":"Week2_Day7/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week2_Day7/#exercise-1-manual-gradient-calculation-45-min","title":"Exercise 1: Manual Gradient Calculation (45 min)","text":"<p>Work through backpropagation by hand to build intuition:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Activation functions from Day 6\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\nprint(\"=\"*70)\nprint(\"MANUAL BACKPROPAGATION CALCULATION\")\nprint(\"=\"*70)\n\n# Tiny network: 1 input -&gt; 1 hidden -&gt; 1 output\nprint(\"\\nNetwork: 1 input \u2192 1 hidden neuron \u2192 1 output\")\n\n# Example data\nx = 0.5\ny_true = 0.8\n\n# Weights\nw1 = 0.4\nb1 = 0.1\nw2 = 0.6\nb2 = 0.2\n\nprint(f\"\\nInput: x = {x}\")\nprint(f\"Target: y = {y_true}\")\nprint(f\"\\nWeights: w1={w1}, b1={b1}, w2={w2}, b2={b2}\")\n\n# Forward pass\nprint(\"\\n\" + \"=\"*70)\nprint(\"FORWARD PASS\")\nprint(\"=\"*70)\n\nz1 = w1 * x + b1\na1 = sigmoid(z1)\nprint(f\"\\nHidden layer:\")\nprint(f\"  z1 = w1*x + b1 = {w1}*{x} + {b1} = {z1}\")\nprint(f\"  a1 = sigmoid(z1) = {a1:.4f}\")\n\nz2 = w2 * a1 + b2\na2 = sigmoid(z2)\nprint(f\"\\nOutput layer:\")\nprint(f\"  z2 = w2*a1 + b2 = {w2}*{a1:.4f} + {b2} = {z2:.4f}\")\nprint(f\"  a2 = sigmoid(z2) = {a2:.4f}\")\n\n# Loss (MSE)\nloss = 0.5 * (y_true - a2)**2\nprint(f\"\\nLoss = 0.5*(y_true - a2)\u00b2 = 0.5*({y_true} - {a2:.4f})\u00b2 = {loss:.4f}\")\n\n# Backward pass\nprint(\"\\n\" + \"=\"*70)\nprint(\"BACKWARD PASS (Computing Gradients)\")\nprint(\"=\"*70)\n\n# Output layer gradients\nprint(\"\\nOutput layer:\")\ndL_da2 = -(y_true - a2)\nprint(f\"  \u2202L/\u2202a2 = -(y_true - a2) = {dL_da2:.4f}\")\n\nda2_dz2 = sigmoid_derivative(z2)\nprint(f\"  \u2202a2/\u2202z2 = sigmoid'(z2) = {da2_dz2:.4f}\")\n\ndL_dz2 = dL_da2 * da2_dz2  # Chain rule!\nprint(f\"  \u2202L/\u2202z2 = \u2202L/\u2202a2 * \u2202a2/\u2202z2 = {dL_dz2:.4f}\")\n\ndL_dw2 = dL_dz2 * a1\nprint(f\"  \u2202L/\u2202w2 = \u2202L/\u2202z2 * a1 = {dL_dw2:.4f}\")\n\ndL_db2 = dL_dz2\nprint(f\"  \u2202L/\u2202b2 = \u2202L/\u2202z2 = {dL_db2:.4f}\")\n\n# Hidden layer gradients\nprint(\"\\nHidden layer:\")\ndL_da1 = dL_dz2 * w2\nprint(f\"  \u2202L/\u2202a1 = \u2202L/\u2202z2 * w2 = {dL_da1:.4f}\")\n\nda1_dz1 = sigmoid_derivative(z1)\nprint(f\"  \u2202a1/\u2202z1 = sigmoid'(z1) = {da1_dz1:.4f}\")\n\ndL_dz1 = dL_da1 * da1_dz1  # Chain rule again!\nprint(f\"  \u2202L/\u2202z1 = \u2202L/\u2202a1 * \u2202a1/\u2202z1 = {dL_dz1:.4f}\")\n\ndL_dw1 = dL_dz1 * x\nprint(f\"  \u2202L/\u2202w1 = \u2202L/\u2202z1 * x = {dL_dw1:.4f}\")\n\ndL_db1 = dL_dz1\nprint(f\"  \u2202L/\u2202b1 = \u2202L/\u2202z1 = {dL_db1:.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"GRADIENT SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\u2202L/\u2202w2 = {dL_dw2:.6f}\")\nprint(f\"\u2202L/\u2202b2 = {dL_db2:.6f}\")\nprint(f\"\u2202L/\u2202w1 = {dL_dw1:.6f}\")\nprint(f\"\u2202L/\u2202b1 = {dL_db1:.6f}\")\n\n# Update weights with gradient descent\nlearning_rate = 0.5\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"WEIGHT UPDATE (learning_rate = {learning_rate})\")\nprint(\"=\"*70)\n\nw2_new = w2 - learning_rate * dL_dw2\nb2_new = b2 - learning_rate * dL_db2\nw1_new = w1 - learning_rate * dL_dw1\nb1_new = b1 - learning_rate * dL_db1\n\nprint(f\"w2: {w2:.4f} \u2192 {w2_new:.4f} (change: {w2_new - w2:.4f})\")\nprint(f\"b2: {b2:.4f} \u2192 {b2_new:.4f} (change: {b2_new - b2:.4f})\")\nprint(f\"w1: {w1:.4f} \u2192 {w1_new:.4f} (change: {w1_new - w1:.4f})\")\nprint(f\"b1: {b1:.4f} \u2192 {b1_new:.4f} (change: {b1_new - b1:.4f})\")\n\n# Verify with new forward pass\nz1_new = w1_new * x + b1_new\na1_new = sigmoid(z1_new)\nz2_new = w2_new * a1_new + b2_new\na2_new = sigmoid(z2_new)\nloss_new = 0.5 * (y_true - a2_new)**2\n\nprint(f\"\\nPrediction: {a2:.4f} \u2192 {a2_new:.4f} (closer to {y_true})\")\nprint(f\"Loss: {loss:.4f} \u2192 {loss_new:.4f} (decreased by {loss - loss_new:.4f})\")\nprint(\"\\n\u2705 Backpropagation worked! Loss decreased.\")\n</code></pre>"},{"location":"Week2_Day7/#exercise-2-implement-backpropagation-in-neural-network-class-75-min","title":"Exercise 2: Implement Backpropagation in Neural Network Class (75 min)","text":"<p>Add backpropagation to yesterday's NeuralNetwork class:</p> <pre><code>class NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n        # Initialize weights\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n\n        self.learning_rate = learning_rate\n\n    def forward(self, X):\n        \"\"\"Forward propagation\"\"\"\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = sigmoid(self.z1)\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y):\n        \"\"\"\n        Backpropagation - compute gradients\n\n        Args:\n            X: inputs (n_samples, n_features)\n            y: targets (n_samples, n_outputs)\n        \"\"\"\n        m = X.shape[0]  # number of samples\n\n        # Output layer gradients\n        dz2 = self.a2 - y  # derivative of sigmoid + MSE\n        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n\n        # Hidden layer gradients\n        da1 = np.dot(dz2, self.W2.T)\n        dz1 = da1 * sigmoid_derivative(self.z1)\n        dW1 = (1/m) * np.dot(X.T, dz1)\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n\n        # Store gradients\n        self.gradients = {\n            'dW2': dW2, 'db2': db2,\n            'dW1': dW1, 'db1': db1\n        }\n\n        return self.gradients\n\n    def update_weights(self):\n        \"\"\"Update weights using computed gradients\"\"\"\n        self.W2 -= self.learning_rate * self.gradients['dW2']\n        self.b2 -= self.learning_rate * self.gradients['db2']\n        self.W1 -= self.learning_rate * self.gradients['dW1']\n        self.b1 -= self.learning_rate * self.gradients['db1']\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Mean Squared Error loss\"\"\"\n        return np.mean((y_true - y_pred) ** 2)\n\n    def train_step(self, X, y):\n        \"\"\"Single training step: forward, backward, update\"\"\"\n        # Forward\n        y_pred = self.forward(X)\n\n        # Compute loss\n        loss = self.compute_loss(y, y_pred)\n\n        # Backward\n        self.backward(X, y)\n\n        # Update\n        self.update_weights()\n\n        return loss\n\n# Test the implementation\nprint(\"\\n\" + \"=\"*70)\nprint(\"TESTING BACKPROPAGATION IMPLEMENTATION\")\nprint(\"=\"*70)\n\n# Simple test data\nX_test = np.array([[0.5]])\ny_test = np.array([[0.8]])\n\nnn = NeuralNetwork(input_size=1, hidden_size=2, output_size=1, learning_rate=0.5)\n\nprint(\"\\nTraining for 10 steps on single sample:\")\nfor step in range(10):\n    loss = nn.train_step(X_test, y_test)\n    pred = nn.forward(X_test)[0, 0]\n    print(f\"Step {step+1}: Loss = {loss:.6f}, Prediction = {pred:.4f}\")\n\nprint(f\"\\nTarget: {y_test[0,0]}\")\nprint(f\"Final prediction: {pred:.4f}\")\nprint(\"\u2705 Network is learning!\" if loss &lt; 0.01 else \"\u26a0\ufe0f May need more training\")\n</code></pre>"},{"location":"Week2_Day7/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week2_Day7/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week2_Day7/#exercise-3-train-xor-network-60-min","title":"Exercise 3: Train XOR Network (60 min)","text":"<p>Finally solve the XOR problem!</p> <pre><code>print(\"=\"*70)\nprint(\"TRAINING NEURAL NETWORK ON XOR\")\nprint(\"=\"*70)\n\n# XOR data\nX_xor = np.array([[0, 0],\n                  [0, 1],\n                  [1, 0],\n                  [1, 1]])\ny_xor = np.array([[0],\n                  [1],\n                  [1],\n                  [0]])\n\n# Create network\nnn_xor = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n\n# Training loop\nepochs = 5000\nlosses = []\npredictions_history = []\n\nprint(f\"\\nTraining for {epochs} epochs...\")\nfor epoch in range(epochs):\n    loss = nn_xor.train_step(X_xor, y_xor)\n    losses.append(loss)\n\n    if (epoch + 1) % 500 == 0:\n        preds = nn_xor.forward(X_xor)\n        print(f\"Epoch {epoch+1:5d}: Loss = {loss:.6f}\")\n        predictions_history.append(preds.copy())\n\n# Final results\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*70)\n\npredictions = nn_xor.forward(X_xor)\nprint(\"\\nInput | Target | Prediction | Correct?\")\nprint(\"------|--------|------------|----------\")\nfor x, y_true, y_pred in zip(X_xor, y_xor, predictions):\n    correct = \"\u2713\" if abs(y_true[0] - y_pred[0]) &lt; 0.1 else \"\u2717\"\n    print(f\"{x}  |   {y_true[0]}    |   {y_pred[0]:.4f}   |    {correct}\")\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Loss curve\naxes[0, 0].plot(losses)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_yscale('log')\n\n# Decision boundary\nh = 0.01\nx_min, x_max = -0.5, 1.5\ny_min, y_max = -0.5, 1.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = nn_xor.forward(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[0, 1].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\naxes[0, 1].scatter(X_xor[y_xor.flatten()==0, 0], X_xor[y_xor.flatten()==0, 1],\n                  c='red', s=200, edgecolors='k', linewidth=2, label='Class 0')\naxes[0, 1].scatter(X_xor[y_xor.flatten()==1, 0], X_xor[y_xor.flatten()==1, 1],\n                  c='blue', s=200, edgecolors='k', linewidth=2, label='Class 1')\naxes[0, 1].set_xlabel('Input 1')\naxes[0, 1].set_ylabel('Input 2')\naxes[0, 1].set_title('Learned Decision Boundary')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Weight matrices\nim = axes[1, 0].imshow(nn_xor.W1, cmap='RdBu', aspect='auto', vmin=-5, vmax=5)\naxes[1, 0].set_title('W1: Input \u2192 Hidden Weights')\naxes[1, 0].set_xlabel('Hidden Neurons')\naxes[1, 0].set_ylabel('Input Features')\nplt.colorbar(im, ax=axes[1, 0])\n\nim = axes[1, 1].imshow(nn_xor.W2, cmap='RdBu', aspect='auto', vmin=-5, vmax=5)\naxes[1, 1].set_title('W2: Hidden \u2192 Output Weights')\naxes[1, 1].set_xlabel('Output')\naxes[1, 1].set_ylabel('Hidden Neurons')\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.suptitle('XOR Problem: Successfully Learned!', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83c\udf89 Congratulations! You've solved XOR with backpropagation!\")\n</code></pre>"},{"location":"Week2_Day7/#exercise-4-spiral-dataset-classification-70-min","title":"Exercise 4: Spiral Dataset Classification (70 min)","text":"<p>More challenging non-linear problem:</p> <pre><code>def make_spiral_data(n_samples=300, noise=0.1):\n    \"\"\"Generate spiral dataset\"\"\"\n    n = n_samples // 2\n\n    # Generate spirals\n    theta = np.linspace(0, 4*np.pi, n)\n\n    # Class 0\n    r0 = theta / (2*np.pi)\n    X0 = np.column_stack([r0 * np.cos(theta) + np.random.randn(n) * noise,\n                          r0 * np.sin(theta) + np.random.randn(n) * noise])\n\n    # Class 1\n    theta += np.pi\n    r1 = theta / (2*np.pi)\n    X1 = np.column_stack([r1 * np.cos(theta) + np.random.randn(n) * noise,\n                          r1 * np.sin(theta) + np.random.randn(n) * noise])\n\n    X = np.vstack([X0, X1])\n    y = np.hstack([np.zeros(n), np.ones(n)]).reshape(-1, 1)\n\n    return X, y\n\n# Generate data\nX_spiral, y_spiral = make_spiral_data(n_samples=300, noise=0.2)\n\n# Visualize\nplt.figure(figsize=(8, 6))\nplt.scatter(X_spiral[y_spiral.flatten()==0, 0], X_spiral[y_spiral.flatten()==0, 1],\n           c='red', edgecolors='k', alpha=0.6, label='Class 0')\nplt.scatter(X_spiral[y_spiral.flatten()==1, 0], X_spiral[y_spiral.flatten()==1, 1],\n           c='blue', edgecolors='k', alpha=0.6, label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Spiral Dataset')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Train network\nprint(\"Training on spiral dataset...\")\nnn_spiral = NeuralNetwork(input_size=2, hidden_size=20, output_size=1, learning_rate=0.3)\n\nepochs = 10000\nlosses_spiral = []\n\nfor epoch in range(epochs):\n    loss = nn_spiral.train_step(X_spiral, y_spiral)\n    losses_spiral.append(loss)\n\n    if (epoch + 1) % 1000 == 0:\n        accuracy = np.mean((nn_spiral.forward(X_spiral) &gt; 0.5) == y_spiral)\n        print(f\"Epoch {epoch+1:5d}: Loss = {loss:.6f}, Accuracy = {accuracy:.4f}\")\n\n# Final visualization\nh = 0.02\nx_min, x_max = X_spiral[:, 0].min() - 0.5, X_spiral[:, 0].max() + 0.5\ny_min, y_max = X_spiral[:, 1].min() - 0.5, X_spiral[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = nn_spiral.forward(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses_spiral)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\nplt.scatter(X_spiral[y_spiral.flatten()==0, 0], X_spiral[y_spiral.flatten()==0, 1],\n           c='red', edgecolors='k', alpha=0.7, label='Class 0')\nplt.scatter(X_spiral[y_spiral.flatten()==1, 0], X_spiral[y_spiral.flatten()==1, 1],\n           c='blue', edgecolors='k', alpha=0.7, label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Learned Decision Boundary')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nfinal_accuracy = np.mean((nn_spiral.forward(X_spiral) &gt; 0.5) == y_spiral)\nprint(f\"\\n\ud83c\udfaf Final Accuracy: {final_accuracy:.2%}\")\n</code></pre>"},{"location":"Week2_Day7/#mini-challenge-hyperparameter-exploration-50-min","title":"Mini-Challenge: Hyperparameter Exploration (50 min)","text":"<p>Explore how different settings affect learning:</p> <pre><code># Test different configurations\nconfigs = [\n    {'hidden_size': 2, 'learning_rate': 0.1, 'name': 'Small network, slow learning'},\n    {'hidden_size': 2, 'learning_rate': 1.0, 'name': 'Small network, fast learning'},\n    {'hidden_size': 10, 'learning_rate': 0.1, 'name': 'Large network, slow learning'},\n    {'hidden_size': 10, 'learning_rate': 1.0, 'name': 'Large network, fast learning'},\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\nfor idx, config in enumerate(configs):\n    # Train network\n    nn = NeuralNetwork(input_size=2, hidden_size=config['hidden_size'],\n                       output_size=1, learning_rate=config['learning_rate'])\n\n    losses = []\n    for epoch in range(2000):\n        loss = nn.train_step(X_xor, y_xor)\n        losses.append(loss)\n\n    # Plot\n    axes[idx].plot(losses)\n    axes[idx].set_xlabel('Epoch')\n    axes[idx].set_ylabel('Loss')\n    axes[idx].set_title(config['name'])\n    axes[idx].grid(True, alpha=0.3)\n    axes[idx].set_yscale('log')\n\n    final_loss = losses[-1]\n    axes[idx].text(0.7, 0.9, f'Final: {final_loss:.4f}',\n                   transform=axes[idx].transAxes,\n                   bbox=dict(boxstyle='round', facecolor='wheat'))\n\nplt.suptitle('Hyperparameter Effects on XOR Learning', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observations:\")\nprint(\"- Too small network may not have enough capacity\")\nprint(\"- Too large learning rate can cause instability\")\nprint(\"- Balance between network size and learning rate matters\")\n</code></pre>"},{"location":"Week2_Day7/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review backpropagation algorithm thoroughly \u2610 Ensure you understand the chain rule application \u2610 Write daily reflection (choose 2-3 prompts below) \u2610 Prepare questions for Friday check-in</p>"},{"location":"Week2_Day7/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does backpropagation enable neural networks to learn?</li> <li>What is the role of the chain rule in computing gradients?</li> <li>What surprised you about training neural networks?</li> <li>How did solving XOR feel compared to yesterday's forward propagation?</li> <li>What challenges did you face in implementing backpropagation?</li> <li>What questions do you still have about gradient descent?</li> </ul> <p>Next: Day 8 - Introduction to PyTorch</p>"},{"location":"Week2_Day8/","title":"Week 2, Day 8: Introduction to PyTorch","text":""},{"location":"Week2_Day8/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand PyTorch tensors and operations</li> <li>Learn automatic differentiation with autograd</li> <li>Build neural networks using nn.Module</li> <li>Recreate Day 6's network in PyTorch</li> <li>Compare NumPy vs PyTorch implementations</li> </ul>"},{"location":"Week2_Day8/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week2_Day8/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week2_Day8/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: PyTorch in 100 Seconds by Fireship (3 min) Quick overview of PyTorch</p> <p>\u2610 Watch: PyTorch Tutorial - Neural Networks by freeCodeCamp (25 min) Comprehensive introduction to PyTorch basics</p> <p>\u2610 Watch: PyTorch Tensors by Aladdin Persson (15 min) Deep dive into tensor operations</p> <p>\u2610 Watch: PyTorch Autograd by Aladdin Persson (10 min) Understanding automatic differentiation</p> <p>\u2610 Watch: Building Neural Networks in PyTorch by Python Engineer (20 min) How to use nn.Module</p>"},{"location":"Week2_Day8/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: PyTorch Quickstart</p> <p>\u2610 Read: Tensors Tutorial</p> <p>\u2610 Read: Autograd Tutorial</p>"},{"location":"Week2_Day8/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week2_Day8/#setup-10-min","title":"Setup (10 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n</code></pre>"},{"location":"Week2_Day8/#exercise-1-pytorch-tensors-matching-day-1s-numpy-45-min","title":"Exercise 1: PyTorch Tensors - Matching Day 1's NumPy (45 min)","text":"<p>Learn PyTorch tensors by comparing with NumPy:</p> <pre><code>print(\"=\"*70)\nprint(\"PYTORCH TENSORS vs NUMPY ARRAYS\")\nprint(\"=\"*70)\n\n# Creating tensors\nprint(\"\\n1. Creating tensors\")\nprint(\"-\" * 40)\n\n# NumPy way\nnp_array = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"NumPy array:\\n{np_array}\")\n\n# PyTorch way\ntorch_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(f\"\\nPyTorch tensor:\\n{torch_tensor}\")\n\n# From NumPy\ntorch_from_np = torch.from_numpy(np_array)\nprint(f\"\\nFrom NumPy:\\n{torch_from_np}\")\n\n# To NumPy\nnp_from_torch = torch_tensor.numpy()\nprint(f\"\\nTo NumPy:\\n{np_from_torch}\")\n\n# Different creation methods\nprint(\"\\n2. Initialization methods\")\nprint(\"-\" * 40)\n\nzeros_np = np.zeros((2, 3))\nzeros_torch = torch.zeros(2, 3)\nprint(f\"Zeros - NumPy:\\n{zeros_np}\")\nprint(f\"Zeros - PyTorch:\\n{zeros_torch}\")\n\nones_torch = torch.ones(2, 3)\nrandom_torch = torch.randn(2, 3)  # Normal distribution\nprint(f\"\\nOnes:\\n{ones_torch}\")\nprint(f\"\\nRandom:\\n{random_torch}\")\n\n# Operations\nprint(\"\\n3. Basic operations\")\nprint(\"-\" * 40)\n\na = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\nb = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n\nprint(f\"a:\\n{a}\")\nprint(f\"\\nb:\\n{b}\")\nprint(f\"\\na + b:\\n{a + b}\")\nprint(f\"\\na * b (element-wise):\\n{a * b}\")\nprint(f\"\\na @ b (matrix multiply):\\n{a @ b}\")\n\n# More operations\nprint(f\"\\nMean: {a.mean()}\")\nprint(f\"Sum: {a.sum()}\")\nprint(f\"Max: {a.max()}\")\nprint(f\"Transpose:\\n{a.t()}\")\n\n# Reshaping\nprint(\"\\n4. Reshaping\")\nprint(\"-\" * 40)\n\nx = torch.arange(12)\nprint(f\"Original: {x}\")\nprint(f\"Shape: {x.shape}\")\n\nx_reshaped = x.view(3, 4)\nprint(f\"\\nReshaped (3, 4):\\n{x_reshaped}\")\n\nx_reshaped2 = x.view(2, 6)\nprint(f\"\\nReshaped (2, 6):\\n{x_reshaped2}\")\n\n# Indexing (similar to NumPy)\nprint(\"\\n5. Indexing and slicing\")\nprint(\"-\" * 40)\n\nmatrix = torch.arange(20).view(4, 5)\nprint(f\"Matrix:\\n{matrix}\")\nprint(f\"\\nFirst row: {matrix[0]}\")\nprint(f\"First column: {matrix[:, 0]}\")\nprint(f\"Submatrix:\\n{matrix[1:3, 2:4]}\")\n\n# Key difference: requires_grad\nprint(\"\\n6. Gradient tracking\")\nprint(\"-\" * 40)\n\nx = torch.tensor([2.0, 3.0], requires_grad=True)\nprint(f\"Tensor with gradient tracking: {x}\")\nprint(f\"requires_grad: {x.requires_grad}\")\n</code></pre>"},{"location":"Week2_Day8/#exercise-2-automatic-differentiation-with-autograd-45-min","title":"Exercise 2: Automatic Differentiation with Autograd (45 min)","text":"<p>Understand PyTorch's autograd system:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"AUTOMATIC DIFFERENTIATION - The Magic of PyTorch\")\nprint(\"=\"*70)\n\n# Simple example\nprint(\"\\n1. Basic autograd example\")\nprint(\"-\" * 40)\n\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2 + 3 * x + 1\n\nprint(f\"x = {x.item()}\")\nprint(f\"y = x\u00b2 + 3x + 1 = {y.item()}\")\n\n# Compute gradient\ny.backward()  # dy/dx\nprint(f\"\\ndy/dx = 2x + 3\")\nprint(f\"At x={x.item()}: dy/dx = {x.grad.item()}\")\nprint(f\"Expected: 2*{x.item()} + 3 = {2*x.item() + 3}\")\n\n# More complex example\nprint(\"\\n2. Neural network-like computation\")\nprint(\"-\" * 40)\n\nx = torch.tensor([[1.0, 2.0]], requires_grad=True)\nw = torch.tensor([[0.5], [0.3]], requires_grad=True)\nb = torch.tensor([[0.1]], requires_grad=True)\n\n# Forward pass\nz = x @ w + b  # Linear layer\na = torch.sigmoid(z)  # Activation\nloss = (a - 1.0) ** 2  # Simple loss\n\nprint(f\"Input: {x}\")\nprint(f\"Weights: {w.t()}\")\nprint(f\"Bias: {b}\")\nprint(f\"Output: {a.item():.4f}\")\nprint(f\"Loss: {loss.item():.4f}\")\n\n# Backward pass\nloss.backward()\n\nprint(f\"\\nGradients:\")\nprint(f\"\u2202loss/\u2202w:\\n{w.grad}\")\nprint(f\"\u2202loss/\u2202b: {b.grad}\")\nprint(f\"\u2202loss/\u2202x: {x.grad}\")\n\n# Manual gradient descent\nprint(\"\\n3. Manual weight update\")\nprint(\"-\" * 40)\n\nlearning_rate = 0.1\n\nwith torch.no_grad():  # Don't track these operations\n    w -= learning_rate * w.grad\n    b -= learning_rate * b.grad\n\n    # Zero gradients for next iteration\n    w.grad.zero_()\n    b.grad.zero_()\n\nprint(f\"Updated weights: {w.t()}\")\nprint(f\"Updated bias: {b}\")\n\n# Visualize gradient flow\nprint(\"\\n4. Computational graph visualization\")\nprint(\"-\" * 40)\n\nx = torch.tensor(3.0, requires_grad=True)\na = x * 2\nb = a * 3\nc = b ** 2\nc.backward()\n\nprint(f\"x = {x.item()}\")\nprint(f\"a = x * 2 = {a.item()}\")\nprint(f\"b = a * 3 = {b.item()}\")\nprint(f\"c = b\u00b2 = {c.item()}\")\nprint(f\"\\ndc/dx = {x.grad.item()}\")\nprint(f\"Expected: dc/dx = 2b * 3 * 2 = {2*b.item()*3*2}\")\n</code></pre>"},{"location":"Week2_Day8/#exercise-3-building-with-nnmodule-30-min","title":"Exercise 3: Building with nn.Module (30 min)","text":"<p>Learn PyTorch's way of building networks:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"BUILDING NETWORKS WITH nn.Module\")\nprint(\"=\"*70)\n\n# Simple network class\nclass SimpleNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNetwork, self).__init__()\n\n        # Define layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Define forward pass\n        x = self.fc1(x)\n        x = self.sigmoid(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# Create network\nmodel = SimpleNetwork(input_size=2, hidden_size=4, output_size=1)\n\nprint(\"Network architecture:\")\nprint(model)\n\nprint(\"\\nParameters:\")\nfor name, param in model.named_parameters():\n    print(f\"{name}: {param.shape}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params}\")\n\n# Test forward pass\nx_test = torch.tensor([[0.5, 0.8]])\noutput = model(x_test)\nprint(f\"\\nTest input: {x_test}\")\nprint(f\"Output: {output.item():.4f}\")\n</code></pre>"},{"location":"Week2_Day8/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week2_Day8/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week2_Day8/#exercise-4-recreate-day-6s-network-in-pytorch-60-min","title":"Exercise 4: Recreate Day 6's Network in PyTorch (60 min)","text":"<p>Build the same network from Day 6, but in PyTorch:</p> <pre><code>print(\"=\"*70)\nprint(\"IMPLEMENTING DAY 6's NETWORK IN PYTORCH\")\nprint(\"=\"*70)\n\n# XOR data in PyTorch\nX_xor = torch.tensor([[0, 0],\n                      [0, 1],\n                      [1, 0],\n                      [1, 1]], dtype=torch.float32)\ny_xor = torch.tensor([[0],\n                      [1],\n                      [1],\n                      [0]], dtype=torch.float32)\n\n# Network definition\nclass XORNetwork(nn.Module):\n    def __init__(self):\n        super(XORNetwork, self).__init__()\n        self.fc1 = nn.Linear(2, 4)  # 2 inputs, 4 hidden\n        self.fc2 = nn.Linear(4, 1)  # 4 hidden, 1 output\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n\n# Create model, loss function, optimizer\nmodel = XORNetwork()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.5)\n\nprint(\"Training XOR network in PyTorch...\")\nprint(f\"Model: {model}\")\n\n# Training loop\nepochs = 5000\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X_xor)\n    loss = criterion(outputs, y_xor)\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear gradients\n    loss.backward()         # Compute gradients\n    optimizer.step()        # Update weights\n\n    losses.append(loss.item())\n\n    if (epoch + 1) % 500 == 0:\n        print(f\"Epoch {epoch+1:5d}: Loss = {loss.item():.6f}\")\n\n# Test the model\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS\")\nprint(\"=\"*70)\n\nwith torch.no_grad():  # Don't track gradients during inference\n    predictions = model(X_xor)\n\nprint(\"\\nInput | Target | Prediction | Correct?\")\nprint(\"------|--------|------------|----------\")\nfor x, y_true, y_pred in zip(X_xor, y_xor, predictions):\n    correct = \"\u2713\" if abs(y_true.item() - y_pred.item()) &lt; 0.1 else \"\u2717\"\n    print(f\"{x.numpy()}  |   {y_true.item()}    |   {y_pred.item():.4f}   |    {correct}\")\n\n# Visualize\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('PyTorch Training Loss')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\n\n# Decision boundary\nh = 0.01\nx_min, x_max = -0.5, 1.5\ny_min, y_max = -0.5, 1.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\ngrid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\nwith torch.no_grad():\n    Z = model(grid).numpy()\nZ = Z.reshape(xx.shape)\n\nplt.subplot(1, 2, 2)\nplt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\nplt.scatter(X_xor[y_xor.flatten()==0, 0], X_xor[y_xor.flatten()==0, 1],\n           c='red', s=200, edgecolors='k', linewidth=2)\nplt.scatter(X_xor[y_xor.flatten()==1, 0], X_xor[y_xor.flatten()==1, 1],\n           c='blue', s=200, edgecolors='k', linewidth=2)\nplt.xlabel('Input 1')\nplt.ylabel('Input 2')\nplt.title('PyTorch Decision Boundary')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2705 PyTorch implementation complete!\")\n</code></pre>"},{"location":"Week2_Day8/#exercise-5-compare-numpy-vs-pytorch-50-min","title":"Exercise 5: Compare NumPy vs PyTorch (50 min)","text":"<p>Side-by-side comparison of implementations:</p> <pre><code>print(\"=\"*70)\nprint(\"NUMPY vs PYTORCH COMPARISON\")\nprint(\"=\"*70)\n\n# Let's compare training time and ease of use\nimport time\n\n# NumPy implementation (from Day 7)\nclass NumpyNN:\n    def __init__(self):\n        self.W1 = np.random.randn(2, 4) * 0.01\n        self.b1 = np.zeros((1, 4))\n        self.W2 = np.random.randn(4, 1) * 0.01\n        self.b2 = np.zeros((1, 1))\n        self.lr = 0.5\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, X):\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = self.sigmoid(self.z1)\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = self.sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y):\n        m = X.shape[0]\n        dz2 = self.a2 - y\n        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n\n        da1 = np.dot(dz2, self.W2.T)\n        dz1 = da1 * self.a1 * (1 - self.a1)\n        dW1 = (1/m) * np.dot(X.T, dz1)\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n\n# Training comparison\nX_np = X_xor.numpy()\ny_np = y_xor.numpy()\n\n# NumPy\nprint(\"\\nTraining with NumPy...\")\nnp_nn = NumpyNN()\nstart = time.time()\nfor _ in range(5000):\n    np_nn.forward(X_np)\n    np_nn.backward(X_np, y_np)\nnumpy_time = time.time() - start\n\n# PyTorch\nprint(\"Training with PyTorch...\")\ntorch_nn = XORNetwork()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(torch_nn.parameters(), lr=0.5)\n\nstart = time.time()\nfor _ in range(5000):\n    outputs = torch_nn(X_xor)\n    loss = criterion(outputs, y_xor)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\npytorch_time = time.time() - start\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON RESULTS\")\nprint(\"=\"*70)\n\nprint(f\"\\nTraining Time:\")\nprint(f\"NumPy:   {numpy_time:.3f} seconds\")\nprint(f\"PyTorch: {pytorch_time:.3f} seconds\")\nprint(f\"Speedup: {numpy_time/pytorch_time:.2f}x\")\n\nprint(f\"\\nCode Complexity:\")\nprint(f\"NumPy:   ~50 lines (manual backprop)\")\nprint(f\"PyTorch: ~15 lines (automatic backprop)\")\n\nprint(f\"\\nAdvantages:\")\nprint(\"\\nNumPy:\")\nprint(\"  + Full control over implementation\")\nprint(\"  + Educational - see every detail\")\nprint(\"  + No dependencies beyond NumPy\")\nprint(\"  - Manual gradient computation\")\nprint(\"  - Error-prone\")\nprint(\"  - No GPU support\")\n\nprint(\"\\nPyTorch:\")\nprint(\"  + Automatic differentiation\")\nprint(\"  + Less code, fewer bugs\")\nprint(\"  + GPU acceleration available\")\nprint(\"  + Production-ready\")\nprint(\"  + Large ecosystem\")\nprint(\"  - Abstraction hides details\")\n\nprint(\"\\n\ud83d\udca1 Recommendation: Learn with NumPy, build with PyTorch!\")\n</code></pre>"},{"location":"Week2_Day8/#mini-challenge-advanced-pytorch-features-70-min","title":"Mini-Challenge: Advanced PyTorch Features (70 min)","text":"<p>Explore more PyTorch capabilities:</p> <pre><code>print(\"=\"*70)\nprint(\"ADVANCED PYTORCH FEATURES\")\nprint(\"=\"*70)\n\n# 1. Different activation functions\nprint(\"\\n1. Exploring activation functions\")\nprint(\"-\" * 40)\n\nclass FlexibleNetwork(nn.Module):\n    def __init__(self, activation='relu'):\n        super(FlexibleNetwork, self).__init__()\n        self.fc1 = nn.Linear(2, 8)\n        self.fc2 = nn.Linear(8, 1)\n\n        # Choose activation\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        x = torch.sigmoid(x)\n        return x\n\n# Compare activations\nactivations = ['relu', 'tanh', 'sigmoid']\nresults = {}\n\nfor act in activations:\n    model = FlexibleNetwork(activation=act)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n    losses = []\n    for epoch in range(2000):\n        outputs = model(X_xor)\n        loss = criterion(outputs, y_xor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n    results[act] = losses\n    print(f\"{act:10s}: Final loss = {losses[-1]:.6f}\")\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\nfor act, losses in results.items():\n    plt.plot(losses, label=act.upper())\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Activation Function Comparison')\nplt.legend()\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 2. Different optimizers\nprint(\"\\n2. Comparing optimizers\")\nprint(\"-\" * 40)\n\noptimizers_to_test = {\n    'SGD': lambda params: optim.SGD(params, lr=0.1),\n    'Adam': lambda params: optim.Adam(params, lr=0.01),\n    'RMSprop': lambda params: optim.RMSprop(params, lr=0.01),\n}\n\noptimizer_results = {}\n\nfor opt_name, opt_fn in optimizers_to_test.items():\n    model = XORNetwork()\n    criterion = nn.MSELoss()\n    optimizer = opt_fn(model.parameters())\n\n    losses = []\n    for epoch in range(1000):\n        outputs = model(X_xor)\n        loss = criterion(outputs, y_xor)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n    optimizer_results[opt_name] = losses\n    print(f\"{opt_name:10s}: Final loss = {losses[-1]:.6f}\")\n\n# Plot\nplt.figure(figsize=(10, 6))\nfor opt_name, losses in optimizer_results.items():\n    plt.plot(losses, label=opt_name)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Optimizer Comparison')\nplt.legend()\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n\u2705 PyTorch exploration complete!\")\n</code></pre>"},{"location":"Week2_Day8/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review PyTorch fundamentals \u2610 Understand autograd mechanism \u2610 Compare with NumPy implementation from Days 6-7 \u2610 Write daily reflection (choose 2-3 prompts below)</p>"},{"location":"Week2_Day8/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does PyTorch's autograd compare to manual backpropagation?</li> <li>What are the advantages of using PyTorch over NumPy?</li> <li>What surprised you about PyTorch?</li> <li>How confident do you feel about building networks in PyTorch?</li> <li>What questions do you have about PyTorch?</li> </ul> <p>Next: Day 9 - Building Neural Networks in PyTorch</p>"},{"location":"Week2_Day9/","title":"Week 2, Day 9: Building Neural Networks in PyTorch","text":""},{"location":"Week2_Day9/#daily-goals","title":"Daily Goals","text":"<ul> <li>Master PyTorch Dataset and DataLoader</li> <li>Implement dropout and batch normalization</li> <li>Build proper training/validation pipelines</li> <li>Apply early stopping</li> <li>Complete Fashion-MNIST classification</li> <li>Compare different architectures and hyperparameters</li> </ul>"},{"location":"Week2_Day9/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week2_Day9/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week2_Day9/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Build the Neural Network by PyTorch (20 min) Official PyTorch tutorial on building networks</p> <p>\u2610 Watch: PyTorch Dataset and DataLoader by Aladdin Persson (15 min) How to handle data efficiently</p> <p>\u2610 Watch: Batch Normalization by StatQuest (8 min) Understanding batch norm</p> <p>\u2610 Watch: Dropout by StatQuest (8 min) Preventing overfitting with dropout</p> <p>\u2610 Watch: Training Neural Networks review if needed (20 min)</p>"},{"location":"Week2_Day9/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: PyTorch Datasets &amp; DataLoaders</p> <p>\u2610 Read: D2L Chapter 8.4 - Batch Normalization</p> <p>\u2610 Read: D2L Chapter 8.5 - Dropout</p>"},{"location":"Week2_Day9/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week2_Day9/#exercise-1-custom-dataset-and-dataloader-50-min","title":"Exercise 1: Custom Dataset and DataLoader (50 min)","text":"<p>Learn to handle data the PyTorch way:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=\"*70)\nprint(\"PYTORCH DATASET AND DATALOADER\")\nprint(\"=\"*70)\n\n# Custom Dataset class\nclass XORDataset(Dataset):\n    def __init__(self, n_samples=1000):\n        \"\"\"\n        Generate XOR-like data\n\n        Args:\n            n_samples: number of samples to generate\n        \"\"\"\n        # Generate random points\n        X = np.random.randn(n_samples, 2)\n\n        # XOR logic: y = 1 if (x1 &gt; 0) XOR (x2 &gt; 0)\n        y = ((X[:, 0] &gt; 0) != (X[:, 1] &gt; 0)).astype(np.float32)\n\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n\n    def __len__(self):\n        \"\"\"Return the number of samples\"\"\"\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        \"\"\"Get a single sample\"\"\"\n        return self.X[idx], self.y[idx]\n\n# Create dataset\ndataset = XORDataset(n_samples=1000)\nprint(f\"Dataset size: {len(dataset)}\")\n\n# Access single sample\nx_sample, y_sample = dataset[0]\nprint(f\"\\nSample 0:\")\nprint(f\"  X: {x_sample}\")\nprint(f\"  y: {y_sample.item()}\")\n\n# Create DataLoader\nbatch_size = 32\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nprint(f\"\\nDataLoader:\")\nprint(f\"  Batch size: {batch_size}\")\nprint(f\"  Number of batches: {len(dataloader)}\")\n\n# Iterate through batches\nprint(f\"\\nFirst batch:\")\nfor X_batch, y_batch in dataloader:\n    print(f\"  X_batch shape: {X_batch.shape}\")\n    print(f\"  y_batch shape: {y_batch.shape}\")\n    print(f\"  First sample: X={X_batch[0]}, y={y_batch[0].item()}\")\n    break  # Just show first batch\n\n# Visualize dataset\nX_all = dataset.X.numpy()\ny_all = dataset.y.numpy().flatten()\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X_all[y_all==0, 0], X_all[y_all==0, 1], \n           c='red', alpha=0.5, label='Class 0')\nplt.scatter(X_all[y_all==1, 0], X_all[y_all==1, 1],\n           c='blue', alpha=0.5, label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('XOR Dataset')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"Week2_Day9/#exercise-2-dropout-and-batch-normalization-60-min","title":"Exercise 2: Dropout and Batch Normalization (60 min)","text":"<p>Add regularization to prevent overfitting:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"REGULARIZATION TECHNIQUES\")\nprint(\"=\"*70)\n\n# Network with dropout\nclass NetworkWithDropout(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n        super(NetworkWithDropout, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.dropout1 = nn.Dropout(p=dropout_prob)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.dropout2 = nn.Dropout(p=dropout_prob)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\n# Network with batch normalization\nclass NetworkWithBatchNorm(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NetworkWithBatchNorm, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.bn2 = nn.BatchNorm1d(hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\n# Network with both\nclass NetworkWithBoth(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n        super(NetworkWithBoth, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(p=dropout_prob)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.bn2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(p=dropout_prob)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = self.dropout2(x)\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\n# Compare them\nmodels = {\n    'Baseline': nn.Sequential(\n        nn.Linear(2, 64),\n        nn.ReLU(),\n        nn.Linear(64, 64),\n        nn.ReLU(),\n        nn.Linear(64, 1),\n        nn.Sigmoid()\n    ),\n    'Dropout': NetworkWithDropout(2, 64, 1, dropout_prob=0.3),\n    'BatchNorm': NetworkWithBatchNorm(2, 64, 1),\n    'Both': NetworkWithBoth(2, 64, 1, dropout_prob=0.3)\n}\n\n# Training function\ndef train_model(model, train_loader, val_loader, epochs=100):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch in train_loader:\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_losses.append(train_loss / len(train_loader))\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                val_loss += loss.item()\n\n        val_losses.append(val_loss / len(val_loader))\n\n    return train_losses, val_losses\n\n# Split data\ntrain_dataset = XORDataset(n_samples=800)\nval_dataset = XORDataset(n_samples=200)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Train all models\nresults = {}\nprint(\"\\nTraining models...\")\nfor name, model in models.items():\n    print(f\"\\n{name}...\")\n    train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=100)\n    results[name] = {'train': train_losses, 'val': val_losses}\n    print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n    print(f\"  Final val loss: {val_losses[-1]:.4f}\")\n\n# Plot comparison\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor idx, (name, losses) in enumerate(results.items()):\n    axes[idx].plot(losses['train'], label='Train', alpha=0.7)\n    axes[idx].plot(losses['val'], label='Validation', alpha=0.7)\n    axes[idx].set_xlabel('Epoch')\n    axes[idx].set_ylabel('Loss')\n    axes[idx].set_title(f'{name}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.suptitle('Regularization Techniques Comparison', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Observations:\")\nprint(\"- Dropout reduces overfitting during training\")\nprint(\"- Batch norm stabilizes training\")\nprint(\"- Combining both often works best\")\n</code></pre>"},{"location":"Week2_Day9/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week2_Day9/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week2_Day9/#exercise-3-complete-training-pipeline-with-validation-60-min","title":"Exercise 3: Complete Training Pipeline with Validation (60 min)","text":"<p>Build a production-ready training system:</p> <pre><code>from torch.utils.data import random_split\n\nprint(\"=\"*70)\nprint(\"COMPLETE TRAINING PIPELINE\")\nprint(\"=\"*70)\n\n# Create comprehensive training function\ndef train_with_validation(model, train_loader, val_loader, epochs=100, \n                         patience=10, min_delta=0.001):\n    \"\"\"\n    Train model with validation and early stopping\n\n    Args:\n        model: PyTorch model\n        train_loader: training data loader\n        val_loader: validation data loader\n        epochs: maximum number of epochs\n        patience: epochs to wait for improvement\n        min_delta: minimum change to qualify as improvement\n\n    Returns:\n        history: dictionary with training history\n    \"\"\"\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': []\n    }\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for X_batch, y_batch in train_loader:\n            # Forward pass\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Metrics\n            train_loss += loss.item()\n            predictions = (outputs &gt; 0.5).float()\n            train_correct += (predictions == y_batch).sum().item()\n            train_total += y_batch.size(0)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n\n                val_loss += loss.item()\n                predictions = (outputs &gt; 0.5).float()\n                val_correct += (predictions == y_batch).sum().item()\n                val_total += y_batch.size(0)\n\n        # Calculate averages\n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        train_acc = train_correct / train_total\n        val_acc = val_correct / val_total\n\n        history['train_loss'].append(avg_train_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d}/{epochs}: \"\n                  f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n                  f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n        # Early stopping check\n        if avg_val_loss &lt; best_val_loss - min_delta:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            # Save best model\n            best_model_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n\n        if patience_counter &gt;= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n            # Restore best model\n            model.load_state_dict(best_model_state)\n            break\n\n    return history\n\n# Test the pipeline\nmodel = NetworkWithBoth(2, 64, 1, dropout_prob=0.3)\n\nprint(\"Training with early stopping...\")\nhistory = train_with_validation(model, train_loader, val_loader, \n                                 epochs=200, patience=15)\n\n# Visualize results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss\naxes[0].plot(history['train_loss'], label='Train Loss', alpha=0.7)\naxes[0].plot(history['val_loss'], label='Validation Loss', alpha=0.7)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training History - Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy\naxes[1].plot(history['train_acc'], label='Train Accuracy', alpha=0.7)\naxes[1].plot(history['val_acc'], label='Validation Accuracy', alpha=0.7)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training History - Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal validation accuracy: {history['val_acc'][-1]:.4f}\")\n</code></pre>"},{"location":"Week2_Day9/#exercise-4-fashion-mnist-classification-120-min","title":"Exercise 4: Fashion-MNIST Classification (120 min)","text":"<p>Apply everything to a real dataset:</p> <pre><code>from torchvision import datasets, transforms\n\nprint(\"=\"*70)\nprint(\"FASHION-MNIST CLASSIFICATION\")\nprint(\"=\"*70)\n\n# Load Fashion-MNIST\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                                      download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False,\n                                     download=True, transform=transform)\n\n# Split train into train/val\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Visualize samples\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Get some samples\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\naxes = axes.flatten()\n\nfor idx in range(10):\n    axes[idx].imshow(images[idx].squeeze(), cmap='gray')\n    axes[idx].set_title(class_names[labels[idx]])\n    axes[idx].axis('off')\n\nplt.suptitle('Fashion-MNIST Samples')\nplt.tight_layout()\nplt.show()\n\n# Build model\nclass FashionMNISTNet(nn.Module):\n    def __init__(self):\n        super(FashionMNISTNet, self).__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        return x\n\nmodel = FashionMNISTNet()\nprint(f\"\\nModel architecture:\\n{model}\")\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Train model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 20\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\nprint(\"\\nTraining Fashion-MNIST model...\")\nfor epoch in range(epochs):\n    # Training\n    model.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n\n    for images, labels in train_loader:\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        train_correct += (predicted == labels).sum().item()\n        train_total += labels.size(0)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n    # Record metrics\n    history['train_loss'].append(train_loss / len(train_loader))\n    history['val_loss'].append(val_loss / len(val_loader))\n    history['train_acc'].append(train_correct / train_total)\n    history['val_acc'].append(val_correct / val_total)\n\n    print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n          f\"Train Acc: {train_correct/train_total:.4f}, \"\n          f\"Val Acc: {val_correct/val_total:.4f}\")\n\n# Test set evaluation\nmodel.eval()\ntest_correct = 0\ntest_total = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        test_correct += (predicted == labels).sum().item()\n        test_total += labels.size(0)\n\ntest_accuracy = test_correct / test_total\nprint(f\"\\n\ud83c\udfaf Test Accuracy: {test_accuracy:.4f}\")\n\n# Visualize training\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Validation')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(history['train_acc'], label='Train')\naxes[1].plot(history['val_acc'], label='Validation')\naxes[1].axhline(y=test_accuracy, color='r', linestyle='--', label='Test')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2705 Fashion-MNIST training complete!\")\n</code></pre>"},{"location":"Week2_Day9/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review dataset/dataloader patterns \u2610 Understand regularization techniques \u2610 Reflect on training pipeline design \u2610 Write daily reflection (choose 2-3 prompts below)</p>"},{"location":"Week2_Day9/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do DataLoaders improve training efficiency?</li> <li>What is the purpose of dropout and batch normalization?</li> <li>How does early stopping prevent overfitting?</li> <li>What challenges did you face with Fashion-MNIST?</li> <li>How ready do you feel for tomorrow's MNIST project?</li> </ul> <p>Next: Day 10 - MNIST Project</p>"},{"location":"Week2_Overview/","title":"Week 2 Overview: Neural Networks Foundations","text":""},{"location":"Week2_Overview/#introduction","title":"Introduction","text":"<p>Week 2 introduces neural networks - the foundation of modern deep learning. You'll understand how neural networks work from first principles, implement them from scratch, then master PyTorch to build production-ready models.</p>"},{"location":"Week2_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Understand neural network architecture and forward propagation</li> <li>Master backpropagation and the chain rule</li> <li>Implement neural networks from scratch in NumPy</li> <li>Learn PyTorch fundamentals (tensors, autograd, nn.Module)</li> <li>Apply regularization techniques (dropout, batch normalization)</li> <li>Complete MNIST digit classification project achieving &gt;98% accuracy</li> </ul>"},{"location":"Week2_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 6: Neural Network Theory and Forward Propagation</li> <li>Day 7: Backpropagation and Training</li> <li>Day 8: Introduction to PyTorch</li> <li>Day 9: Building Neural Networks in PyTorch</li> <li>Day 10: MNIST Project - Digit Classification</li> </ul>"},{"location":"Week2_Overview/#key-resources","title":"Key Resources","text":"<p>Videos: 3Blue1Brown (primary), StatQuest (supporting), PyTorch tutorials (practical) Text: Dive into Deep Learning Chapter 5, PyTorch documentation</p>"},{"location":"Week2_Overview/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Watch 3Blue1Brown videos actively - pause and predict</li> <li>Implement from scratch before using PyTorch (Days 6-7)</li> <li>Type code, don't copy-paste</li> <li>Debug systematically - check shapes and gradients</li> <li>Complete daily reflections</li> </ul> <p>Ready to start? Begin with Day 6: Neural Network Theory</p>"},{"location":"Week3_Day11/","title":"Week 3, Day 11: CNN Theory - Convolutions, Filters, Feature Maps","text":""},{"location":"Week3_Day11/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand what convolutions are and why they work for images</li> <li>Learn about filters/kernels and feature detection</li> <li>Master stride, padding, and output size calculations</li> <li>Understand pooling operations</li> <li>Implement 2D convolution from scratch</li> <li>Build first CNN in PyTorch</li> <li>Visualize learned features</li> </ul>"},{"location":"Week3_Day11/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week3_Day11/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week3_Day11/#video-learning-2-hours","title":"Video Learning (2 hours)","text":"<p>\u2610 Watch: But what is a convolution? by 3Blue1Brown (20 min) THE essential video for understanding convolutions visually</p> <p>\u2610 Watch: Convolutional Neural Networks (CNNs) explained by deeplizard (15 min) Clear explanation of CNN components</p> <p>\u2610 Watch: CNNs Part 1 - Convolution by StatQuest (20 min) Detailed breakdown of convolution operation</p> <p>\u2610 Watch: CNNs Part 2 - Pooling by StatQuest (15 min) Understanding pooling layers</p> <p>\u2610 Watch: Visualizing Convolutional Networks by Stanford CS231n (20 min) See what CNNs actually learn</p> <p>\u2610 Watch: CNN Architectures by Lex Fridman (15 min) Overview of evolution</p>"},{"location":"Week3_Day11/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 7.1 - From Fully Connected to Convolutional</p> <p>\u2610 Read: D2L Chapter 7.2 - Convolutions for Images</p> <p>\u2610 Read: D2L Chapter 7.3 - Padding and Stride</p> <p>\u2610 Read: D2L Chapter 7.4 - Pooling</p>"},{"location":"Week3_Day11/#hands-on-coding-part-1-15-hours","title":"Hands-on Coding - Part 1 (1.5 hours)","text":""},{"location":"Week3_Day11/#setup-10-min","title":"Setup (10 min)","text":"<pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n# Set random seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Week 3, Day 11: Convolutional Neural Networks\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"Week3_Day11/#exercise-1-understanding-convolution-operation-45-min","title":"Exercise 1: Understanding Convolution Operation (45 min)","text":"<p>Learn convolution through manual calculation and visualization:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 1: MANUAL CONVOLUTION\")\nprint(\"=\"*70)\n\n# Simple 1D convolution first\nprint(\"\\n1. 1D Convolution Example\")\nprint(\"-\" * 40)\n\nsignal_1d = np.array([1, 2, 3, 4, 5])\nkernel_1d = np.array([0, 1, 0.5])\n\nprint(f\"Signal: {signal_1d}\")\nprint(f\"Kernel: {kernel_1d}\")\n\n# Manual convolution\ndef convolve_1d_manual(signal, kernel):\n    \"\"\"Manually compute 1D convolution\"\"\"\n    n = len(signal)\n    k = len(kernel)\n    output_size = n - k + 1\n    output = np.zeros(output_size)\n\n    for i in range(output_size):\n        output[i] = np.sum(signal[i:i+k] * kernel)\n\n    return output\n\nresult_manual = convolve_1d_manual(signal_1d, kernel_1d)\nprint(f\"\\nManual result: {result_manual}\")\n\n# Using scipy\nresult_scipy = signal.correlate(signal_1d, kernel_1d, mode='valid')\nprint(f\"SciPy result:  {result_scipy}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].stem(signal_1d)\naxes[0].set_title('Input Signal')\naxes[0].set_xlabel('Position')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].stem(kernel_1d)\naxes[1].set_title('Kernel/Filter')\naxes[1].set_xlabel('Position')\naxes[1].grid(True, alpha=0.3)\n\naxes[2].stem(result_manual)\naxes[2].set_title('Convolution Output')\naxes[2].set_xlabel('Position')\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('1D Convolution', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# 2D convolution - the image case\nprint(\"\\n2. 2D Convolution (Images)\")\nprint(\"-\" * 40)\n\n# Create simple image\nimage = np.array([\n    [1, 2, 3, 0],\n    [0, 1, 2, 3],\n    [3, 0, 1, 2],\n    [2, 3, 0, 1]\n], dtype=float)\n\n# Edge detection kernel\nedge_kernel = np.array([\n    [-1, -1, -1],\n    [-1,  8, -1],\n    [-1, -1, -1]\n], dtype=float)\n\nprint(f\"Image shape: {image.shape}\")\nprint(f\"Kernel shape: {edge_kernel.shape}\")\n\ndef convolve_2d_manual(image, kernel):\n    \"\"\"\n    Manually compute 2D convolution\n\n    Args:\n        image: 2D array (H, W)\n        kernel: 2D array (K, K)\n\n    Returns:\n        output: 2D array (H-K+1, W-K+1)\n    \"\"\"\n    h, w = image.shape\n    kh, kw = kernel.shape\n\n    output_h = h - kh + 1\n    output_w = w - kw + 1\n    output = np.zeros((output_h, output_w))\n\n    print(f\"\\nOutput size: {output.shape}\")\n    print(\"\\nStep-by-step convolution:\")\n\n    for i in range(output_h):\n        for j in range(output_w):\n            # Extract region\n            region = image[i:i+kh, j:j+kw]\n            # Element-wise multiply and sum\n            output[i, j] = np.sum(region * kernel)\n\n            if i == 0 and j == 0:\n                print(f\"\\nPosition (0, 0):\")\n                print(f\"Region:\\n{region}\")\n                print(f\"Kernel:\\n{kernel}\")\n                print(f\"Element-wise product:\\n{region * kernel}\")\n                print(f\"Sum: {output[i, j]}\")\n\n    return output\n\n# Apply convolution\noutput = convolve_2d_manual(image, edge_kernel)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nim0 = axes[0].imshow(image, cmap='gray')\naxes[0].set_title('Input Image (4\u00d74)')\naxes[0].axis('off')\nplt.colorbar(im0, ax=axes[0])\n\nim1 = axes[1].imshow(edge_kernel, cmap='RdBu', vmin=-1, vmax=8)\naxes[1].set_title('Edge Detection Kernel (3\u00d73)')\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1])\n\nim2 = axes[2].imshow(output, cmap='gray')\naxes[2].set_title('Output Feature Map (2\u00d72)')\naxes[2].axis('off')\nplt.colorbar(im2, ax=axes[2])\n\n# Annotate output with values\nfor i in range(output.shape[0]):\n    for j in range(output.shape[1]):\n        axes[2].text(j, i, f'{output[i,j]:.0f}', \n                    ha='center', va='center', color='red', fontsize=12)\n\nplt.suptitle('2D Convolution Step-by-Step', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Manual convolution complete\")\n</code></pre>"},{"location":"Week3_Day11/#exercise-2-common-image-filters-45-min","title":"Exercise 2: Common Image Filters (45 min)","text":"<p>Explore different filters and their effects:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: IMAGE FILTERS\")\nprint(\"=\"*70)\n\n# Load a sample image (or create one)\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Download sample image\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/300px-Cat03.jpg\"\nresponse = requests.get(url)\nsample_image = Image.open(BytesIO(response.content)).convert('L')  # Grayscale\nsample_image = sample_image.resize((128, 128))\nimg_array = np.array(sample_image, dtype=float) / 255.0\n\nprint(f\"Image shape: {img_array.shape}\")\n\n# Define common filters\nfilters = {\n    'Identity': np.array([[0, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 0]]),\n\n    'Blur': (1/9) * np.array([[1, 1, 1],\n                              [1, 1, 1],\n                              [1, 1, 1]]),\n\n    'Edge Detection': np.array([[-1, -1, -1],\n                               [-1,  8, -1],\n                               [-1, -1, -1]]),\n\n    'Sharpen': np.array([[ 0, -1,  0],\n                        [-1,  5, -1],\n                        [ 0, -1,  0]]),\n\n    'Horizontal Edge': np.array([[-1, -1, -1],\n                                [ 0,  0,  0],\n                                [ 1,  1,  1]]),\n\n    'Vertical Edge': np.array([[-1, 0, 1],\n                              [-1, 0, 1],\n                              [-1, 0, 1]]),\n}\n\n# Apply filters\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\naxes = axes.flatten()\n\n# Original image\naxes[0].imshow(img_array, cmap='gray')\naxes[0].set_title('Original Image', fontsize=12, fontweight='bold')\naxes[0].axis('off')\n\n# Apply each filter\nfor idx, (name, kernel) in enumerate(filters.items(), 1):\n    # Convolve\n    filtered = signal.correlate2d(img_array, kernel, mode='same', boundary='symm')\n\n    axes[idx].imshow(filtered, cmap='gray')\n    axes[idx].set_title(f'{name}', fontsize=12)\n    axes[idx].axis('off')\n\n    # Show kernel in corner\n    axins = axes[idx].inset_axes([0.7, 0.7, 0.25, 0.25])\n    axins.imshow(kernel, cmap='RdBu', vmin=-2, vmax=5)\n    axins.axis('off')\n\n# Hide extra subplots\nfor idx in range(len(filters) + 1, len(axes)):\n    axes[idx].axis('off')\n\nplt.suptitle('Common Image Filters', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Image filters explored\")\n</code></pre>"},{"location":"Week3_Day11/#exercise-3-stride-and-padding-30-min","title":"Exercise 3: Stride and Padding (30 min)","text":"<p>Understand how stride and padding affect output size:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: STRIDE AND PADDING\")\nprint(\"=\"*70)\n\ndef calculate_output_size(input_size, kernel_size, stride, padding):\n    \"\"\"\n    Calculate output size of convolution\n\n    Formula: output_size = floor((input_size + 2*padding - kernel_size) / stride) + 1\n    \"\"\"\n    return ((input_size + 2*padding - kernel_size) // stride) + 1\n\n# Example calculations\nprint(\"\\nOutput Size Calculations:\")\nprint(\"-\" * 60)\nprint(f\"{'Input':&gt;6} | {'Kernel':&gt;6} | {'Stride':&gt;6} | {'Padding':&gt;7} | {'Output':&gt;6}\")\nprint(\"-\" * 60)\n\nconfigs = [\n    (28, 3, 1, 0),  # MNIST with 3\u00d73, no padding\n    (28, 3, 1, 1),  # MNIST with 3\u00d73, padding=1 (same size)\n    (28, 5, 1, 0),  # MNIST with 5\u00d75, no padding\n    (28, 3, 2, 0),  # MNIST with stride=2 (downsampling)\n    (32, 3, 1, 1),  # CIFAR-10 with 3\u00d73, padding=1\n    (224, 7, 2, 3), # ImageNet first layer (ResNet)\n]\n\nfor input_s, kernel_s, stride, padding in configs:\n    output_s = calculate_output_size(input_s, kernel_s, stride, padding)\n    print(f\"{input_s:&gt;6} | {kernel_s:&gt;6} | {stride:&gt;6} | {padding:&gt;7} | {output_s:&gt;6}\")\n\n# Visualize stride effect\nprint(\"\\nVisualizing Stride:\")\nprint(\"-\" * 40)\n\nimage_small = np.random.rand(6, 6)\nkernel_small = np.ones((3, 3)) / 9  # Average filter\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Stride = 1\noutput_s1 = np.zeros((4, 4))\nfor i in range(4):\n    for j in range(4):\n        output_s1[i, j] = np.sum(image_small[i:i+3, j:j+3] * kernel_small)\n\naxes[0].imshow(output_s1, cmap='viridis')\naxes[0].set_title('Stride = 1\\nOutput: 4\u00d74')\naxes[0].axis('off')\n\n# Stride = 2\noutput_s2 = np.zeros((2, 2))\nfor i in range(2):\n    for j in range(2):\n        output_s2[i, j] = np.sum(image_small[i*2:i*2+3, j*2:j*2+3] * kernel_small)\n\naxes[1].imshow(output_s2, cmap='viridis')\naxes[1].set_title('Stride = 2\\nOutput: 2\u00d72')\naxes[1].axis('off')\n\n# Stride = 3\noutput_s3 = np.zeros((2, 2))\ncount = 0\nfor i in range(2):\n    for j in range(2):\n        if i*3+3 &lt;= 6 and j*3+3 &lt;= 6:\n            output_s3[i, j] = np.sum(image_small[i*3:i*3+3, j*3:j*3+3] * kernel_small)\n            count += 1\n\naxes[2].imshow(output_s3, cmap='viridis')\naxes[2].set_title('Stride = 3\\nOutput: 2\u00d72')\naxes[2].axis('off')\n\nplt.suptitle(f'Effect of Stride (Input: 6\u00d76, Kernel: 3\u00d73)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize padding\nprint(\"\\nVisualizing Padding:\")\nprint(\"-\" * 40)\n\nimage_tiny = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]], dtype=float)\n\n# No padding\nimage_nopad = image_tiny\n\n# Padding = 1\nimage_pad1 = np.pad(image_tiny, pad_width=1, mode='constant', constant_values=0)\n\n# Padding = 2\nimage_pad2 = np.pad(image_tiny, pad_width=2, mode='constant', constant_values=0)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(image_nopad, cmap='viridis', vmin=0, vmax=9)\naxes[0].set_title('No Padding\\nSize: 3\u00d73')\nfor i in range(3):\n    for j in range(3):\n        axes[0].text(j, i, f'{image_nopad[i,j]:.0f}', \n                    ha='center', va='center', color='white', fontsize=12)\naxes[0].axis('off')\n\naxes[1].imshow(image_pad1, cmap='viridis', vmin=0, vmax=9)\naxes[1].set_title('Padding = 1\\nSize: 5\u00d75')\naxes[1].axis('off')\n\naxes[2].imshow(image_pad2, cmap='viridis', vmin=0, vmax=9)\naxes[2].set_title('Padding = 2\\nSize: 7\u00d77')\naxes[2].axis('off')\n\nplt.suptitle('Effect of Padding', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key Insights: - Stride &gt; 1: Downsamples the output (reduces spatial dimensions) - Padding: Preserves spatial dimensions and edge information - 'Same' padding: Output size = Input size (when stride=1) - 'Valid' padding: No padding, output shrinks</p>"},{"location":"Week3_Day11/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week3_Day11/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Review: Replay key sections from morning videos as needed</p> <p>\u2610 Watch: Understanding CNNs with practical code review (15 min)</p>"},{"location":"Week3_Day11/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week3_Day11/#exercise-4-pooling-operations-40-min","title":"Exercise 4: Pooling Operations (40 min)","text":"<p>Learn how pooling reduces spatial dimensions:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: POOLING OPERATIONS\")\nprint(\"=\"*70)\n\n# Max pooling\ndef max_pool_2d(image, pool_size=2, stride=2):\n    \"\"\"\n    Apply max pooling\n\n    Args:\n        image: 2D array\n        pool_size: size of pooling window\n        stride: stride for pooling\n\n    Returns:\n        pooled: downsampled array\n    \"\"\"\n    h, w = image.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n\n    pooled = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[i*stride:i*stride+pool_size, \n                          j*stride:j*stride+pool_size]\n            pooled[i, j] = np.max(region)\n\n    return pooled\n\n# Average pooling\ndef avg_pool_2d(image, pool_size=2, stride=2):\n    \"\"\"Apply average pooling\"\"\"\n    h, w = image.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n\n    pooled = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[i*stride:i*stride+pool_size,\n                          j*stride:j*stride+pool_size]\n            pooled[i, j] = np.mean(region)\n\n    return pooled\n\n# Test on image\ntest_image = np.array([\n    [1, 3, 2, 4],\n    [5, 6, 1, 3],\n    [2, 1, 4, 2],\n    [3, 5, 2, 1]\n], dtype=float)\n\nmax_pooled = max_pool_2d(test_image, pool_size=2, stride=2)\navg_pooled = avg_pool_2d(test_image, pool_size=2, stride=2)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nim0 = axes[0].imshow(test_image, cmap='viridis')\naxes[0].set_title('Original (4\u00d74)')\nfor i in range(4):\n    for j in range(4):\n        axes[0].text(j, i, f'{test_image[i,j]:.0f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[0].axis('off')\nplt.colorbar(im0, ax=axes[0])\n\n# Draw pooling regions\nfor i in range(0, 4, 2):\n    for j in range(0, 4, 2):\n        rect = plt.Rectangle((j-0.5, i-0.5), 2, 2, \n                             fill=False, edgecolor='red', linewidth=2)\n        axes[0].add_patch(rect)\n\nim1 = axes[1].imshow(max_pooled, cmap='viridis')\naxes[1].set_title('Max Pool 2\u00d72 (2\u00d72)')\nfor i in range(2):\n    for j in range(2):\n        axes[1].text(j, i, f'{max_pooled[i,j]:.0f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1])\n\nim2 = axes[2].imshow(avg_pooled, cmap='viridis')\naxes[2].set_title('Avg Pool 2\u00d72 (2\u00d72)')\nfor i in range(2):\n    for j in range(2):\n        axes[2].text(j, i, f'{avg_pooled[i,j]:.1f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[2].axis('off')\nplt.colorbar(im2, ax=axes[2])\n\nplt.suptitle('Pooling Operations', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key Differences: - Max Pool: Takes maximum value (preserves strong features) - Avg Pool: Takes average (smooths features) - Both reduce spatial dimensions \u2192 fewer parameters \u2192 faster - Pooling provides translation invariance</p>"},{"location":"Week3_Day11/#exercise-5-first-cnn-in-pytorch-60-min","title":"Exercise 5: First CNN in PyTorch (60 min)","text":"<p>Build a simple CNN for MNIST:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: FIRST CNN IN PYTORCH\")\nprint(\"=\"*70)\n\n# Define simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, \n                               kernel_size=3, stride=1, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n                               kernel_size=3, stride=1, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # Conv block 1\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x = self.pool1(x)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = torch.relu(x)\n        x = self.pool2(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# Create model\nmodel = SimpleCNN()\nprint(\"Simple CNN Architecture:\")\nprint(model)\nprint()\n\n# Calculate parameter count\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\n\n# Trace through the network with a sample input\nprint(\"\\nTracing through the network:\")\nprint(\"-\" * 60)\n\nsample_input = torch.randn(1, 1, 28, 28)\nprint(f\"Input shape: {sample_input.shape} [batch, channels, height, width]\")\n\n# Layer by layer\nx = sample_input\nprint(f\"\\nAfter input: {x.shape}\")\n\nx = model.conv1(x)\nprint(f\"After conv1 (16 filters, 3\u00d73): {x.shape}\")\n\nx = torch.relu(x)\nprint(f\"After ReLU: {x.shape}\")\n\nx = model.pool1(x)\nprint(f\"After pool1 (2\u00d72): {x.shape}\")\n\nx = model.conv2(x)\nprint(f\"After conv2 (32 filters, 3\u00d73): {x.shape}\")\n\nx = torch.relu(x)\nprint(f\"After ReLU: {x.shape}\")\n\nx = model.pool2(x)\nprint(f\"After pool2 (2\u00d72): {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After flatten: {x.shape}\")\n\nx = model.fc1(x)\nprint(f\"After fc1: {x.shape}\")\n\nx = torch.relu(x)\nx = model.dropout(x)\nprint(f\"After ReLU + Dropout: {x.shape}\")\n\nx = model.fc2(x)\nprint(f\"After fc2 (output): {x.shape}\")\n\n# Load MNIST\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"\\nDataset loaded:\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Train CNN\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining CNN on MNIST...\")\nepochs = 5\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test CNN\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_accuracy = correct / total\nprint(f\"\\n Test Accuracy: {test_accuracy:.4f}\")\n\n# Compare with Week 2's fully connected network\nprint(\"\\nComparison with Week 2:\")\nprint(\"-\" * 60)\nprint(\"Fully Connected (Week 2): ~95-96% accuracy, ~500K parameters\")\nprint(f\"CNN (Week 3):             ~{test_accuracy*100:.1f}% accuracy, {total_params:,} parameters\")\nprint(\"\\n CNNs achieve similar/better accuracy with MANY fewer parameters!\")\n\nprint(\"\\n First CNN complete\")\n</code></pre>"},{"location":"Week3_Day11/#exercise-6-visualizing-learned-features-50-min","title":"Exercise 6: Visualizing Learned Features (50 min)","text":"<p>See what the CNN has learned:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 6: VISUALIZING LEARNED FEATURES\")\nprint(\"=\"*70)\n\n# 1. Visualize conv1 filters\nprint(\"\\n1. Visualizing First Layer Filters\")\nprint(\"-\" * 40)\n\n# Get conv1 weights\nconv1_weights = model.conv1.weight.data.cpu()\nprint(f\"Conv1 weights shape: {conv1_weights.shape}\")  # [out_channels, in_channels, h, w]\n\n# Plot first layer filters\nfig, axes = plt.subplots(4, 4, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx in range(16):\n    filter_img = conv1_weights[idx, 0, :, :]  # [3, 3]\n    axes[idx].imshow(filter_img, cmap='gray')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\n\nplt.suptitle('Learned Filters in Conv1', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# 2. Visualize feature maps\nprint(\"\\n2. Visualizing Feature Maps\")\nprint(\"-\" * 40)\n\n# Get a sample image\ndataiter = iter(test_loader)\nimages, labels = next(dataiter)\nsample_image = images[0:1]  # Take first image\nsample_label = labels[0].item()\n\nprint(f\"Sample image label: {sample_label}\")\n\n# Hook to capture intermediate activations\nactivations = {}\n\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks\nmodel.conv1.register_forward_hook(get_activation('conv1'))\nmodel.conv2.register_forward_hook(get_activation('conv2'))\n\n# Forward pass\nmodel.eval()\nwith torch.no_grad():\n    output = model(sample_image)\n    prediction = output.argmax(dim=1).item()\n\nprint(f\"Predicted: {prediction}\")\n\n# Visualize conv1 feature maps\nconv1_features = activations['conv1'][0]  # [16, 28, 28]\nprint(f\"Conv1 feature maps shape: {conv1_features.shape}\")\n\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(16):\n    axes[idx].imshow(conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Conv1 Feature Maps (Label: {sample_label}, Pred: {prediction})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize conv2 feature maps\nconv2_features = activations['conv2'][0]  # [32, 14, 14]\nprint(f\"Conv2 feature maps shape: {conv2_features.shape}\")\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\naxes = axes.flatten()\n\nfor idx in range(32):\n    axes[idx].imshow(conv2_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Filter {idx}', fontsize=8)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Conv2 Feature Maps (Deeper features)', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Observations: - Early layers detect simple features (edges, curves) - Deeper layers combine simple features into complex patterns - Different filters activate for different patterns</p>"},{"location":"Week3_Day11/#mini-challenge-understanding-receptive-fields-40-min","title":"Mini-Challenge: Understanding Receptive Fields (40 min)","text":"<p>Explore what each neuron \"sees\":</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: RECEPTIVE FIELDS\")\nprint(\"=\"*70)\n\ndef calculate_receptive_field(layers_info):\n    \"\"\"\n    Calculate receptive field size\n\n    layers_info: list of (kernel_size, stride) tuples\n    \"\"\"\n    rf = 1\n    stride_prod = 1\n\n    print(\"\\nReceptive Field Calculation:\")\n    print(\"-\" * 60)\n    print(f\"{'Layer':&gt;10} | {'Kernel':&gt;6} | {'Stride':&gt;6} | {'RF':&gt;6} | {'Stride Prod':&gt;12}\")\n    print(\"-\" * 60)\n\n    for i, (k, s) in enumerate(layers_info):\n        rf = rf + (k - 1) * stride_prod\n        stride_prod *= s\n        print(f\"Layer {i+1:&gt;3} | {k:&gt;6} | {s:&gt;6} | {rf:&gt;6} | {stride_prod:&gt;12}\")\n\n    return rf\n\n# Calculate for our SimpleCNN\nprint(\"\\nSimpleCNN Receptive Field:\")\nlayers = [\n    (3, 1),  # conv1: 3\u00d73, stride 1\n    (2, 2),  # pool1: 2\u00d72, stride 2\n    (3, 1),  # conv2: 3\u00d73, stride 1\n    (2, 2),  # pool2: 2\u00d72, stride 2\n]\n\nrf = calculate_receptive_field(layers)\nprint(f\"\\nFinal receptive field: {rf}\u00d7{rf}\")\nprint(f\"This means each neuron in the output 'sees' a {rf}\u00d7{rf} region of the input\")\n\n# Visualize receptive field\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n# Draw input image grid (28\u00d728)\nfor i in range(29):\n    ax.axhline(i, color='gray', linewidth=0.5, alpha=0.3)\n    ax.axvline(i, color='gray', linewidth=0.5, alpha=0.3)\n\n# Highlight receptive field (centered)\ncenter = 14\nhalf_rf = rf // 2\nrect = plt.Rectangle((center - half_rf, center - half_rf), rf, rf,\n                     fill=True, facecolor='blue', alpha=0.3, edgecolor='blue', linewidth=3)\nax.add_patch(rect)\n\nax.set_xlim(0, 28)\nax.set_ylim(28, 0)\nax.set_aspect('equal')\nax.set_title(f'Receptive Field Visualization\\nRF = {rf}\u00d7{rf} pixels', \n             fontsize=14, fontweight='bold')\nax.set_xlabel('Width (pixels)')\nax.set_ylabel('Height (pixels)')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key Insights: - Receptive field grows with depth - Pooling increases receptive field size - Deeper networks 'see' larger contexts - Each output neuron is influenced by a {rf}\u00d7{rf} region of input - Deeper layers have larger receptive fields - This is how CNNs capture hierarchical features</p>"},{"location":"Week3_Day11/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review convolution operation thoroughly \u2610 Understand why CNNs work for images \u2610 Write daily reflection (choose 2-3 prompts below) \u2610 Prepare questions for Monday check-in</p>"},{"location":"Week3_Day11/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does convolution differ from fully connected layers?</li> <li>Why do CNNs work better for images than fully connected networks?</li> <li>What surprised you about feature visualizations?</li> <li>How does pooling help CNNs?</li> <li>What is the purpose of multiple filters in a convolutional layer?</li> <li>What questions do you still have about CNNs?</li> </ul> <p>Next: Day 12 - Classic Architectures (LeNet, AlexNet)</p>"},{"location":"Week3_Day12/","title":"Week 3, Day 12: Classic Architectures - LeNet &amp; AlexNet","text":""},{"location":"Week3_Day12/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand LeNet-5 architecture (1998) - the CNN pioneer</li> <li>Study AlexNet (2012) - the ImageNet breakthrough</li> <li>Implement both architectures in PyTorch</li> <li>Compare performance: fully connected vs LeNet vs AlexNet</li> <li>Understand historical context and evolution</li> <li>Visualize architecture differences</li> </ul>"},{"location":"Week3_Day12/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week3_Day12/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week3_Day12/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: LeNet-5 Architecture Explained (15 min) Understanding the first successful CNN</p> <p>\u2610 Watch: AlexNet Explained by Yannic Kilcher (25 min) Deep dive into the ImageNet breakthrough</p> <p>\u2610 Watch: History of CNNs - ImageNet Evolution (20 min) Context for why AlexNet mattered</p> <p>\u2610 Watch: CNN Architectures Comparison by Lex Fridman (15 min) Evolution from LeNet to modern CNNs</p> <p>\u2610 Watch: Understanding Deep Learning by 3Blue1Brown review (15 min) Reinforce concepts</p>"},{"location":"Week3_Day12/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 8.1 - AlexNet</p> <p>\u2610 Read: D2L Chapter 7.6 - LeNet</p> <p>\u2610 Optional: Original LeNet-5 paper - sections 1-3 (Gradient-Based Learning Applied to Document Recognition)</p>"},{"location":"Week3_Day12/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week3_Day12/#exercise-1-implement-lenet-5-60-min","title":"Exercise 1: Implement LeNet-5 (60 min)","text":"<p>Build the classic 1998 architecture:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: LENET-5 IMPLEMENTATION\")\nprint(\"=\"*70)\n\n# LeNet-5 Architecture\nclass LeNet5(nn.Module):\n    \"\"\"\n    LeNet-5 (1998) by Yann LeCun\n\n    Original paper: Gradient-Based Learning Applied to Document Recognition\n\n    Architecture:\n    - Input: 32x32 grayscale image\n    - Conv1: 6 filters, 5x5 kernel\n    - Pool1: 2x2 average pooling\n    - Conv2: 16 filters, 5x5 kernel\n    - Pool2: 2x2 average pooling\n    - FC1: 120 units\n    - FC2: 84 units\n    - Output: 10 classes\n    \"\"\"\n    def __init__(self):\n        super(LeNet5, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Conv block 1\n        x = self.conv1(x)\n        x = torch.tanh(x)  # Original used tanh\n        x = self.pool1(x)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = torch.tanh(x)\n        x = self.pool2(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected\n        x = self.fc1(x)\n        x = torch.tanh(x)\n        x = self.fc2(x)\n        x = torch.tanh(x)\n        x = self.fc3(x)\n\n        return x\n\n# Create model\nlenet = LeNet5().to(device)\nprint(\"\\nLeNet-5 Architecture:\")\nprint(lenet)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in lenet.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Trace through with sample input\nprint(\"\\nArchitecture Flow:\")\nprint(\"-\" * 60)\n\nsample = torch.randn(1, 1, 32, 32)\nprint(f\"Input: {sample.shape}\")\n\n# Layer by layer\nx = sample\nx = lenet.conv1(x)\nprint(f\"After Conv1 (6@5x5): {x.shape}\")\nx = torch.tanh(x)\nx = lenet.pool1(x)\nprint(f\"After Pool1 (AvgPool 2x2): {x.shape}\")\n\nx = lenet.conv2(x)\nprint(f\"After Conv2 (16@5x5): {x.shape}\")\nx = torch.tanh(x)\nx = lenet.pool2(x)\nprint(f\"After Pool2 (AvgPool 2x2): {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After Flatten: {x.shape}\")\n\nx = lenet.fc1(x)\nprint(f\"After FC1 (120): {x.shape}\")\nx = torch.tanh(x)\n\nx = lenet.fc2(x)\nprint(f\"After FC2 (84): {x.shape}\")\nx = torch.tanh(x)\n\nx = lenet.fc3(x)\nprint(f\"After FC3 (10): {x.shape}\")\n\n# Load MNIST with padding to make it 32x32 (LeNet's expected input)\ntransform_lenet = transforms.Compose([\n    transforms.Pad(2),  # Pad MNIST from 28x28 to 32x32\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                               download=True, transform=transform_lenet)\ntest_dataset = datasets.MNIST(root='./data', train=False,\n                              download=True, transform=transform_lenet)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"\\nDataset: MNIST (padded to 32x32)\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Train LeNet\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters(), lr=0.001)\n\nprint(\"\\nTraining LeNet-5 on MNIST...\")\nepochs = 5\ntrain_losses = []\ntrain_accs = []\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    lenet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = lenet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n    train_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\ntraining_time = time.time() - start_time\n\n# Test LeNet\nlenet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = lenet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nlenet_accuracy = correct / total\nprint(f\"\\nLeNet-5 Test Accuracy: {lenet_accuracy:.4f}\")\nprint(f\"Training time: {training_time:.1f} seconds\")\n\nprint(\"\\nLeNet-5 implementation complete\")\n</code></pre>"},{"location":"Week3_Day12/#exercise-2-implement-alexnet-simplified-60-min","title":"Exercise 2: Implement AlexNet (Simplified) (60 min)","text":"<p>Build the 2012 ImageNet winner:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: ALEXNET IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass AlexNet(nn.Module):\n    \"\"\"\n    AlexNet (2012) by Alex Krizhevsky\n\n    Original paper: ImageNet Classification with Deep CNNs\n\n    Simplified version for MNIST (original was for 224x224 ImageNet)\n\n    Key innovations:\n    - ReLU activation (faster than tanh)\n    - Dropout for regularization\n    - Deeper network (8 layers)\n    - Data augmentation\n    - Multiple GPUs (not implemented here)\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n\n        # Convolutional layers\n        self.features = nn.Sequential(\n            # Conv1\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv2\n            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv3\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv4\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv5\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 4 * 4, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create model\nalexnet = AlexNet(num_classes=10).to(device)\nprint(\"\\nAlexNet Architecture:\")\nprint(alexnet)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in alexnet.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Compare with LeNet\nlenet_params = sum(p.numel() for p in lenet.parameters())\nprint(f\"LeNet-5 parameters: {lenet_params:,}\")\nprint(f\"AlexNet is {total_params/lenet_params:.1f}x larger than LeNet\")\n\n# Train AlexNet\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n\nprint(\"\\nTraining AlexNet on MNIST...\")\nepochs = 5\nalexnet_losses = []\nalexnet_accs = []\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    alexnet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = alexnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    alexnet_losses.append(epoch_loss)\n    alexnet_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\nalexnet_time = time.time() - start_time\n\n# Test AlexNet\nalexnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = alexnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nalexnet_accuracy = correct / total\nprint(f\"\\nAlexNet Test Accuracy: {alexnet_accuracy:.4f}\")\nprint(f\"Training time: {alexnet_time:.1f} seconds\")\n\nprint(\"\\nAlexNet implementation complete\")\n</code></pre>"},{"location":"Week3_Day12/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week3_Day12/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week3_Day12/#exercise-3-architecture-comparison-50-min","title":"Exercise 3: Architecture Comparison (50 min)","text":"<p>Compare all three approaches comprehensively:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: COMPREHENSIVE COMPARISON\")\nprint(\"=\"*70)\n\n# Load Day 11's SimpleCNN for comparison\nfrom Week3_Day11 import SimpleCNN  # Assuming it's available\n\nsimplecnn = SimpleCNN().to(device)\nsimplecnn_params = sum(p.numel() for p in simplecnn.parameters())\n\n# Create comparison table\nprint(\"\\nModel Comparison:\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Accuracy':&gt;10} | {'Training Time':&gt;14}\")\nprint(\"=\"*80)\nprint(f\"{'SimpleCNN':&lt;15} | {simplecnn_params:&gt;12,} | {'~98.5%':&gt;10} | {'~30s':&gt;14}\")\nprint(f\"{'LeNet-5':&lt;15} | {lenet_params:&gt;12,} | {lenet_accuracy:&gt;10.4f} | {training_time:&gt;13.1f}s\")\nprint(f\"{'AlexNet':&lt;15} | {total_params:&gt;12,} | {alexnet_accuracy:&gt;10.4f} | {alexnet_time:&gt;13.1f}s\")\nprint(\"=\"*80)\n\n# Visualize training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss comparison\naxes[0].plot(train_losses, label='LeNet-5', marker='o')\naxes[0].plot(alexnet_losses, label='AlexNet', marker='s')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy comparison\naxes[1].plot(train_accs, label='LeNet-5', marker='o')\naxes[1].plot(alexnet_accs, label='AlexNet', marker='s')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy Comparison')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('LeNet-5 vs AlexNet Training', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Architecture visualization\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# LeNet-5 diagram\nax = axes[0]\nlayers_lenet = [\n    (\"Input\\n32\u00d732\u00d71\", 0),\n    (\"Conv1\\n28\u00d728\u00d76\", 1),\n    (\"Pool1\\n14\u00d714\u00d76\", 2),\n    (\"Conv2\\n10\u00d710\u00d716\", 3),\n    (\"Pool2\\n5\u00d75\u00d716\", 4),\n    (\"FC1\\n120\", 5),\n    (\"FC2\\n84\", 6),\n    (\"Output\\n10\", 7)\n]\n\nfor i, (label, pos) in enumerate(layers_lenet):\n    ax.add_patch(plt.Rectangle((pos, 0), 0.8, 0.5, \n                                facecolor='lightblue', edgecolor='black', linewidth=2))\n    ax.text(pos + 0.4, 0.25, label, ha='center', va='center', fontsize=9)\n\n    if i &lt; len(layers_lenet) - 1:\n        ax.arrow(pos + 0.8, 0.25, 0.15, 0, head_width=0.1, \n                head_length=0.05, fc='black', ec='black')\n\nax.set_xlim(-0.5, 8)\nax.set_ylim(-0.2, 0.7)\nax.axis('off')\nax.set_title('LeNet-5 Architecture', fontsize=14, fontweight='bold', pad=20)\n\n# AlexNet diagram\nax = axes[1]\nlayers_alexnet = [\n    (\"Input\\n32\u00d732\u00d71\", 0),\n    (\"Conv1\\n32\u00d732\u00d764\", 1),\n    (\"Pool1\\n16\u00d716\u00d764\", 2),\n    (\"Conv2\\n16\u00d716\u00d7192\", 3),\n    (\"Pool2\\n8\u00d78\u00d7192\", 4),\n    (\"Conv3-5\\n8\u00d78\u00d7256\", 5),\n    (\"Pool3\\n4\u00d74\u00d7256\", 6),\n    (\"FC\u00d72\\n4096\", 7),\n    (\"Output\\n10\", 8)\n]\n\nfor i, (label, pos) in enumerate(layers_alexnet):\n    color = 'lightcoral' if 'FC' in label else 'lightgreen'\n    ax.add_patch(plt.Rectangle((pos, 0), 0.8, 0.5,\n                                facecolor=color, edgecolor='black', linewidth=2))\n    ax.text(pos + 0.4, 0.25, label, ha='center', va='center', fontsize=9)\n\n    if i &lt; len(layers_alexnet) - 1:\n        ax.arrow(pos + 0.8, 0.25, 0.15, 0, head_width=0.1,\n                head_length=0.05, fc='black', ec='black')\n\nax.set_xlim(-0.5, 9)\nax.set_ylim(-0.2, 0.7)\nax.axis('off')\nax.set_title('AlexNet Architecture (Simplified)', fontsize=14, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key Observations - LeNet (1998): Small, efficient, tanh activation - AlexNet (2012): Deeper, ReLU, dropout, more parameters - Both work well on MNIST (too easy for modern CNNs) - Real power shows on complex datasets (ImageNet)</p>"},{"location":"Week3_Day12/#exercise-4-historical-context-analysis-40-min","title":"Exercise 4: Historical Context Analysis (40 min)","text":"<p>Understand the evolution and impact. Note: this code is just an explanation of the historical development of these models. Focus on the output of this code, not the code itself</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: HISTORICAL CONTEXT\")\nprint(\"=\"*70)\n\n# Timeline of CNN evolution\nprint(\"\\nCNN Evolution Timeline:\")\nprint(\"=\"*80)\n\nmilestones = [\n    (\"1989\", \"LeNet-1\", \"Yann LeCun\", \"First successful CNN for digit recognition\"),\n    (\"1998\", \"LeNet-5\", \"LeCun et al.\", \"Refined architecture, deployed in production\"),\n    (\"2012\", \"AlexNet\", \"Krizhevsky\", \"ImageNet breakthrough, 15.3% \u2192 10.9% error\"),\n    (\"2014\", \"VGG\", \"Simonyan &amp; Zisserman\", \"Very deep networks with small filters\"),\n    (\"2015\", \"ResNet\", \"He et al.\", \"Skip connections enable 100+ layer networks\"),\n    (\"2017\", \"DenseNet\", \"Huang et al.\", \"Dense connections between layers\"),\n]\n\nfor year, name, author, achievement in milestones:\n    print(f\"{year}: {name:12s} by {author:20s} - {achievement}\")\n\n# AlexNet innovations\nprint(\"\\n\" + \"=\"*80)\nprint(\"AlexNet's Key Innovations (Why it Changed Everything)\")\nprint(\"=\"*80)\n\ninnovations = {\n    \"ReLU Activation\": {\n        \"Before\": \"tanh/sigmoid (slow training, vanishing gradients)\",\n        \"After\": \"ReLU (6x faster training, better gradients)\",\n        \"Impact\": \"Enabled training of deeper networks\"\n    },\n    \"Dropout\": {\n        \"Before\": \"L2 regularization only\",\n        \"After\": \"Random neuron dropout during training\",\n        \"Impact\": \"Reduced overfitting significantly\"\n    },\n    \"Data Augmentation\": {\n        \"Before\": \"Limited or no augmentation\",\n        \"After\": \"Crops, flips, color jitter\",\n        \"Impact\": \"Effective dataset size increased 2048x\"\n    },\n    \"GPU Training\": {\n        \"Before\": \"CPU-only (slow)\",\n        \"After\": \"Parallel training on 2 GPUs\",\n        \"Impact\": \"Made large-scale training feasible\"\n    },\n    \"Local Response Normalization\": {\n        \"Before\": \"No normalization between filters\",\n        \"After\": \"Normalize activations across channels\",\n        \"Impact\": \"Later replaced by Batch Normalization\"\n    }\n}\n\nfor innovation, details in innovations.items():\n    print(f\"\\n{innovation}:\")\n    print(f\"  Before: {details['Before']}\")\n    print(f\"  After:  {details['After']}\")\n    print(f\"  Impact: {details['Impact']}\")\n\n# ImageNet competition results\nprint(\"\\n\" + \"=\"*80)\nprint(\"ImageNet Competition Results (Top-5 Error Rate)\")\nprint(\"=\"*80)\n\nimagenet_results = [\n    (2010, 28.2, \"Traditional CV (SIFT + Fisher Vectors)\"),\n    (2011, 25.8, \"Traditional CV\"),\n    (2012, 16.4, \"AlexNet (First Deep CNN)\"),\n    (2013, 11.7, \"ZFNet\"),\n    (2014, 7.3, \"VGG &amp; GoogLeNet\"),\n    (2015, 3.57, \"ResNet-152\"),\n    (2017, 2.25, \"SENet\"),\n]\n\nyears = [r[0] for r in imagenet_results]\nerrors = [r[1] for r in imagenet_results]\nnames = [r[2] for r in imagenet_results]\n\nplt.figure(figsize=(12, 7))\nplt.plot(years, errors, marker='o', linewidth=2, markersize=10)\n\n# Annotate key points\nfor year, error, name in imagenet_results:\n    if year in [2011, 2012, 2015]:\n        plt.annotate(name, xy=(year, error), xytext=(10, -20 if year==2012 else 20),\n                    textcoords='offset points', fontsize=9,\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n                    arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3,rad=0'))\n\nplt.axvline(2012, color='red', linestyle='--', alpha=0.5, label='AlexNet Year')\nplt.axhline(5.1, color='green', linestyle='--', alpha=0.5, label='Human Performance (~5%)')\n\nplt.xlabel('Year')\nplt.ylabel('Top-5 Error Rate (%)')\nplt.title('ImageNet Competition: The Deep Learning Revolution', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nWhy AlexNet Was Revolutionary:\")\nprint(\"- First deep learning win in ImageNet competition\")\nprint(\"- Error rate: 25.8% \u2192 16.4% (37% reduction!)\")\nprint(\"- Proved deep learning &gt; traditional computer vision\")\nprint(\"- Sparked the deep learning boom\")\nprint(\"- Made GPUs essential for AI research\")\n\nprint(\"\\nHistorical context understood\")\n</code></pre>"},{"location":"Week3_Day12/#exercise-5-feature-visualization-comparison-50-min","title":"Exercise 5: Feature Visualization Comparison (50 min)","text":"<p>Compare what LeNet and AlexNet learn. Focus on the output of this code.</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: FEATURE VISUALIZATION\")\nprint(\"=\"*70)\n\n# Visualize LeNet filters\nprint(\"\\n1. LeNet-5 First Layer Filters\")\nprint(\"-\" * 40)\n\nlenet_conv1_weights = lenet.conv1.weight.data.cpu()\nprint(f\"LeNet Conv1 filters shape: {lenet_conv1_weights.shape}\")  # [6, 1, 5, 5]\n\nfig, axes = plt.subplots(1, 6, figsize=(15, 3))\nfor idx in range(6):\n    axes[idx].imshow(lenet_conv1_weights[idx, 0], cmap='gray')\n    axes[idx].set_title(f'Filter {idx}')\n    axes[idx].axis('off')\nplt.suptitle('LeNet-5 Conv1 Filters (5\u00d75)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize AlexNet filters\nprint(\"\\n2. AlexNet First Layer Filters\")\nprint(\"-\" * 40)\n\nalexnet_conv1_weights = alexnet.features[0].weight.data.cpu()\nprint(f\"AlexNet Conv1 filters shape: {alexnet_conv1_weights.shape}\")  # [64, 1, 3, 3]\n\n# Show first 16 filters\nfig, axes = plt.subplots(4, 4, figsize=(10, 10))\naxes = axes.flatten()\nfor idx in range(16):\n    axes[idx].imshow(alexnet_conv1_weights[idx, 0], cmap='gray')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\nplt.suptitle('AlexNet Conv1 Filters (3\u00d73, first 16 of 64)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Get feature maps for comparison\ndataiter = iter(test_loader)\nimages, labels = next(dataiter)\nsample_image = images[0:1]\n\n# LeNet feature maps\nactivations_lenet = {}\ndef get_activation_lenet(name):\n    def hook(model, input, output):\n        activations_lenet[name] = output.detach()\n    return hook\n\nlenet.conv1.register_forward_hook(get_activation_lenet('conv1'))\nlenet.conv2.register_forward_hook(get_activation_lenet('conv2'))\n\nlenet.eval()\nwith torch.no_grad():\n    _ = lenet(sample_image)\n\n# AlexNet feature maps\nactivations_alexnet = {}\ndef get_activation_alexnet(name):\n    def hook(model, input, output):\n        activations_alexnet[name] = output.detach()\n    return hook\n\nalexnet.features[0].register_forward_hook(get_activation_alexnet('conv1'))\nalexnet.features[3].register_forward_hook(get_activation_alexnet('conv2'))\n\nalexnet.eval()\nwith torch.no_grad():\n    _ = alexnet(sample_image)\n\n# Visualize LeNet feature maps\nlenet_conv1_features = activations_lenet['conv1'][0]\nprint(f\"\\nLeNet Conv1 feature maps: {lenet_conv1_features.shape}\")\n\nfig, axes = plt.subplots(1, 6, figsize=(15, 3))\nfor idx in range(6):\n    axes[idx].imshow(lenet_conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Map {idx}', fontsize=10)\n    axes[idx].axis('off')\nplt.suptitle(f'LeNet-5 Conv1 Feature Maps (Label: {labels[0].item()})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize AlexNet feature maps\nalexnet_conv1_features = activations_alexnet['conv1'][0]\nprint(f\"AlexNet Conv1 feature maps: {alexnet_conv1_features.shape}\")\n\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.flatten()\nfor idx in range(16):\n    axes[idx].imshow(alexnet_conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Map {idx}', fontsize=9)\n    axes[idx].axis('off')\nplt.suptitle(f'AlexNet Conv1 Feature Maps (first 16 of 64, Label: {labels[0].item()})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Observations - LeNet: 6 filters, captures basic edges and patterns - AlexNet: 64 filters, more diverse feature detection - More filters \u2192 more capacity to learn complex features - Both learn hierarchical representations</p>"},{"location":"Week3_Day12/#mini-challenge-design-your-own-architecture-60-min","title":"Mini-Challenge: Design Your Own Architecture (60 min)","text":"<p>Create a custom CNN:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: CUSTOM CNN DESIGN\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nYour task: Design a CNN better than LeNet but smaller than AlexNet\n\nRequirements:\n- Input: 32\u00d732\u00d71 (MNIST with padding)\n- Output: 10 classes\n- Target: &gt;99% accuracy\n- Constraint: &lt;500K parameters\n- Use modern techniques: ReLU, BatchNorm, Dropout\n\nDesign Considerations:\n1. How many conv layers?\n2. What kernel sizes?\n3. When to pool?\n4. How much dropout?\n5. FC layer sizes?\n\nTry different designs and compare!\n\"\"\")\n\nclass CustomCNN(nn.Module):\n    \"\"\"Your custom architecture here\"\"\"\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n\n        # TODO: Design your architecture\n        # Hint: Start with 3-4 conv layers\n        # Use BatchNorm after convolutions\n        # Use ReLU activation\n        # Add dropout before FC layers\n\n        # Example starter:\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 16\u00d716\n\n            # Block 2\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 8\u00d78\n\n            # Block 3\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 4\u00d74\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(256, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create and test your model\ncustom_model = CustomCNN().to(device)\ncustom_params = sum(p.numel() for p in custom_model.parameters())\n\nprint(f\"\\nYour Custom CNN:\")\nprint(custom_model)\nprint(f\"\\nParameters: {custom_params:,}\")\nprint(f\"Within budget: {'Yes' if custom_params &lt; 500000 else 'No'}\")\n\n# Train it\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(custom_model.parameters(), lr=0.001)\n\nprint(\"\\nTraining your custom CNN...\")\nepochs = 5\n\nfor epoch in range(epochs):\n    custom_model.train()\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = custom_model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_acc = correct / total\n    print(f\"Epoch {epoch+1}/{epochs}: Accuracy = {epoch_acc:.4f}\")\n\n# Test\ncustom_model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = custom_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ncustom_accuracy = correct / total\nprint(f\"\\nYour Custom CNN Test Accuracy: {custom_accuracy:.4f}\")\n\n# Final comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL COMPARISON\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Accuracy':&gt;10}\")\nprint(\"=\"*80)\nprint(f\"{'LeNet-5':&lt;15} | {lenet_params:&gt;12,} | {lenet_accuracy:&gt;10.4f}\")\nprint(f\"{'AlexNet':&lt;15} | {total_params:&gt;12,} | {alexnet_accuracy:&gt;10.4f}\")\nprint(f\"{'Your CNN':&lt;15} | {custom_params:&gt;12,} | {custom_accuracy:&gt;10.4f}\")\nprint(\"=\"*80)\n\nprint(\"\\nCustom architecture complete!\")\n</code></pre>"},{"location":"Week3_Day12/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review LeNet and AlexNet architectures \u2610 Understand historical significance \u2610 Write daily reflection (choose 2-3 prompts below)  </p>"},{"location":"Week3_Day12/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How did CNNs evolve from LeNet to AlexNet?</li> <li>Why was AlexNet's 2012 win so significant?</li> <li>What innovations from AlexNet are still used today?</li> <li>How does your custom CNN compare to the classics?</li> <li>What design choices matter most in CNN architecture?</li> <li>What questions do you have about CNN design?</li> </ul> <p>Next: Day 13 - Modern Architectures (VGG, ResNet)</p>"},{"location":"Week3_Day13/","title":"Week 3, Day 13: Modern Architectures - VGG &amp; ResNet","text":""},{"location":"Week3_Day13/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand VGG philosophy: deeper networks with small filters</li> <li>Learn ResNet and skip connections</li> <li>Understand vanishing gradient problem and how ResNet solves it</li> <li>Implement VGG-style blocks and ResNet blocks</li> <li>Compare network depth effects</li> <li>Visualize gradient flow</li> </ul>"},{"location":"Week3_Day13/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week3_Day13/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week3_Day13/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: VGG Networks Explained (15 min) Understanding the philosophy of very deep networks</p> <p>\u2610 Watch: ResNet Explained by Yannic Kilcher (30 min) Deep dive into residual learning</p> <p>\u2610 Watch: ResNet: Why it works (15 min) Understanding skip connections</p> <p>\u2610 Watch: Batch Normalization Explained by StatQuest (15 min) Critical for training deep networks</p> <p>\u2610 Watch: Vanishing Gradients Problem (15 min) Why deep networks were hard to train</p>"},{"location":"Week3_Day13/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 8.6 - VGG</p> <p>\u2610 Read: D2L Chapter 8.7 - ResNet</p> <p>\u2610 Optional: Original ResNet paper introduction (Deep Residual Learning for Image Recognition)</p>"},{"location":"Week3_Day13/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week3_Day13/#exercise-1-vgg-blocks-and-architecture-60-min","title":"Exercise 1: VGG Blocks and Architecture (60 min)","text":"<p>Key VGG insight - Why 3\u00d73 convolutions?</p> <p>Two 3\u00d73 convs:   - Receptive field: 5\u00d75   - Parameters: 2 \u00d7 (3\u00d73) = 18 per channel   - Non-linearities: 2 (ReLU after each)</p> <p>One 5\u00d75 conv:   - Receptive field: 5\u00d75   - Parameters: 1 \u00d7 (5\u00d75) = 25 per channel   - Non-linearities: 1</p> <p>Result: 28% fewer parameters + more non-linearity!</p> <p>Understand VGG's modular design:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: VGG ARCHITECTURE\")\nprint(\"=\"*70)\n\ndef vgg_block(num_convs, in_channels, out_channels):\n    \"\"\"\n    Create a VGG block\n\n    VGG philosophy: Stack multiple 3\u00d73 convolutions\n    Benefit: Two 3\u00d73 convs have same receptive field as one 5\u00d75\n            but fewer parameters and more non-linearity\n\n    Args:\n        num_convs: number of conv layers in block\n        in_channels: input channels\n        out_channels: output channels\n    \"\"\"\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n        in_channels = out_channels\n\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    return nn.Sequential(*layers)\n\n\n\nclass VGG16_MNIST(nn.Module):\n    \"\"\"\n    VGG-16 architecture adapted for MNIST\n\n    Original VGG-16 was designed for ImageNet (224\u00d7224 RGB)\n    This is a simplified version for MNIST (32\u00d732 grayscale)\n\n    Architecture:\n    - 5 VGG blocks with increasing channels: 64, 128, 256, 512, 512\n    - Each block has 2-3 conv layers\n    - FC layers at the end\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(VGG16_MNIST, self).__init__()\n\n        self.features = nn.Sequential(\n            # Block 1: 32\u00d732 \u2192 16\u00d716\n            vgg_block(2, 1, 64),\n\n            # Block 2: 16\u00d716 \u2192 8\u00d78\n            vgg_block(2, 64, 128),\n\n            # Block 3: 8\u00d78 \u2192 4\u00d74\n            vgg_block(3, 128, 256),\n\n            # Block 4: 4\u00d74 \u2192 2\u00d72\n            vgg_block(3, 256, 512),\n\n            # Block 5: 2\u00d72 \u2192 1\u00d71\n            vgg_block(3, 512, 512),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 1 * 1, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create model\nvgg = VGG16_MNIST().to(device)\nprint(\"\\nVGG-16 Architecture (MNIST version):\")\nprint(vgg)\n\n# Count parameters\nvgg_params = sum(p.numel() for p in vgg.parameters())\nprint(f\"\\nTotal parameters: {vgg_params:,}\")\n\n# Analyze each VGG block\nprint(\"\\nVGG Block Analysis:\")\nprint(\"-\" * 60)\nsample = torch.randn(1, 1, 32, 32)\nx = sample\n\nblock_num = 1\nfor module in vgg.features:\n    if isinstance(module, nn.Sequential):\n        x = module(x)\n        print(f\"After Block {block_num}: {x.shape}\")\n        block_num += 1\n\nprint(f\"After Flatten: {x.view(x.size(0), -1).shape}\")\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Pad(2),  # 28\u00d728 \u2192 32\u00d732\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Train VGG (just a few epochs due to size)\nprint(\"\\nTraining VGG-16 on MNIST (3 epochs)...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vgg.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    vgg.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        outputs = vgg(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        if (batch_idx + 1) % 200 == 0:\n            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: \"\n                  f\"Loss = {running_loss/(batch_idx+1):.4f}\")\n\n    epoch_acc = correct / total\n    print(f\"Epoch {epoch+1}/{epochs}: Accuracy = {epoch_acc:.4f}\")\n\n# Test\nvgg.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = vgg(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nvgg_accuracy = correct / total\nprint(f\"\\nVGG-16 Test Accuracy: {vgg_accuracy:.4f}\")\nprint(f\"Parameters: {vgg_params:,}\")\n\nprint(\"\\nVGG implementation complete\")\n</code></pre>"},{"location":"Week3_Day13/#exercise-2-understanding-residual-blocks-60-min","title":"Exercise 2: Understanding Residual Blocks (60 min)","text":"<p>Breakthrough idea of ResNet:</p> <p>The Vanishing Gradient Problem: - Deep networks hard to train (gradients disappear) - Adding layers made performance WORSE (degradation problem) - Not overfitting - training error also increased!</p> <p>ResNet's Solution: Skip Connections: - Instead of learning H(x), learn F(x) = H(x) - x - Output: H(x) = F(x) + x - If optimal is identity, just learn F(x) = 0 (easy!) - Gradients flow directly through skip connections</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: RESIDUAL BLOCKS\")\nprint(\"=\"*70)\n\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    Basic Residual Block\n\n    Two 3\u00d73 convolutions with skip connection\n\n           x\n           |\n       [Conv-BN-ReLU]\n           |\n       [Conv-BN]\n           |\n          (+)  \u2190 x (skip connection)\n           |\n         [ReLU]\n           |\n          out\n    \"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut connection\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            # Need to match dimensions\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels,\n                         kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n\n        # Main path\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = torch.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # Add skip connection\n        out += self.shortcut(identity)\n        out = torch.relu(out)\n\n        return out\n\n# Visualize the difference\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARING: Plain Network vs Residual Network\")\nprint(\"=\"*60)\n\nclass PlainBlock(nn.Module):\n    \"\"\"Plain block without skip connection\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(PlainBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = torch.relu(self.bn2(self.conv2(out)))\n        return out\n\n# Test gradient flow\nplain_block = PlainBlock(64, 64)\nres_block = ResidualBlock(64, 64)\n\n# Create input\nx = torch.randn(1, 64, 32, 32, requires_grad=True)\n\n# Forward pass\nplain_out = plain_block(x)\nres_out = res_block(x)\n\n# Backward pass\nplain_loss = plain_out.sum()\nres_loss = res_out.sum()\n\nplain_loss.backward()\nplain_grad = x.grad.clone()\n\nx.grad.zero_()\n\nres_loss.backward()\nres_grad = x.grad.clone()\n\n# Compare gradient magnitudes\nprint(f\"\\nGradient magnitude comparison:\")\nprint(f\"Plain block: {plain_grad.abs().mean().item():.6f}\")\nprint(f\"Residual block: {res_grad.abs().mean().item():.6f}\")\nprint(f\"Ratio: {(res_grad.abs().mean() / plain_grad.abs().mean()).item():.2f}x\")\n\nprint(\"\\nSkip connections maintain gradient flow!\")\n\nprint(\"\\nResidual blocks understood\")\n</code></pre>"},{"location":"Week3_Day13/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week3_Day13/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week3_Day13/#exercise-3-implement-resnet-18-70-min","title":"Exercise 3: Implement ResNet-18 (70 min)","text":"<p>Build a complete ResNet architecture:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: RESNET-18 IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass ResNet18(nn.Module):\n    \"\"\"\n    ResNet-18 architecture\n\n    Structure:\n    - Initial conv layer (7\u00d77 or 3\u00d73 for small images)\n    - 4 residual stages with [2,2,2,2] blocks\n    - Average pooling\n    - FC layer\n\n    Channels: 64 \u2192 128 \u2192 256 \u2192 512\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(ResNet18, self).__init__()\n\n        # Initial convolution\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Residual stages\n        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n\n        # Global average pooling\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Classifier\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        \"\"\"Create a residual stage\"\"\"\n        layers = []\n\n        # First block (may downsample)\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n\n        # Remaining blocks\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # Initial conv\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n\n        # Residual stages\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # Global pooling\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n\n        # Classifier\n        x = self.fc(x)\n\n        return x\n\n# Create ResNet-18\nresnet18 = ResNet18(num_classes=10)\nprint(\"\\nResNet-18 Architecture:\")\nprint(resnet18)\n\n# Count parameters\nresnet_params = sum(p.numel() for p in resnet18.parameters())\nprint(f\"\\nTotal parameters: {resnet_params:,}\")\n\n# Trace through architecture\nprint(\"\\nArchitecture flow:\")\nprint(\"-\" * 60)\nsample = torch.randn(1, 1, 32, 32)\nprint(f\"Input: {sample.shape}\")\n\nx = sample\nx = resnet18.conv1(x)\nx = resnet18.bn1(x)\nx = torch.relu(x)\nprint(f\"After initial conv: {x.shape}\")\n\nx = resnet18.layer1(x)\nprint(f\"After layer1 (64 channels): {x.shape}\")\n\nx = resnet18.layer2(x)\nprint(f\"After layer2 (128 channels): {x.shape}\")\n\nx = resnet18.layer3(x)\nprint(f\"After layer3 (256 channels): {x.shape}\")\n\nx = resnet18.layer4(x)\nprint(f\"After layer4 (512 channels): {x.shape}\")\n\nx = resnet18.avg_pool(x)\nprint(f\"After global avg pool: {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After flatten: {x.shape}\")\n\n# Train ResNet-18\nprint(\"\\nTraining ResNet-18 on MNIST...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n\nepochs = 5\nresnet_losses = []\nresnet_accs = []\n\nfor epoch in range(epochs):\n    resnet18.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = resnet18(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    resnet_losses.append(epoch_loss)\n    resnet_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test ResNet-18\nresnet18.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet18(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nresnet_accuracy = correct / total\nprint(f\"\\nResNet-18 Test Accuracy: {resnet_accuracy:.4f}\")\n\nprint(\"\\nResNet-18 implementation complete\")\n</code></pre>"},{"location":"Week3_Day13/#exercise-4-compare-plain-vs-residual-networks-60-min","title":"Exercise 4: Compare Plain vs Residual Networks (60 min)","text":"<p>Empirically demonstrate ResNet's advantage:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: PLAIN VS RESIDUAL COMPARISON\")\nprint(\"=\"*70)\n\nclass PlainNet(nn.Module):\n    \"\"\"Plain network (no skip connections)\"\"\"\n    def __init__(self, num_classes=10):\n        super(PlainNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 64, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Same structure as ResNet but NO skip connections\n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n\n        # First block\n        layers.append(nn.Conv2d(in_channels, out_channels, 3, \n                               stride=stride, padding=1, bias=False))\n        layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.ReLU(inplace=True))\n\n        # Remaining blocks\n        for _ in range(1, num_blocks):\n            layers.append(nn.Conv2d(out_channels, out_channels, 3,\n                                   padding=1, bias=False))\n            layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(nn.ReLU(inplace=True))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Create plain network\nplainnet = PlainNet(num_classes=10)\nplainnet_params = sum(p.numel() for p in plainnet.parameters())\n\nprint(f\"\\nPlainNet parameters: {plainnet_params:,}\")\nprint(f\"ResNet-18 parameters: {resnet_params:,}\")\nprint(f\"Difference: {abs(plainnet_params - resnet_params):,} \"\n      f\"({abs(plainnet_params - resnet_params)/resnet_params*100:.1f}%)\")\n\n# Train both networks\nprint(\"\\nTraining PlainNet...\")\ncriterion = nn.CrossEntropyLoss()\nplain_optimizer = optim.Adam(plainnet.parameters(), lr=0.001)\n\nepochs = 5\nplain_losses = []\nplain_accs = []\n\nfor epoch in range(epochs):\n    plainnet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = plainnet(images)\n        loss = criterion(outputs, labels)\n\n        plain_optimizer.zero_grad()\n        loss.backward()\n        plain_optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    plain_losses.append(epoch_loss)\n    plain_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test PlainNet\nplainnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = plainnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nplain_accuracy = correct / total\nprint(f\"\\nPlainNet Test Accuracy: {plain_accuracy:.4f}\")\n\n# Comparison visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss comparison\naxes[0].plot(plain_losses, label='PlainNet', marker='o', linewidth=2)\naxes[0].plot(resnet_losses, label='ResNet-18', marker='s', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss: Plain vs Residual')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy comparison\naxes[1].plot(plain_accs, label='PlainNet', marker='o', linewidth=2)\naxes[1].plot(resnet_accs, label='ResNet-18', marker='s', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy: Plain vs Residual')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Final comparison table\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL COMPARISON\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Test Accuracy':&gt;14} | {'Difference':&gt;12}\")\nprint(\"=\"*80)\nprint(f\"{'PlainNet':&lt;15} | {plainnet_params:&gt;12,} | {plain_accuracy:&gt;14.4f} | {'baseline':&gt;12}\")\nprint(f\"{'ResNet-18':&lt;15} | {resnet_params:&gt;12,} | {resnet_accuracy:&gt;14.4f} | \"\n      f\"{'+' if resnet_accuracy &gt; plain_accuracy else ''}{(resnet_accuracy - plain_accuracy):.4f}:&gt;12}\")\nprint(\"=\"*80)\n</code></pre> <p>Key Observations: - Skip connections improve training stability - ResNet often converges faster - Performance difference more dramatic on complex datasets - Skip connections enable much deeper networks (50, 101, 152 layers)</p>"},{"location":"Week3_Day13/#exercise-5-visualize-gradient-flow-40-min-optional","title":"Exercise 5: Visualize Gradient Flow (40 min) - OPTIONAL","text":"<p>See how skip connections help gradients:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: GRADIENT FLOW VISUALIZATION\")\nprint(\"=\"*70)\n\ndef analyze_gradients(model, model_name):\n    \"\"\"Analyze gradient magnitudes throughout network\"\"\"\n    model.train()\n\n    # Forward pass\n    images, labels = next(iter(train_loader))\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n\n    # Backward pass\n    model.zero_grad()\n    loss.backward()\n\n    # Collect gradients\n    grad_norms = []\n    layer_names = []\n\n    for name, param in model.named_parameters():\n        if param.grad is not None and 'weight' in name:\n            grad_norm = param.grad.norm().item()\n            grad_norms.append(grad_norm)\n            # Simplify layer names\n            simple_name = name.split('.')[0]\n            if simple_name not in layer_names or len(layer_names) &lt; 5:\n                layer_names.append(simple_name)\n            else:\n                layer_names.append('')\n\n    return grad_norms, layer_names\n\n# Analyze both networks\nprint(\"\\nAnalyzing gradient flow...\")\nplain_grads, plain_layers = analyze_gradients(plainnet, \"PlainNet\")\nresnet_grads, resnet_layers = analyze_gradients(resnet18, \"ResNet-18\")\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# PlainNet gradients\naxes[0].bar(range(len(plain_grads)), plain_grads, color='coral', alpha=0.7)\naxes[0].set_xlabel('Layer (deeper \u2192)')\naxes[0].set_ylabel('Gradient Norm')\naxes[0].set_title('PlainNet: Gradient Magnitudes')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3, axis='y')\naxes[0].axhline(y=1e-5, color='red', linestyle='--', label='Very small gradient')\naxes[0].legend()\n\n# ResNet gradients\naxes[1].bar(range(len(resnet_grads)), resnet_grads, color='steelblue', alpha=0.7)\naxes[1].set_xlabel('Layer (deeper \u2192)')\naxes[1].set_ylabel('Gradient Norm')\naxes[1].set_title('ResNet-18: Gradient Magnitudes')\naxes[1].set_yscale('log')\naxes[1].grid(True, alpha=0.3, axis='y')\naxes[1].axhline(y=1e-5, color='red', linestyle='--', label='Very small gradient')\naxes[1].legend()\n\nplt.suptitle('Gradient Flow Comparison', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Observations: - PlainNet: Gradients get smaller in earlier layers (vanishing gradients) - ResNet: More uniform gradient distribution throughout network - Skip connections create 'highways' for gradient flow - This is why ResNet can go 100+ layers deep</p>"},{"location":"Week3_Day13/#mini-challenge-design-deep-resnet-50-min-optional","title":"Mini-Challenge: Design Deep ResNet (50 min) - OPTIONAL","text":"<p>Apply residual learning principles:</p> <p>Your challenge: Design a deeper ResNet variant</p> <p>Options to explore: 1. ResNet-34: [3,4,6,3] blocks in each stage 2. Add more channels: 64 \u2192 128 \u2192 256 \u2192 512 \u2192 1024 3. Different block structures (bottleneck blocks) 4. Experiment with initial conv size 5. Try different downsampling strategies</p> <p>Goal: Beat your Day 12 custom CNN!</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: DEEPER RESNET\")\nprint(\"=\"*70)\n\nclass DeepResNet(nn.Module):\n    \"\"\"Your deeper ResNet design\"\"\"\n    def __init__(self, num_classes=10):\n        super(DeepResNet, self).__init__()\n\n        # TODO: Design your architecture\n        # Consider: How many stages? How many blocks per stage?\n        #          What channel progression?\n\n        self.conv1 = nn.Conv2d(1, 64, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Your layers here\n        self.layer1 = self._make_layer(64, 64, 3, stride=1)\n        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\ndeep_resnet = DeepResNet()\ndeep_params = sum(p.numel() for p in deep_resnet.parameters())\n\nprint(f\"\\nYour Deep ResNet:\")\nprint(f\"Parameters: {deep_params:,}\")\nprint(f\"Architecture: [3,4,6,3] blocks (ResNet-34 style)\")\n\n# Quick training\nprint(\"\\nTraining your Deep ResNet (3 epochs)...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(deep_resnet.parameters(), lr=0.001)\n\nfor epoch in range(3):\n    deep_resnet.train()\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = deep_resnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f\"Epoch {epoch+1}/3: Accuracy = {correct/total:.4f}\")\n\n# Test\ndeep_resnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = deep_resnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ndeep_accuracy = correct / total\nprint(f\"\\n Your Deep ResNet Accuracy: {deep_accuracy:.4f}\")\n\nprint(\"\\n Deep ResNet challenge complete!\")\n</code></pre>"},{"location":"Week3_Day13/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review VGG and ResNet architectures \u2610 Understand skip connections deeply \u2610 Write daily reflection (choose 2-3 prompts below)  </p>"},{"location":"Week3_Day13/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>Why do skip connections solve the vanishing gradient problem?</li> <li>How does VGG's design philosophy differ from AlexNet?</li> <li>What makes ResNet such a breakthrough?</li> <li>How do gradients flow differently in plain vs residual networks?</li> <li>What would you consider when choosing network depth?</li> <li>What questions do you have about modern architectures?</li> </ul> <p>Next: Day 14 - Transfer Learning &amp; Data Augmentation</p>"},{"location":"Week3_Day14/","title":"Week 3, Day 14: Transfer Learning &amp; Data Augmentation","text":""},{"location":"Week3_Day14/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand transfer learning and when to use it</li> <li>Learn feature extraction vs fine-tuning</li> <li>Master data augmentation techniques</li> <li>Use pretrained models from torchvision</li> <li>Apply transfer learning to new datasets</li> <li>Design effective augmentation strategies</li> </ul>"},{"location":"Week3_Day14/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week3_Day14/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week3_Day14/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Transfer Learning Explained (20 min) Core concepts and when to use transfer learning</p> <p>\u2610 Watch: Fine-Tuning Neural Networks (15 min) Feature extraction vs full fine-tuning</p> <p>\u2610 Watch: Data Augmentation Techniques (20 min) How to increase effective dataset size</p> <p>\u2610 Watch: PyTorch Transfer Learning Tutorial (reading + 20 min) Practical implementation patterns</p>"},{"location":"Week3_Day14/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 14.2 - Fine-Tuning</p> <p>\u2610 Read: PyTorch torchvision.models docs</p> <p>\u2610 Read: PyTorch transforms docs</p>"},{"location":"Week3_Day14/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":"<p>Transfer Learning Strategy: 1. Load pretrained model (learned features from ImageNet) 2. Replace final layer (for new task) 3. Choose: freeze backbone OR fine-tune all layers</p> <p>When to use which: Feature Extraction:   - Faster training (fewer parameters)   - Less risk of overfitting (frozen weights) BUT...   - Limited adaptation to new domain  </p> <p>Fine-Tuning:   - Better adaptation to new domain   - Can achieve higher accuracy BUT...   - Requires more data to avoid overfitting   - Slower training</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copy\n\nprint(\"=\"*70)\nprint(\"DAY 14: TRANSFER LEARNING &amp; DATA AUGMENTATION\")\nprint(\"=\"*70)\n\n#### Exercise 1: Understanding Pretrained Models (45 min)\n\nprint(\"\\nEXERCISE 1: PRETRAINED MODELS\")\nprint(\"-\" * 60)\n\n# Load pretrained ResNet18\nresnet_pretrained = models.resnet18(pretrained=True)\nprint(\"\\nPretrained ResNet-18 (trained on ImageNet):\")\nprint(f\"Total parameters: {sum(p.numel() for p in resnet_pretrained.parameters()):,}\")\n\n# Examine architecture\nprint(\"\\nArchitecture summary:\")\nfor name, module in resnet_pretrained.named_children():\n    if hasattr(module, '__len__'):\n        print(f\"{name}: {len(module)} sub-modules\")\n    else:\n        print(f\"{name}: {module.__class__.__name__}\")\n\n# Look at final layer\nprint(f\"\\nOriginal classifier (for 1000 ImageNet classes):\")\nprint(resnet_pretrained.fc)\n\n# Modify for our task (10 classes)\nnum_features = resnet_pretrained.fc.in_features\nresnet_pretrained.fc = nn.Linear(num_features, 10)\nprint(f\"\\nModified classifier (for 10 classes):\")\nprint(resnet_pretrained.fc)\n\n#### Exercise 2: Feature Extraction vs Fine-Tuning (50 min)\n\nprint(\"\\n\\nEXERCISE 2: FEATURE EXTRACTION VS FINE-TUNING\")\nprint(\"-\" * 60)\n\n# Prepare data (CIFAR-10 for more challenging task)\nprint(\"\\nLoading CIFAR-10...\")\n\n# Transforms for pretrained models (expecting ImageNet statistics)\ntransform_pretrained = transforms.Compose([\n    transforms.Resize(32),  # Ensure 32x32\n    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])  # ImageNet stats\n])\n\n# For MNIST (grayscale to RGB)\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                              download=True, transform=transform_pretrained)\ntest_dataset = datasets.MNIST(root='./data', train=False,\n                             download=True, transform=transform_pretrained)\n\n# Split train into train/val\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_data, val_data = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"Training: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_dataset)}\")\n\n# Strategy 1: Feature Extraction (freeze backbone)\nprint(\"\\n1. Feature Extraction (Freeze Backbone)\")\nprint(\"-\" * 60)\n\nmodel_feature_extract = models.resnet18(pretrained=True)\n# Modify final layer\nmodel_feature_extract.fc = nn.Linear(model_feature_extract.fc.in_features, 10)\n\n# Freeze all layers except final\nfor param in model_feature_extract.parameters():\n    param.requires_grad = False\n# Unfreeze final layer\nfor param in model_feature_extract.fc.parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in model_feature_extract.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,} (only final layer)\")\n\n# Train\ndef train_model(model, train_loader, val_loader, epochs=3, lr=0.001, name=\"Model\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n\n        for images, labels in train_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"{name} Epoch {epoch+1}/{epochs}: \"\n              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return history\n\nprint(\"\\nTraining feature extraction model...\")\nhistory_extract = train_model(model_feature_extract, train_loader, val_loader, \n                              epochs=3, lr=0.001, name=\"Feature Extract\")\n\n# Strategy 2: Fine-Tuning (train all layers)\nprint(\"\\n\\n2. Fine-Tuning (Unfreeze All Layers)\")\nprint(\"-\" * 60)\n\nmodel_finetune = models.resnet18(pretrained=True)\nmodel_finetune.fc = nn.Linear(model_finetune.fc.in_features, 10)\n\n# All parameters trainable\nfor param in model_finetune.parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in model_finetune.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,} (all layers)\")\n\nprint(\"\\nTraining fine-tuned model...\")\nhistory_finetune = train_model(model_finetune, train_loader, val_loader,\n                               epochs=3, lr=0.0001, name=\"Fine-Tune\")  # Lower LR!\n\n# Compare\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"COMPARISON: Feature Extraction vs Fine-Tuning\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(history_extract['val_acc'])+1)\n\naxes[0].plot(epochs_range, history_extract['val_loss'], label='Feature Extract', marker='o')\naxes[0].plot(epochs_range, history_finetune['val_loss'], label='Fine-Tune', marker='s')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Validation Loss')\naxes[0].set_title('Validation Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(epochs_range, history_extract['val_acc'], label='Feature Extract', marker='o')\naxes[1].plot(epochs_range, history_finetune['val_acc'], label='Fine-Tune', marker='s')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('Validation Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal validation accuracy:\")\nprint(f\"  Feature Extraction: {history_extract['val_acc'][-1]:.4f}\")\nprint(f\"  Fine-Tuning: {history_finetune['val_acc'][-1]:.4f}\")\n\n\n\n#### Exercise 3: Data Augmentation (45 min)\n\nprint(\"\\n\\nEXERCISE 3: DATA AUGMENTATION\")\nprint(\"-\" * 60)\n\n# Show augmentation effects\naugmentations = {\n    'Original': transforms.Compose([\n        transforms.ToTensor(),\n    ]),\n    'Random Crop': transforms.Compose([\n        transforms.RandomCrop(28, padding=4),\n        transforms.ToTensor(),\n    ]),\n    'Horizontal Flip': transforms.Compose([\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.ToTensor(),\n    ]),\n    'Rotation': transforms.Compose([\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n    ]),\n    'Color Jitter': transforms.Compose([\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n    ]),\n    'Random Affine': transforms.Compose([\n        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ToTensor(),\n    ]),\n}\n\n# Get sample image\nmnist_raw = datasets.MNIST(root='./data', train=True, download=True)\nsample_img, label = mnist_raw[0]\n\n# Apply augmentations\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor idx, (name, transform) in enumerate(augmentations.items()):\n    augmented = transform(sample_img)\n    axes[idx].imshow(augmented.squeeze(), cmap='gray')\n    axes[idx].set_title(name)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Data Augmentation Examples (Original Label: {label})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Train with strong augmentation\nprint(\"\\nTraining with strong augmentation...\")\n\ntransform_augmented = transforms.Compose([\n    transforms.RandomCrop(28, padding=4),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_augmented = datasets.MNIST(root='./data', train=True, \n                                 download=True, transform=transform_augmented)\ntrain_aug_loader = DataLoader(train_augmented, batch_size=128, shuffle=True)\n\n# Simple CNN to test augmentation effect\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)\n        x = torch.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Train without augmentation\nmodel_no_aug = SimpleCNN()\nprint(\"Training WITHOUT augmentation...\")\nhistory_no_aug = train_model(model_no_aug, train_loader, val_loader, \n                             epochs=5, name=\"No Aug\")\n\n# Train with augmentation  \nmodel_with_aug = SimpleCNN()\nprint(\"\\nTraining WITH augmentation...\")\nhistory_with_aug = train_model(model_with_aug, train_aug_loader, val_loader,\n                               epochs=5, name=\"With Aug\")\n\n# Compare\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nepochs_range = range(1, 6)\nax.plot(epochs_range, history_no_aug['val_acc'], label='No Augmentation', marker='o')\nax.plot(epochs_range, history_with_aug['val_acc'], label='With Augmentation', marker='s')\nax.set_xlabel('Epoch')\nax.set_ylabel('Validation Accuracy')\nax.set_title('Effect of Data Augmentation')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal validation accuracy:\")\nprint(f\"  Without augmentation: {history_no_aug['val_acc'][-1]:.4f}\")\nprint(f\"  With augmentation: {history_with_aug['val_acc'][-1]:.4f}\")\nprint(f\"  Improvement: {history_with_aug['val_acc'][-1] - history_no_aug['val_acc'][-1]:.4f}\")\n\nprint(\"\\n Exercises 1-3 complete\")\n</code></pre>"},{"location":"Week3_Day14/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week3_Day14/#mini-challenge-complete-transfer-learning-pipeline-35-hours","title":"Mini-Challenge: Complete Transfer Learning Pipeline (3.5 hours)","text":"<p>Your challenge: Build the best possible model for CIFAR-10</p> <p>Steps: 1. Design augmentation strategy 2. Choose pretrained model (ResNet18, ResNet50, VGG, etc.) 3. Decide: feature extraction or fine-tuning? 4. Train and evaluate 5. Document your choices</p> <p>Target: &gt;90% accuracy on CIFAR-10</p> <pre><code>print(\"\\n\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: COMPLETE TRANSFER LEARNING PROJECT\")\nprint(\"=\"*70)\n\n\n\n# Load CIFAR-10\nprint(\"\\nLoading CIFAR-10 (color images, 10 classes)...\")\n\n# Design your augmentation\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ncifar_train = datasets.CIFAR10(root='./data', train=True, \n                               download=True, transform=transform_train)\ncifar_test = datasets.CIFAR10(root='./data', train=False,\n                              download=True, transform=transform_test)\n\n# Split train into train/val\ntrain_size = int(0.9 * len(cifar_train))\nval_size = len(cifar_train) - train_size\ncifar_train_data, cifar_val_data = random_split(cifar_train, [train_size, val_size])\n\ncifar_train_loader = DataLoader(cifar_train_data, batch_size=128, shuffle=True, num_workers=2)\ncifar_val_loader = DataLoader(cifar_val_data, batch_size=128, shuffle=False, num_workers=2)\ncifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)\n\nprint(f\"CIFAR-10 loaded:\")\nprint(f\"  Training: {len(cifar_train_data)}\")\nprint(f\"  Validation: {len(cifar_val_data)}\")\nprint(f\"  Test: {len(cifar_test)}\")\n\n# Choose and modify pretrained model\nprint(\"\\nUsing pretrained ResNet18...\")\nmodel = models.resnet18(pretrained=True)\n\n# Modify for CIFAR-10 (10 classes)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)\n\n# Strategy: Fine-tune all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# Train with learning rate scheduling\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nprint(\"\\nTraining on CIFAR-10...\")\nepochs = 10\nbest_val_acc = 0\nhistory = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n\nfor epoch in range(epochs):\n    # Train\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in cifar_train_loader:\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(cifar_train_loader)\n    train_acc = correct / total\n\n    # Validate\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in cifar_val_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_acc = correct / total\n\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n          f\"Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n\n    # Save best model\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_cifar10_model.pth')\n\n    scheduler.step()\n\n# Load best model and test\nmodel.load_state_dict(torch.load('best_cifar10_model.pth'))\nmodel.eval()\n\ncorrect = 0\ntotal = 0\nclass_correct = [0] * 10\nclass_total = [0] * 10\n\nwith torch.no_grad():\n    for images, labels in cifar_test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        # Per-class accuracy\n        for label, prediction in zip(labels, predicted):\n            if label == prediction:\n                class_correct[label] += 1\n            class_total[label] += 1\n\ntest_acc = correct / total\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS\")\nprint(\"=\"*70)\nprint(f\"Best validation accuracy: {best_val_acc:.4f}\")\nprint(f\"Test accuracy: {test_acc:.4f}\")\n\n# Per-class accuracy\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nprint(\"\\nPer-class accuracy:\")\nfor i, class_name in enumerate(classes):\n    acc = class_correct[i] / class_total[i] if class_total[i] &gt; 0 else 0\n    print(f\"  {class_name:8s}: {acc:.4f} ({class_correct[i]}/{class_total[i]})\")\n\n# Visualize training\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(history['train_acc'])+1)\n\naxes[0].plot(epochs_range, history['train_loss'])\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(epochs_range, history['train_acc'], label='Train')\naxes[1].plot(epochs_range, history['val_acc'], label='Validation')\naxes[1].axhline(y=test_acc, color='r', linestyle='--', label=f'Test: {test_acc:.4f}')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Challenge complete!\")\nprint(\"\\n All Day 14 exercises complete\")\n</code></pre>"},{"location":"Week3_Day14/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review transfer learning concepts \u2610 Understand when to freeze vs fine-tune \u2610 Document augmentation strategies \u2610 Write daily reflection  </p>"},{"location":"Week3_Day14/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept learned today?</li> <li>When should you use feature extraction vs fine-tuning?</li> <li>How does data augmentation improve models?</li> <li>What augmentations are appropriate for your domain?</li> <li>How did transfer learning compare to training from scratch?</li> <li>What would you do differently on your next transfer learning project?</li> </ul> <p>Next: Day 15 - CIFAR-10 Project (Capstone)</p>"},{"location":"Week3_Day15/","title":"Week 3, Day 15: CIFAR-10 Project - Color Image Classification","text":""},{"location":"Week3_Day15/#daily-goals","title":"Daily Goals","text":"<ul> <li>Complete end-to-end CIFAR-10 classification project</li> <li>Apply all Week 3 concepts (CNNs, architectures, transfer learning, augmentation)</li> <li>Achieve &gt;85% accuracy (target: &gt;90%)</li> <li>Create professional documentation</li> <li>Build portfolio-ready computer vision project</li> </ul>"},{"location":"Week3_Day15/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week3_Day15/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week3_Day15/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: CIFAR-10 Image Classification walkthrough (15 min)</p> <p>\u2610 Optional Review: Any Week 3 videos as needed (15 min)</p>"},{"location":"Week3_Day15/#project-briefing-30-min","title":"Project Briefing (30 min)","text":"<p>CIFAR-10 IMAGE CLASSIFICATION PROJECT</p> <p>Dataset: CIFAR-10 - 60,000 color images (32\u00d732 RGB) - 10 classes: plane, car, bird, cat, deer, dog, frog, horse, ship, truck - 50,000 training, 10,000 test - More challenging than MNIST!</p> <p>Goal: Build CNN achieving &gt;85% accuracy (target: &gt;90%)</p> <p>Project Structure: - Phase 1: Data Exploration (30 min) - Phase 2: Baseline Model (45 min)   - Phase 3: Improved Architecture (60 min) - Phase 4: Transfer Learning (60 min) - Phase 5: Analysis &amp; Documentation (45 min)  </p> <p>Success Criteria: - Minimum: &gt;70% test accuracy - Target: &gt;85% test accuracy - Stretch: &gt;90% test accuracy - Professional documentation - Clear visualizations</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport time\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"=\"*70)\nprint(\"CIFAR-10 IMAGE CLASSIFICATION PROJECT\")\nprint(\"Week 3, Day 15 Capstone\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"Week3_Day15/#phase-1-data-exploration-30-min","title":"Phase 1: Data Exploration (30 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: DATA EXPLORATION\")\nprint(\"=\"*70)\n\n# Load CIFAR-10\ntransform_basic = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\n\ncifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_basic)\ncifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_basic)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"Training samples: {len(cifar_train)}\")\nprint(f\"Test samples: {len(cifar_test)}\")\nprint(f\"Image shape: {cifar_train[0][0].shape}\")  # [C, H, W]\nprint(f\"Number of classes: 10\")\n\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n# Visualize samples\nfig, axes = plt.subplots(5, 10, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i in range(50):\n    img, label = cifar_train[i]\n    # Denormalize for visualization\n    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\n    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\n    img = torch.clamp(img, 0, 1)\n\n    axes[i].imshow(img.permute(1, 2, 0))\n    axes[i].set_title(classes[label], fontsize=8)\n    axes[i].axis('off')\n\nplt.suptitle('CIFAR-10 Dataset Samples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Class distribution\nlabels = [cifar_train[i][1] for i in range(len(cifar_train))]\nunique, counts = np.unique(labels, return_counts=True)\n\nplt.figure(figsize=(12, 6))\nplt.bar([classes[i] for i in unique], counts, color='steelblue', edgecolor='black')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('CIFAR-10 Class Distribution (Training Set)')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3, axis='y')\nfor i, (cls, count) in enumerate(zip(unique, counts)):\n    plt.text(i, count + 50, str(count), ha='center', fontsize=10)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nClass distribution (perfectly balanced):\")\nfor cls_idx, count in zip(unique, counts):\n    print(f\"  {classes[cls_idx]:8s}: {count} samples\")\n\nprint(\"\\n Data exploration complete\")\n</code></pre>"},{"location":"Week3_Day15/#phase-2-baseline-model-45-min","title":"Phase 2: Baseline Model (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: BASELINE MODEL\")\nprint(\"=\"*70)\n\n# Split train into train/val\ntrain_size = int(0.9 * len(cifar_train))\nval_size = len(cifar_train) - train_size\ntrain_data, val_data = random_split(cifar_train, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\ntest_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)\n\nprint(f\"Data split:\")\nprint(f\"  Training: {len(train_data)}\")\nprint(f\"  Validation: {len(val_data)}\")\nprint(f\"  Test: {len(cifar_test)}\")\n\n# Simple baseline CNN\nclass BaselineCNN(nn.Module):\n    def __init__(self):\n        super(BaselineCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, 10)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)  # 16x16\n        x = torch.relu(self.conv2(x))\n        x = self.pool(x)  # 8x8\n        x = torch.relu(self.conv3(x))\n        x = self.pool(x)  # 4x4\n        x = x.view(-1, 128 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nbaseline = BaselineCNN()\nprint(\"\\nBaseline CNN:\")\nprint(baseline)\nprint(f\"Parameters: {sum(p.numel() for p in baseline.parameters()):,}\")\n\n# Training function\ndef train_model(model, train_loader, val_loader, epochs, lr, model_name=\"Model\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for images, labels in train_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n\n        # Validate\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        train_acc = train_correct / train_total\n        val_acc = val_correct / val_total\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        print(f\"{model_name} Epoch {epoch+1}/{epochs}: \"\n              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return history\n\nprint(\"\\nTraining baseline...\")\nbaseline_history = train_model(baseline, train_loader, val_loader, \n                               epochs=10, lr=0.001, model_name=\"Baseline\")\n\n# Test baseline\nbaseline.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = baseline(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nbaseline_acc = correct / total\nprint(f\"\\n Baseline Test Accuracy: {baseline_acc:.4f}\")\n\nprint(\"\\n Baseline model complete\")\n</code></pre>"},{"location":"Week3_Day15/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week3_Day15/#phase-3-improved-architecture-60-min","title":"Phase 3: Improved Architecture (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: IMPROVED ARCHITECTURE WITH MODERN TECHNIQUES\")\nprint(\"=\"*70)\n\n# Add data augmentation\ntransform_augmented = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\n\ntrain_augmented = datasets.CIFAR10(root='./data', train=True, \n                                   download=True, transform=transform_augmented)\ntrain_aug_data, _ = random_split(train_augmented, [train_size, val_size])\ntrain_aug_loader = DataLoader(train_aug_data, batch_size=128, shuffle=True, num_workers=2)\n\n# Improved CNN with batch norm and better architecture\nclass ImprovedCNN(nn.Module):\n    def __init__(self):\n        super(ImprovedCNN, self).__init__()\n\n        # Block 1\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        # Block 2\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n\n        # Block 3\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.3)\n\n        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # Block 1\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)  # 16x16\n        x = self.dropout(x)\n\n        # Block 2\n        x = torch.relu(self.bn3(self.conv3(x)))\n        x = torch.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)  # 8x8\n        x = self.dropout(x)\n\n        # Block 3\n        x = torch.relu(self.bn5(self.conv5(x)))\n        x = torch.relu(self.bn6(self.conv6(x)))\n        x = self.pool(x)  # 4x4\n        x = self.dropout(x)\n\n        # Classifier\n        x = x.view(-1, 256 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nimproved = ImprovedCNN()\nprint(\"\\nImproved CNN:\")\nprint(f\"Parameters: {sum(p.numel() for p in improved.parameters()):,}\")\n\nprint(\"\\nTraining improved model with augmentation...\")\nimproved_history = train_model(improved, train_aug_loader, val_loader,\n                               epochs=15, lr=0.001, model_name=\"Improved\")\n\n# Test improved\nimproved.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = improved(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nimproved_acc = correct / total\nprint(f\"\\n Improved Test Accuracy: {improved_acc:.4f}\")\n\nprint(\"\\n Improved architecture complete\")\n</code></pre>"},{"location":"Week3_Day15/#phase-4-transfer-learning-60-min","title":"Phase 4: Transfer Learning (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: TRANSFER LEARNING WITH RESNET18\")\nprint(\"=\"*70)\n\n# Load pretrained ResNet18\nresnet = models.resnet18(pretrained=True)\n\n# Modify for CIFAR-10\nnum_features = resnet.fc.in_features\nresnet.fc = nn.Linear(num_features, 10)\n\n# Fine-tune all layers\nfor param in resnet.parameters():\n    param.requires_grad = True\n\nprint(f\"\\nResNet-18 (pretrained on ImageNet):\")\nprint(f\"Total parameters: {sum(p.numel() for p in resnet.parameters()):,}\")\n\n# Train with learning rate scheduling\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\nprint(\"\\nTraining ResNet-18 with transfer learning...\")\nepochs = 20\nresnet_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\nbest_val_acc = 0\n\nfor epoch in range(epochs):\n    # Train\n    resnet.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n\n    for images, labels in train_aug_loader:\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    # Validate\n    resnet.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = resnet(images)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    train_loss /= len(train_aug_loader)\n    val_loss /= len(val_loader)\n    train_acc = train_correct / train_total\n    val_acc = val_correct / val_total\n\n    resnet_history['train_loss'].append(train_loss)\n    resnet_history['val_loss'].append(val_loss)\n    resnet_history['train_acc'].append(train_acc)\n    resnet_history['val_acc'].append(val_acc)\n\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(resnet.state_dict(), 'best_cifar10_resnet.pth')\n\n    print(f\"ResNet Epoch {epoch+1}/{epochs}: \"\n          f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n    scheduler.step()\n\n# Load best and test\nresnet.load_state_dict(torch.load('best_cifar10_resnet.pth'))\nresnet.eval()\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nresnet_acc = correct / total\nprint(f\"\\n ResNet-18 Test Accuracy: {resnet_acc:.4f}\")\n\nprint(\"\\n Transfer learning complete\")\n</code></pre>"},{"location":"Week3_Day15/#phase-5-analysis-documentation-45-min","title":"Phase 5: Analysis &amp; Documentation (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: COMPREHENSIVE ANALYSIS\")\nprint(\"=\"*70)\n\n# Model comparison\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Model':&lt;20} | {'Test Accuracy':&gt;14} | {'Improvement':&gt;12}\")\nprint(\"=\"*70)\nprint(f\"{'Baseline CNN':&lt;20} | {baseline_acc:&gt;14.4f} | {'baseline':&gt;12}\")\nprint(f\"{'Improved CNN':&lt;20} | {improved_acc:&gt;14.4f} | {f'+{(improved_acc-baseline_acc):.4f}':&gt;12}\")\nprint(f\"{'ResNet-18 Transfer':&lt;20} | {resnet_acc:&gt;14.4f} | {f'+{(resnet_acc-baseline_acc):.4f}':&gt;12}\")\nprint(\"=\"*70)\n\n# Training curves\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Baseline\naxes[0,0].plot(baseline_history['train_loss'], label='Train')\naxes[0,0].plot(baseline_history['val_loss'], label='Val')\naxes[0,0].set_title('Baseline - Loss')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\naxes[1,0].plot(baseline_history['train_acc'], label='Train')\naxes[1,0].plot(baseline_history['val_acc'], label='Val')\naxes[1,0].axhline(y=baseline_acc, color='r', linestyle='--', label=f'Test: {baseline_acc:.4f}')\naxes[1,0].set_title('Baseline - Accuracy')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Improved\naxes[0,1].plot(improved_history['train_loss'], label='Train')\naxes[0,1].plot(improved_history['val_loss'], label='Val')\naxes[0,1].set_title('Improved - Loss')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\naxes[1,1].plot(improved_history['train_acc'], label='Train')\naxes[1,1].plot(improved_history['val_acc'], label='Val')\naxes[1,1].axhline(y=improved_acc, color='r', linestyle='--', label=f'Test: {improved_acc:.4f}')\naxes[1,1].set_title('Improved - Accuracy')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\n# ResNet\naxes[0,2].plot(resnet_history['train_loss'], label='Train')\naxes[0,2].plot(resnet_history['val_loss'], label='Val')\naxes[0,2].set_title('ResNet-18 - Loss')\naxes[0,2].legend()\naxes[0,2].grid(True, alpha=0.3)\n\naxes[1,2].plot(resnet_history['train_acc'], label='Train')\naxes[1,2].plot(resnet_history['val_acc'], label='Val')\naxes[1,2].axhline(y=resnet_acc, color='r', linestyle='--', label=f'Test: {resnet_acc:.4f}')\naxes[1,2].set_title('ResNet-18 - Accuracy')\naxes[1,2].legend()\naxes[1,2].grid(True, alpha=0.3)\n\nplt.suptitle('Training Comparison', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\nprint(\"\\nGenerating confusion matrix for best model...\")\nresnet.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix - ResNet-18 (Accuracy: {resnet_acc:.4f})')\nplt.tight_layout()\nplt.show()\n\n# Per-class accuracy\nprint(\"\\nPer-class Performance:\")\nprint(\"-\" * 50)\nfor i in range(10):\n    class_correct = cm[i, i]\n    class_total = cm[i, :].sum()\n    class_acc = class_correct / class_total if class_total &gt; 0 else 0\n    print(f\"{classes[i]:8s}: {class_acc:.4f} ({class_correct}/{class_total})\")\n\n# Visualize mistakes\nprint(\"\\nVisualizing mistakes...\")\nresnet.eval()\nmistakes = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        probs = torch.softmax(outputs, dim=1)\n        confidences, predicted = torch.max(probs, 1)\n\n        mask = predicted != labels\n        if mask.sum() &gt; 0:\n            for img, true, pred, conf in zip(images[mask], labels[mask], \n                                             predicted[mask], confidences[mask]):\n                mistakes.append((img, true.item(), pred.item(), conf.item()))\n                if len(mistakes) &gt;= 20:\n                    break\n        if len(mistakes) &gt;= 20:\n            break\n\nfig, axes = plt.subplots(4, 5, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, (img, true_label, pred_label, conf) in enumerate(mistakes[:20]):\n    # Denormalize\n    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\n    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\n    img = torch.clamp(img, 0, 1)\n\n    axes[i].imshow(img.permute(1, 2, 0))\n    axes[i].set_title(f'True: {classes[true_label]}\\nPred: {classes[pred_label]} ({conf:.2f})', \n                     fontsize=9)\n    axes[i].axis('off')\n\nplt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Final report - this is printed to the screen and exported as a text file\nreport = f\"\"\"\n{'='*70}\nCIFAR-10 PROJECT FINAL REPORT\n{'='*70}\n\nDATASET\n-------\nTraining samples: 50,000\nTest samples: 10,000\nImage size: 32\u00d732 RGB\nClasses: 10 (balanced)\n\nMODELS EVALUATED\n----------------\n1. Baseline CNN\n   - Simple 3-layer CNN\n   - No augmentation, basic dropout\n   - Test Accuracy: {baseline_acc:.4f}\n\n2. Improved CNN\n   - Deeper (6 conv layers)\n   - Batch normalization\n   - Data augmentation\n   - Test Accuracy: {improved_acc:.4f}\n   - Improvement: +{(improved_acc-baseline_acc):.4f}\n\n3. ResNet-18 (Transfer Learning)\n   - Pretrained on ImageNet\n   - Fine-tuned all layers\n   - Data augmentation + LR scheduling\n   - Test Accuracy: {resnet_acc:.4f}\n   - Improvement: +{(resnet_acc-baseline_acc):.4f}\n\nBEST MODEL: ResNet-18 Transfer Learning\nStatus: {' TARGET ACHIEVED (&gt;85%)' if resnet_acc &gt; 0.85 else ' Below target'}\n        {' STRETCH GOAL (&gt;90%)!' if resnet_acc &gt; 0.90 else ''}\n\nKEY INSIGHTS\n------------\n1. Data augmentation critical for CIFAR-10\n2. Batch normalization stabilizes training\n3. Transfer learning provides major boost\n4. Depth matters but skip connections help\n\nTECHNIQUES APPLIED\n------------------\n Custom CNN architectures\n Batch normalization\n Dropout regularization\n Data augmentation\n Transfer learning\n Learning rate scheduling\n Early stopping\n\n{'='*70}\nWeek 3 Complete! \n{'='*70}\n\"\"\"\n\nprint(report) \n\n# Save report\nwith open('cifar10_project_report.txt', 'w') as f:\n    f.write(report)\n\nprint(\"\\n\ud83d\udcc4 Report saved: cifar10_project_report.txt\")\nprint(\"\ud83d\udcbe Model saved: best_cifar10_resnet.pth\")\n\nprint(\"\\n Project complete!\")\n</code></pre> <p>KEY INSIGHTS</p> <ol> <li>Data augmentation critical for CIFAR-10</li> <li>Batch normalization stabilizes training</li> <li>Transfer learning provides major boost</li> <li>Depth matters but skip connections help</li> </ol> <p>TECHNIQUES APPLIED  - Custom CNN architectures  - Batch normalization  - Dropout regularization  - Data augmentation  - Transfer learning  - Learning rate scheduling  - Early stopping  </p>"},{"location":"Week3_Day15/#week-3-reflection-prompts-address-all","title":"Week 3 Reflection Prompts (Address All):","text":"<ul> <li>What was the most valuable thing you learned this week?</li> <li>How did your understanding of CNNs evolve?</li> <li>What was your biggest challenge? How did you overcome it?</li> <li>How does your CIFAR-10 accuracy compare to expectations?</li> <li>What connections did you make between Days 11-15?</li> <li>How confident do you feel about computer vision now?</li> <li>What would you do differently on your next CV project?</li> </ul>"},{"location":"Week3_Day15/#week-3-achievement-checklist","title":"Week 3 Achievement Checklist:","text":"<p>\u2610 Understood convolutions and their advantages \u2610 Implemented CNNs from scratch \u2610 Built LeNet, AlexNet, VGG, ResNet \u2610 Mastered transfer learning \u2610 Applied data augmentation effectively \u2610 Completed CIFAR-10 project with &gt;85% accuracy \u2610 Created professional documentation \u2610 Built portfolio-ready CV project  </p>"},{"location":"Week3_Day15/#week-3-complete","title":"Week 3 Complete!","text":"<p>Achievements Unlocked: -  CNN fundamentals mastered -  Classic and modern architectures -  Transfer learning proficiency -  Production CV skills -  CIFAR-10 portfolio project</p> <p>What You Can Now Do: - Build CNNs for any image classification task - Use pretrained models effectively - Design custom architectures - Understand computer vision research papers - Deploy image classification systems</p> <p>Next Week Preview: Week 4 covers advanced topics: RNNs, LSTMs, Transformers, and GANs!</p> <p>Congratulations on completing Week 3! </p> <p>You now have solid computer vision skills and a portfolio project to show for it.</p>"},{"location":"Week3_Overview/","title":"Week 3 Overview: Deep Learning &amp; Convolutional Neural Networks","text":""},{"location":"Week3_Overview/#introduction","title":"Introduction","text":"<p>Week 3 introduces Convolutional Neural Networks (CNNs) - the architecture that revolutionized computer vision. You'll learn why convolutions work, how to build modern CNN architectures, and complete CIFAR-10 color image classification. This week bridges the gap between understanding neural networks (Week 2) and applying them to real-world image problems.</p> <p>CNNs are the backbone of facial recognition, self-driving cars, medical image analysis, and countless other computer vision applications. By understanding CNNs deeply, you'll grasp the foundation of modern AI vision systems.</p>"},{"location":"Week3_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Understand convolutional layers and how they detect features hierarchically</li> <li>Learn pooling, padding, and stride operations</li> <li>Implement CNNs from scratch in NumPy and PyTorch</li> <li>Study classic architectures: LeNet, AlexNet, VGG, ResNet</li> <li>Master transfer learning and data augmentation</li> <li>Complete CIFAR-10 classification project (10 classes, 32\u00d732 color images)</li> <li>Achieve &gt;85% accuracy on CIFAR-10</li> </ul>"},{"location":"Week3_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 11: CNN Theory - Convolutions, Filters, Feature Maps</li> <li>Day 12: Classic Architectures - LeNet, AlexNet  </li> <li>Day 13: Modern Architectures - VGG, ResNet, Skip Connections</li> <li>Day 14: Transfer Learning &amp; Data Augmentation</li> <li>Day 15: CIFAR-10 Project - Color Image Classification</li> </ul>"},{"location":"Week3_Overview/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>By the end of Week 3, you will be able to:</p> <ul> <li>Explain how convolutions exploit spatial structure in images</li> <li>Calculate output sizes for conv/pool layers given input size, kernel, stride, padding</li> <li>Implement 2D convolutions from scratch</li> <li>Build and train CNNs in PyTorch</li> <li>Understand weight sharing and translation invariance</li> <li>Recognize classic CNN architectures (LeNet through ResNet)</li> <li>Apply transfer learning using pretrained models</li> <li>Design effective data augmentation strategies</li> <li>Complete end-to-end image classification projects</li> <li>Visualize and interpret learned features</li> </ul>"},{"location":"Week3_Overview/#primary-resources","title":"Primary Resources","text":""},{"location":"Week3_Overview/#video-resources","title":"Video Resources","text":"<p>3Blue1Brown &amp; Visual Explanations (Primary) - Visual, intuitive explanations of convolutions - How CNNs \"see\" and process images - Perfect for building deep intuition</p> <p>StatQuest by Josh Starmer (Supporting) - Clear breakdowns of CNN components - Detailed technical explanations - Complements visual intuition</p> <p>Research Paper Walkthroughs (Advanced) - Yannic Kilcher for paper deep-dives - Understanding architectural innovations - Historical context and motivation</p> <p>PyTorch Tutorials (Practical) - Official torchvision tutorials - Transfer learning patterns - Production-ready code</p>"},{"location":"Week3_Overview/#text-resources","title":"Text Resources","text":"<p>Dive into Deep Learning (d2l.ai) (Primary) - Chapter 7: Convolutional Neural Networks (theory) - Chapter 8: Modern CNN Architectures (applications) - Mathematical foundations with PyTorch code - Interactive examples</p> <p>Original Research Papers (Secondary) - LeNet-5 (1998): Gradient-Based Learning Applied to Document Recognition - AlexNet (2012): ImageNet Classification with Deep CNNs - VGG (2014): Very Deep Convolutional Networks - ResNet (2015): Deep Residual Learning for Image Recognition</p> <p>PyTorch Documentation (Reference) - torch.nn.Conv2d, MaxPool2d - torchvision.models (pretrained models) - torchvision.transforms (data augmentation)</p>"},{"location":"Week3_Overview/#week-progression","title":"Week Progression","text":""},{"location":"Week3_Overview/#days-11-12-foundation","title":"Days 11-12: Foundation","text":"<p>Build understanding from first principles: - What convolutions are and why they work - Implement convolutions manually - Study how CNNs evolved (LeNet \u2192 AlexNet) - Understand the ImageNet breakthrough</p>"},{"location":"Week3_Overview/#days-13-14-modern-techniques","title":"Days 13-14: Modern Techniques","text":"<p>Master state-of-the-art approaches: - Very deep networks (VGG) - Skip connections (ResNet) - Transfer learning - Data augmentation</p>"},{"location":"Week3_Overview/#day-15-integration","title":"Day 15: Integration","text":"<p>Apply everything in complete project: - CIFAR-10 classification - Multiple architectures - Comprehensive analysis - Portfolio-ready documentation</p>"},{"location":"Week3_Overview/#prerequisites","title":"Prerequisites","text":"<p>From Week 2, you should be comfortable with: - PyTorch basics (tensors, autograd, nn.Module) - Training loops and validation - DataLoaders and datasets - Loss functions and optimizers - Model evaluation</p>"},{"location":"Week3_Overview/#daily-time-allocation","title":"Daily Time Allocation","text":"<p>Same structure as previous weeks: - Morning Session (4 hours): Video learning + foundational exercises - Afternoon Session (4 hours): Advanced exercises + mini-challenge - Reflection (30 minutes): Consolidate learning</p> <p>Total: 8 hours per day</p>"},{"location":"Week3_Overview/#new-tools-this-week","title":"New Tools This Week","text":""},{"location":"Week3_Overview/#torchvisionmodels","title":"torchvision.models","text":"<pre><code>import torchvision.models as models\n\n# Load pretrained ResNet\nmodel = models.resnet18(pretrained=True)\n</code></pre>"},{"location":"Week3_Overview/#torchvisiontransforms","title":"torchvision.transforms","text":"<pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n</code></pre>"},{"location":"Week3_Overview/#gpu-acceleration-recommended","title":"GPU Acceleration (Recommended)","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n</code></pre> <p>Colab Setup: Runtime \u2192 Change runtime type \u2192 GPU (free!)</p>"},{"location":"Week3_Overview/#week-2-vs-week-3-comparison","title":"Week 2 vs Week 3 Comparison","text":"Aspect Week 2 (Fully Connected) Week 3 (Convolutional) Input Flattened vectors 2D/3D spatial data Parameters ~100K ~1-10M Key Operation Matrix multiplication Convolution Structure Treats pixels independently Exploits spatial relationships Invariance None Translation invariance Dataset MNIST (28\u00d728 grayscale) CIFAR-10 (32\u00d732 RGB) Architecture Simple (2-4 layers) Deep hierarchies (10-100+ layers) Training CPU sufficient GPU strongly recommended"},{"location":"Week3_Overview/#common-challenges","title":"Common Challenges","text":""},{"location":"Week3_Overview/#challenge-1-convolutions-are-conceptually-confusing","title":"Challenge 1: \"Convolutions are conceptually confusing\"","text":"<p>Solution: - Start with 1D convolutions (signals) - Draw 2D convolutions by hand - Use online visualization tools - Connect to familiar concepts (blurring, edge detection)</p>"},{"location":"Week3_Overview/#challenge-2-too-many-architectures-to-remember","title":"Challenge 2: \"Too many architectures to remember\"","text":"<p>Solution: - Focus on understanding why architectures evolved - Learn principles, not memorization - Key concepts: depth, skip connections, efficiency</p>"},{"location":"Week3_Overview/#challenge-3-training-is-too-slow","title":"Challenge 3: \"Training is too slow\"","text":"<p>Solution: - Use GPU in Colab (free!) - Start with small models for debugging - Use subsets of data initially - Leverage transfer learning</p>"},{"location":"Week3_Overview/#challenge-4-accuracy-plateaus-around-70","title":"Challenge 4: \"Accuracy plateaus around 70%\"","text":"<p>Solution: - Check learning rate (try scheduler) - Add data augmentation - Try deeper architecture - Use transfer learning - Verify batch norm is working - Check for implementation bugs</p>"},{"location":"Week3_Overview/#challenge-5-shape-mismatches-everywhere","title":"Challenge 5: \"Shape mismatches everywhere\"","text":"<p>Solution: - Print tensor shapes after every layer - Calculate expected output sizes manually - Use torchsummary package - Understand padding effects</p>"},{"location":"Week3_Overview/#tips-for-success","title":"Tips for Success","text":""},{"location":"Week3_Overview/#visualization-is-key","title":"Visualization is Key","text":"<ul> <li>CNNs are inherently visual</li> <li>Visualize filters, feature maps, predictions</li> <li>Use tools like matplotlib, tensorboard</li> <li>See what the network \"sees\"</li> </ul>"},{"location":"Week3_Overview/#start-simple-add-complexity","title":"Start Simple, Add Complexity","text":"<ul> <li>Begin with tiny CNN (1-2 conv layers)</li> <li>Verify it works</li> <li>Gradually add depth, techniques</li> <li>Debug incrementally</li> </ul>"},{"location":"Week3_Overview/#understand-dont-memorize","title":"Understand, Don't Memorize","text":"<ul> <li>Why does this architecture choice help?</li> <li>What problem does this solve?</li> <li>When would I use this?</li> </ul>"},{"location":"Week3_Overview/#leverage-gpu","title":"Leverage GPU","text":"<ul> <li>CNNs benefit enormously from GPU</li> <li>Colab provides free GPU access</li> <li>Expect 10-50x speedup</li> </ul>"},{"location":"Week3_Overview/#connect-to-week-2","title":"Connect to Week 2","text":"<ul> <li>Similar training patterns</li> <li>Same debugging approaches</li> <li>Different architecture, same principles</li> </ul>"},{"location":"Week3_Overview/#assessment-milestones","title":"Assessment Milestones","text":"<p>By Day 11 End: - \u2610 Understand convolutions conceptually - \u2610 Implement 2D convolution from scratch - \u2610 Build basic CNN in PyTorch - \u2610 Visualize learned filters</p> <p>By Day 13 End: - \u2610 Implement ResNet architecture - \u2610 Understand skip connections deeply - \u2610 Train deep networks successfully - \u2610 Compare architectures empirically</p> <p>By Day 15 End: - \u2610 Complete CIFAR-10 project - \u2610 Achieve &gt;85% test accuracy - \u2610 Professional documentation - \u2610 Understand transfer learning - \u2610 Portfolio-ready results</p>"},{"location":"Week3_Overview/#motivation","title":"Motivation","text":""},{"location":"Week3_Overview/#why-cnns-matter","title":"Why CNNs Matter","text":"<p>Real-World Applications: - Computer Vision: Object detection, facial recognition, scene understanding - Medical AI: Disease detection from X-rays, MRIs, CT scans - Autonomous Vehicles: Lane detection, obstacle recognition - Content Moderation: Inappropriate content detection - Agriculture: Crop disease identification - Security: Surveillance systems - Retail: Visual search, product recommendations</p> <p>Career Relevance: - Most in-demand deep learning skill - Foundation for advanced topics - Essential for CV/AI roles - Transferable to other domains</p>"},{"location":"Week3_Overview/#by-friday-youll-be-able-to","title":"By Friday, You'll Be Able To:","text":"<ul> <li>Build image classifiers for any dataset</li> <li>Use pretrained models (ResNet, VGG, etc.)</li> <li>Understand research papers on CNNs</li> <li>Design custom architectures</li> <li>Debug vision systems</li> <li>Have impressive portfolio project</li> </ul> <p>This is where theory meets powerful real-world applications! \ud83d\ude80</p>"},{"location":"Week3_Overview/#connection-to-future-weeks","title":"Connection to Future Weeks","text":"<p>Week 3 \u2192 Week 4: - CNNs (spatial) \u2192 RNNs (temporal/sequential) - Convolutions \u2192 Attention mechanisms - Image features \u2192 Text/sequence features - CNNs as components in larger systems</p> <p>Week 3 \u2192 Week 5: - Architecture design \u2192 Deployment considerations - Model complexity \u2192 Production trade-offs - Experimentation \u2192 Best practices</p>"},{"location":"Week3_Overview/#week-3-checklist","title":"Week 3 Checklist","text":"<p>Before Starting: - \u2610 Week 2 complete and understood - \u2610 Comfortable with PyTorch - \u2610 Understand neural network training - \u2610 Colab account ready (with GPU access) - \u2610 Ready to work with images</p> <p>During the Week: - \u2610 Watch all assigned videos - \u2610 Complete all coding exercises - \u2610 Implement architectures from scratch - \u2610 Attend check-ins (M/W/F) - \u2610 Complete daily reflections - \u2610 Visualize everything</p> <p>End of Week: - \u2610 CIFAR-10 project complete - \u2610 Understand CNNs deeply - \u2610 Can use transfer learning - \u2610 Portfolio project documented - \u2610 Ready for Week 4</p>"},{"location":"Week3_Overview/#study-recommendations","title":"Study Recommendations","text":""},{"location":"Week3_Overview/#active-learning","title":"Active Learning","text":"<ul> <li>Code along with videos</li> <li>Modify examples - see what breaks</li> <li>Compare approaches - which works better?</li> <li>Visualize constantly - what is the network learning?</li> </ul>"},{"location":"Week3_Overview/#peer-learning","title":"Peer Learning","text":"<ul> <li>Share visualizations on Teams</li> <li>Compare results on CIFAR-10</li> <li>Discuss architectures - why did you choose that?</li> <li>Debug together - two heads better than one</li> </ul>"},{"location":"Week3_Overview/#deep-understanding","title":"Deep Understanding","text":"<ul> <li>Why does this work? - Always ask</li> <li>Draw architectures - Sketch networks by hand</li> <li>Calculate by hand - Output sizes, parameter counts</li> <li>Connect concepts - How does this relate to Week 2?</li> </ul>"},{"location":"Week3_Overview/#final-thoughts","title":"Final Thoughts","text":"<p>Week 3 is where neural networks become truly powerful for vision tasks. The concepts you learn this week (convolution, hierarchy, skip connections, transfer learning) are fundamental to modern AI.</p> <p>CNNs represent one of deep learning's greatest success stories - taking computer vision from hand-crafted features to learned representations that surpass human performance on many tasks.</p> <p>By completing this week, you'll not just know how to use CNNs, but why they work, when to use them, and how to design effective architectures.</p> <p>The concepts are challenging but the payoff is enormous. Let's build some computer vision systems!</p> <p>Ready to start? Begin with Day 11: CNN Theory &amp; Convolutions</p> <p>Need to review? Go back to Week 2 Overview</p>"},{"location":"Week4_Day16/","title":"Week 4, Day 16: RNNs &amp; LSTMs - Sequential Data Processing","text":""},{"location":"Week4_Day16/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand how RNNs process sequential data</li> <li>Learn about vanishing gradients in RNNs</li> <li>Master LSTM architecture and gates</li> <li>Implement RNN and LSTM from scratch</li> <li>Train models on sequential tasks</li> <li>Visualize hidden state evolution</li> </ul>"},{"location":"Week4_Day16/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week4_Day16/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week4_Day16/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Recurrent Neural Networks by StatQuest (20 min) Clear explanation of RNN fundamentals</p> <p>\u2610 Watch: LSTM Networks by StatQuest (15 min) Understanding LSTM gates</p> <p>\u2610 Watch: The Unreasonable Effectiveness of RNNs by Andrej Karpathy (20 min) Motivation and applications</p> <p>\u2610 Watch: Illustrated Guide to LSTM's and GRU's (15 min) Visual walkthrough</p> <p>\u2610 Watch: Sequence Models by Andrew Ng (20 min) Comprehensive overview</p>"},{"location":"Week4_Day16/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 9.1-9.3 - RNNs</p> <p>\u2610 Read: D2L Chapter 10.1-10.2 - LSTMs</p> <p>\u2610 Read: Understanding LSTM Networks by Chris Olah</p>"},{"location":"Week4_Day16/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week4_Day16/#exercise-1-understanding-rnn-forward-pass-45-min","title":"Exercise 1: Understanding RNN Forward Pass (45 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: RNN FORWARD PASS\")\nprint(\"=\"*70)\n\n# Simple RNN from scratch\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Simple RNN implementation\n\n        At each time step:\n        h_t = tanh(W_ih @ x_t + W_hh @ h_{t-1} + b_h)\n        y_t = W_ho @ h_t + b_o\n        \"\"\"\n        # Initialize weights\n        self.W_ih = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n\n        self.W_ho = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_o = np.zeros((output_size, 1))\n\n        self.hidden_size = hidden_size\n\n    def forward(self, X, h_prev=None):\n        \"\"\"\n        Forward pass through sequence\n\n        Args:\n            X: input sequence (seq_len, input_size)\n            h_prev: previous hidden state (hidden_size, 1)\n\n        Returns:\n            outputs: predictions at each time step\n            hidden_states: hidden states at each time step\n        \"\"\"\n        seq_len = X.shape[0]\n\n        # Initialize hidden state\n        if h_prev is None:\n            h = np.zeros((self.hidden_size, 1))\n        else:\n            h = h_prev\n\n        outputs = []\n        hidden_states = [h.copy()]\n\n        # Process sequence\n        for t in range(seq_len):\n            x_t = X[t].reshape(-1, 1)\n\n            # Update hidden state\n            h = np.tanh(self.W_ih @ x_t + self.W_hh @ h + self.b_h)\n\n            # Compute output\n            y_t = self.W_ho @ h + self.b_o\n\n            outputs.append(y_t)\n            hidden_states.append(h.copy())\n\n        return np.array(outputs), np.array(hidden_states)\n\n# Test RNN\nprint(\"\\nTesting simple RNN:\")\nrnn = SimpleRNN(input_size=1, hidden_size=3, output_size=1)\n\n# Simple sequence: [0.1, 0.2, 0.3, 0.4, 0.5]\nX = np.array([[0.1], [0.2], [0.3], [0.4], [0.5]])\nprint(f\"Input sequence: {X.flatten()}\")\n\noutputs, hidden_states = rnn.forward(X)\n\nprint(f\"\\nOutputs shape: {outputs.shape}\")\nprint(f\"Hidden states shape: {hidden_states.shape}\")\n\nprint(f\"\\nOutputs at each time step:\")\nfor t, out in enumerate(outputs):\n    print(f\"  t={t}: {out[0, 0]:.4f}\")\n\n# Visualize hidden state evolution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Hidden state dimensions over time\nfor i in range(rnn.hidden_size):\n    axes[0].plot([h[i, 0] for h in hidden_states], \n                 label=f'h_{i}', marker='o')\naxes[0].set_xlabel('Time Step')\naxes[0].set_ylabel('Hidden State Value')\naxes[0].set_title('Hidden State Evolution')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Output over time\naxes[1].plot(X.flatten(), label='Input', marker='o')\naxes[1].plot([out[0, 0] for out in outputs], label='Output', marker='s')\naxes[1].set_xlabel('Time Step')\naxes[1].set_ylabel('Value')\naxes[1].set_title('Input vs Output Sequence')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Key Insight: RNN maintains hidden state that evolves over time!\")\nprint(\"\\n Exercise 1 complete\")\n</code></pre>"},{"location":"Week4_Day16/#exercise-2-character-level-prediction-60-min","title":"Exercise 2: Character-Level Prediction (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: CHARACTER-LEVEL PREDICTION\")\nprint(\"=\"*70)\n\n# Create simple character sequence\ntext = \"hello world\"\nchars = sorted(list(set(text)))\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for i, ch in enumerate(chars)}\n\nprint(f\"Text: '{text}'\")\nprint(f\"Unique characters: {chars}\")\nprint(f\"Vocabulary size: {len(chars)}\")\n\n# Convert text to indices\nX_text = [char_to_ix[ch] for ch in text[:-1]]\nY_text = [char_to_ix[ch] for ch in text[1:]]\n\nprint(f\"\\nInput sequence (chars): {[text[i] for i in range(len(text)-1)]}\")\nprint(f\"Target sequence (next char): {[text[i] for i in range(1, len(text))]}\")\n\n# Build RNN in PyTorch\nclass CharRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_size):\n        super(CharRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n\n        # RNN layer\n        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n\n        # Output layer\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        # Embed input\n        embedded = self.embedding(x)\n\n        # RNN forward\n        output, hidden = self.rnn(embedded, hidden)\n\n        # Project to vocabulary\n        output = self.fc(output)\n\n        return output, hidden\n\n    def init_hidden(self, batch_size):\n        return torch.zeros(1, batch_size, self.hidden_size)\n\n# Create model\nvocab_size = len(chars)\nhidden_size = 32\nmodel = CharRNN(vocab_size, hidden_size)\n\nprint(f\"\\nCharRNN Model:\")\nprint(model)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Training setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Convert to tensors\nX_tensor = torch.tensor(X_text).unsqueeze(0)  # (1, seq_len)\nY_tensor = torch.tensor(Y_text).unsqueeze(0)\n\nprint(f\"\\nTraining on sequence: '{text}'\")\nprint(\"Learning to predict next character...\")\n\n# Train\nlosses = []\nfor epoch in range(500):\n    # Forward pass\n    hidden = model.init_hidden(1)\n    output, hidden = model(X_tensor, hidden)\n\n    # Reshape for loss\n    loss = criterion(output.view(-1, vocab_size), Y_tensor.view(-1))\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item())\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/500: Loss = {loss.item():.4f}\")\n\n# Plot training\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Character RNN Training')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Test predictions\nprint(\"\\nTesting predictions:\")\nmodel.eval()\nwith torch.no_grad():\n    hidden = model.init_hidden(1)\n    output, _ = model(X_tensor, hidden)\n    predictions = torch.argmax(output, dim=2).squeeze().numpy()\n\n    print(\"\\nInput \u2192 Predicted:\")\n    for i in range(len(X_text)):\n        input_char = ix_to_char[X_text[i]]\n        true_char = ix_to_char[Y_text[i]]\n        pred_char = ix_to_char[predictions[i]]\n        correct = \"\" if pred_char == true_char else \"\"\n        print(f\"  '{input_char}' \u2192 '{pred_char}' (true: '{true_char}') {correct}\")\n\nprint(\"\\n Exercise 2 complete\")\n</code></pre>"},{"location":"Week4_Day16/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week4_Day16/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week4_Day16/#exercise-3-lstm-implementation-70-min","title":"Exercise 3: LSTM Implementation (70 min)","text":"<p>LSTM Gates: 1. Forget gate: $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$    - Decides what to remove from cell state</p> <ol> <li>Input gate: $$i_t = \u03c3(W_i \\cdot [h_{t-1}, x_t] + b_i)$$</li> <li> <p>Decides what new information to add</p> </li> <li> <p>Cell gate: $$C\u0303t = tanh(W_C \\cdot [h, x_t] + b_C)$$</p> </li> <li> <p>Creates new candidate cell state</p> </li> <li> <p>Output gate: $$o_t = \u03c3(W_o \\cdot [h_{t-1}, x_t] + b_o)$$</p> </li> <li>Decides what to output</li> </ol> <p>Cell state update: $$C_t = f_t \\odot C_{t-1} + i_t \\odot C\u0303_t$$</p> <p>Hidden state: $$h_t = o_t \\odot tanh(C_t)$$</p> <p>Key Insights: - Forget gate controls what to remove from memory - Input gate controls what new info to add - Cell state is long-term memory - Hidden state is output at each step</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: LSTM IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass SimpleLSTM:\n    def __init__(self, input_size, hidden_size):\n        \"\"\"Simple LSTM from scratch\"\"\"\n        self.hidden_size = hidden_size\n\n        # Forget gate\n        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_f = np.zeros((hidden_size, 1))\n\n        # Input gate\n        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_i = np.zeros((hidden_size, 1))\n\n        # Cell gate\n        self.W_C = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_C = np.zeros((hidden_size, 1))\n\n        # Output gate\n        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_o = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def forward_step(self, x_t, h_prev, C_prev):\n        \"\"\"Single LSTM step\"\"\"\n        # Concatenate input and hidden state\n        combined = np.vstack([h_prev, x_t])\n\n        # Forget gate\n        f_t = self.sigmoid(self.W_f @ combined + self.b_f)\n\n        # Input gate\n        i_t = self.sigmoid(self.W_i @ combined + self.b_i)\n\n        # Cell gate\n        C_tilde = np.tanh(self.W_C @ combined + self.b_C)\n\n        # Update cell state\n        C_t = f_t * C_prev + i_t * C_tilde\n\n        # Output gate\n        o_t = self.sigmoid(self.W_o @ combined + self.b_o)\n\n        # Update hidden state\n        h_t = o_t * np.tanh(C_t)\n\n        # Return gates for visualization\n        gates = {\n            'forget': f_t,\n            'input': i_t,\n            'cell': C_tilde,\n            'output': o_t\n        }\n\n        return h_t, C_t, gates\n\n    def forward(self, X):\n        \"\"\"Forward pass through sequence\"\"\"\n        seq_len = X.shape[0]\n\n        h = np.zeros((self.hidden_size, 1))\n        C = np.zeros((self.hidden_size, 1))\n\n        hidden_states = []\n        cell_states = []\n        all_gates = []\n\n        for t in range(seq_len):\n            x_t = X[t].reshape(-1, 1)\n            h, C, gates = self.forward_step(x_t, h, C)\n\n            hidden_states.append(h.copy())\n            cell_states.append(C.copy())\n            all_gates.append(gates)\n\n        return hidden_states, cell_states, all_gates\n\n# Test LSTM\nlstm = SimpleLSTM(input_size=1, hidden_size=4)\n\n# Test sequence\nX = np.array([[0.1], [0.5], [0.9], [0.3], [0.7]])\nprint(f\"\\nInput sequence: {X.flatten()}\")\n\nhidden_states, cell_states, all_gates = lstm.forward(X)\n\n# Visualize gates\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\ngate_names = ['forget', 'input', 'cell', 'output']\ngate_titles = ['Forget Gate', 'Input Gate', 'Cell Gate', 'Output Gate']\n\nfor idx, (gate_name, title) in enumerate(zip(gate_names, gate_titles)):\n    ax = axes[idx // 2, idx % 2]\n\n    gate_values = [gates[gate_name] for gates in all_gates]\n    gate_array = np.hstack(gate_values).T\n\n    im = ax.imshow(gate_array, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n    ax.set_xlabel('Hidden Unit')\n    ax.set_ylabel('Time Step')\n    ax.set_title(title)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle('LSTM Gate Activations Over Time', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Compare cell state vs hidden state\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Cell state\ncell_array = np.hstack(cell_states).T\nim = axes[0].imshow(cell_array, cmap='viridis', aspect='auto')\naxes[0].set_xlabel('Hidden Unit')\naxes[0].set_ylabel('Time Step')\naxes[0].set_title('Cell State Evolution')\nplt.colorbar(im, ax=axes[0])\n\n# Hidden state\nhidden_array = np.hstack(hidden_states).T\nim = axes[1].imshow(hidden_array, cmap='viridis', aspect='auto')\naxes[1].set_xlabel('Hidden Unit')\naxes[1].set_ylabel('Time Step')\naxes[1].set_title('Hidden State Evolution')\nplt.colorbar(im, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Exercise 3 complete\")\n</code></pre>"},{"location":"Week4_Day16/#exercise-4-sequence-prediction-with-pytorch-lstm-60-min","title":"Exercise 4: Sequence Prediction with PyTorch LSTM (60 min)","text":"<p>NOTE: This exercise is mostly for visualisation - feel free to copy this code entirely</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: SEQUENCE PREDICTION WITH PYTORCH LSTM\")\nprint(\"=\"*70)\n\n# Generate sine wave data\ndef generate_sine_data(seq_len=100, n_sequences=1000):\n    \"\"\"Generate sine wave sequences for prediction\"\"\"\n    X = []\n    Y = []\n\n    for _ in range(n_sequences):\n        start = np.random.rand() * 2 * np.pi\n        x = np.sin(np.linspace(start, start + 4*np.pi, seq_len + 1))\n        X.append(x[:-1])\n        Y.append(x[1:])\n\n    return np.array(X), np.array(Y)\n\nX_train, Y_train = generate_sine_data(seq_len=50, n_sequences=1000)\nX_test, Y_test = generate_sine_data(seq_len=50, n_sequences=100)\n\nprint(f\"Training data: {X_train.shape}\")\nprint(f\"Test data: {X_test.shape}\")\n\n# Visualize sample\nplt.figure(figsize=(12, 5))\nfor i in range(3):\n    plt.plot(X_train[i], label=f'Sequence {i}', alpha=0.7)\nplt.xlabel('Time Step')\nplt.ylabel('Value')\nplt.title('Sample Training Sequences (Sine Waves)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Build LSTM model\nclass LSTMPredictor(nn.Module):\n    def __init__(self, input_size=1, hidden_size=32, num_layers=2):\n        super(LSTMPredictor, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                            batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        # LSTM forward\n        out, _ = self.lstm(x)\n\n        # Project to output\n        out = self.fc(out)\n\n        return out\n\nmodel = LSTMPredictor(input_size=1, hidden_size=32, num_layers=2)\nprint(f\"\\nLSTM Predictor:\")\nprint(model)\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Prepare data\nX_train_tensor = torch.FloatTensor(X_train).unsqueeze(-1)  # (N, seq, 1)\nY_train_tensor = torch.FloatTensor(Y_train).unsqueeze(-1)\nX_test_tensor = torch.FloatTensor(X_test).unsqueeze(-1)\nY_test_tensor = torch.FloatTensor(Y_test).unsqueeze(-1)\n\n# Training\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining LSTM on sine wave prediction...\")\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n\n    # Forward\n    predictions = model(X_train_tensor)\n    loss = criterion(predictions, Y_train_tensor)\n\n    # Backward\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    train_losses.append(loss.item())\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.6f}\")\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    test_predictions = model(X_test_tensor)\n    test_loss = criterion(test_predictions, Y_test_tensor)\n\nprint(f\"\\nTest Loss: {test_loss.item():.6f}\")\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Training loss\naxes[0, 0].plot(train_losses)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Sample predictions\nfor i in range(3):\n    idx = i\n\n    true = Y_test[idx]\n    pred = test_predictions[idx].squeeze().numpy()\n\n    axes[0, 1].plot(true, label=f'True {i}', linestyle='--', alpha=0.7)\n    axes[0, 1].plot(pred, label=f'Pred {i}', alpha=0.7)\n\naxes[0, 1].set_xlabel('Time Step')\naxes[0, 1].set_ylabel('Value')\naxes[0, 1].set_title('Sample Predictions')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Single sequence detail\nidx = 0\naxes[1, 0].plot(X_test[idx], label='Input', marker='o', markersize=3)\naxes[1, 0].plot(Y_test[idx], label='True Next', marker='s', markersize=3)\naxes[1, 0].plot(test_predictions[idx].squeeze().numpy(), \n                label='Predicted Next', marker='^', markersize=3)\naxes[1, 0].set_xlabel('Time Step')\naxes[1, 0].set_ylabel('Value')\naxes[1, 0].set_title('Detailed View: Single Sequence')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Error distribution\nerrors = (Y_test_tensor - test_predictions).squeeze().numpy().flatten()\naxes[1, 1].hist(errors, bins=50, edgecolor='black')\naxes[1, 1].set_xlabel('Prediction Error')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title(f'Error Distribution (Mean: {errors.mean():.4f})')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Exercise 4 complete\")\n</code></pre>"},{"location":"Week4_Day16/#mini-challenge-many-to-one-sentiment-50-min","title":"Mini-Challenge: Many-to-One Sentiment (50 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: SIMPLE SENTIMENT CLASSIFICATION\")\nprint(\"=\"*70)\n\n# Create simple sentiment dataset\npositive_words = [\n    \"great\", \"excellent\", \"amazing\", \"wonderful\", \"fantastic\",\n    \"love\", \"best\", \"perfect\", \"awesome\", \"brilliant\"\n]\n\nnegative_words = [\n    \"terrible\", \"awful\", \"horrible\", \"worst\", \"hate\",\n    \"bad\", \"poor\", \"disappointing\", \"pathetic\", \"useless\"\n]\n\n# Generate simple sentences\ndef generate_sentence(words, n_words=5):\n    return ' '.join(np.random.choice(words, size=n_words))\n\n# Create dataset\nn_samples = 500\nsentences = []\nlabels = []\n\nfor _ in range(n_samples // 2):\n    sentences.append(generate_sentence(positive_words))\n    labels.append(1)\n    sentences.append(generate_sentence(negative_words))\n    labels.append(0)\n\nprint(f\"Dataset: {len(sentences)} sentences\")\nprint(f\"\\nExamples:\")\nfor i in range(3):\n    sent = sentences[i]\n    label = \"Positive\" if labels[i] == 1 else \"Negative\"\n    print(f\"  '{sent}' \u2192 {label}\")\n\n# Build vocabulary\nall_words = set(' '.join(sentences).split())\nword_to_ix = {word: i for i, word in enumerate(sorted(all_words))}\nvocab_size = len(word_to_ix)\n\nprint(f\"\\nVocabulary size: {vocab_size}\")\n\n# Convert sentences to indices\ndef sentence_to_indices(sentence, word_to_ix, max_len=10):\n    indices = [word_to_ix[w] for w in sentence.split()[:max_len]]\n    # Pad\n    while len(indices) &lt; max_len:\n        indices.append(0)\n    return indices\n\nX = torch.tensor([sentence_to_indices(s, word_to_ix) for s in sentences])\nY = torch.tensor(labels).float()\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"Y shape: {Y.shape}\")\n\n# Build classifier\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=16, hidden_dim=32):\n        super(SentimentLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)\n\n        # LSTM (use final hidden state)\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Classify\n        out = self.fc(hidden.squeeze(0))\n        out = self.sigmoid(out)\n\n        return out\n\nmodel = SentimentLSTM(vocab_size, embedding_dim=16, hidden_dim=32)\nprint(f\"\\nSentiment Classifier:\")\nprint(model)\n\n# Train\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining...\")\nepochs = 100\n\nfor epoch in range(epochs):\n    model.train()\n\n    predictions = model(X).squeeze()\n    loss = criterion(predictions, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 20 == 0:\n        accuracy = ((predictions &gt; 0.5) == Y).float().mean()\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {accuracy:.4f}\")\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X).squeeze()\n    predicted_labels = (predictions &gt; 0.5).long()\n    accuracy = (predicted_labels == Y).float().mean()\n\nprint(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n\n# Test new sentences\ntest_sentences = [\n    \"amazing wonderful excellent\",\n    \"terrible awful horrible\",\n    \"great best love\",\n    \"worst hate bad\"\n]\n\nprint(\"\\nTesting new sentences:\")\nmodel.eval()\nwith torch.no_grad():\n    for sent in test_sentences:\n        X_test = torch.tensor([sentence_to_indices(sent, word_to_ix)])\n        pred = model(X_test).item()\n        label = \"Positive\" if pred &gt; 0.5 else \"Negative\"\n        print(f\"  '{sent}' \u2192 {label} (confidence: {pred:.4f})\")\n\nprint(\"\\n Mini-challenge complete!\")\n</code></pre>"},{"location":"Week4_Day16/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review RNN and LSTM architectures \u2610 Understand vanishing gradients problem \u2610 Write daily reflection  </p>"},{"location":"Week4_Day16/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do RNNs differ from feedforward networks?</li> <li>Why do LSTMs solve the vanishing gradient problem?</li> <li>What role does each LSTM gate play?</li> <li>What challenges did you face with sequential modeling?</li> <li>How might you apply RNNs to your domain?</li> </ul> <p>Next: Day 17 - Attention Mechanisms &amp; Transformers</p>"},{"location":"Week4_Day17/","title":"Week 4, Day 17: Attention Mechanisms &amp; Transformers","text":""},{"location":"Week4_Day17/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand attention mechanism fundamentals</li> <li>Learn self-attention and multi-head attention</li> <li>Grasp Transformer architecture conceptually</li> <li>Implement attention from scratch</li> <li>Understand position encodings</li> <li>Connect to modern LLMs (GPT, BERT)</li> </ul>"},{"location":"Week4_Day17/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week4_Day17/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week4_Day17/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Attention Mechanism by StatQuest (15 min) Clear explanation of attention basics</p> <p>\u2610 Watch: Attention in Neural Networks (20 min) Visual explanation</p> <p>\u2610 Watch: Illustrated Guide to Transformers (15 min) Step-by-step walkthrough</p> <p>\u2610 Watch: Attention is All You Need by Yannic Kilcher (30 min) Deep dive into Transformer paper</p> <p>\u2610 Watch: How GPT Models Work (10 min) Connection to modern LLMs</p>"},{"location":"Week4_Day17/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: The Illustrated Transformer by Jay Alammar ESSENTIAL - Best visual explanation</p> <p>\u2610 Read: D2L Chapter 11.1-11.3 - Attention Mechanisms</p> <p>\u2610 Read: The Annotated Transformer (skim) Line-by-line code walkthrough</p>"},{"location":"Week4_Day17/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week4_Day17/#exercise-1-simple-attention-mechanism-50-min","title":"Exercise 1: Simple Attention Mechanism (50 min)","text":"<p>Attention Intuition: - Traditional RNN: Fixed-size context vector for entire sequence - Attention: Dynamically focus on relevant parts of input - Query: What am I looking for? - Key: What do I offer? - Value: What do I actually contain?</p> <p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: SIMPLE ATTENTION MECHANISM\")\nprint(\"=\"*70)\n\n# Focus on this method\ndef simple_attention(query, keys, values):\n    \"\"\"\n    Simple attention mechanism\n\n    Args:\n        query: (d_k,) - what we're looking for\n        keys: (seq_len, d_k) - what each position offers\n        values: (seq_len, d_v) - actual content at each position\n\n    Returns:\n        output: (d_v,) - weighted sum of values\n        weights: (seq_len,) - attention weights\n    \"\"\"\n    # Compute attention scores\n    scores = np.dot(keys, query)  # (seq_len,)\n\n    # Softmax to get weights\n    weights = np.exp(scores) / np.sum(np.exp(scores))\n\n    # Weighted sum of values\n    output = np.dot(weights, values)  # (d_v,)\n\n    return output, weights\n\n# Example: Looking up information\nprint(\"\\nExample: Simple lookup with attention\")\n\n# Sequence: \"The cat sat on mat\"\nwords = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\nseq_len = len(words)\n\n# Simplified: each word has a key and value (random for demo)\nnp.random.seed(42)\nd_k = 4  # key/query dimension\nd_v = 3  # value dimension\n\nkeys = np.random.randn(seq_len, d_k)\nvalues = np.random.randn(seq_len, d_v)\n\n# Query: looking for \"animal\" concept (similar to \"cat\" key)\nquery = keys[1] + np.random.randn(d_k) * 0.1  # Similar to \"cat\"\n\noutput, weights = simple_attention(query, keys, values)\n\nprint(f\"\\nQuery (looking for animal):\")\nprint(f\"Attention weights:\")\nfor word, weight in zip(words, weights):\n    bar = \"\u2588\" * int(weight * 50)\n    print(f\"  {word:5s}: {weight:.4f} {bar}\")\n\nprint(f\"\\nMost attended word: {words[np.argmax(weights)]}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Attention weights\naxes[0].bar(words, weights, color='steelblue', edgecolor='black')\naxes[0].set_ylabel('Attention Weight')\naxes[0].set_title('Attention Weights')\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Heatmap\naxes[1].imshow(weights.reshape(1, -1), cmap='YlOrRd', aspect='auto')\naxes[1].set_xticks(range(seq_len))\naxes[1].set_xticklabels(words)\naxes[1].set_yticks([0])\naxes[1].set_yticklabels(['Attention'])\naxes[1].set_title('Attention Heatmap')\n\nfor i, weight in enumerate(weights):\n    axes[1].text(i, 0, f'{weight:.2f}', ha='center', va='center', \n                fontsize=10, color='black' if weight &lt; 0.5 else 'white')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Attention allows dynamic focus on relevant information!\")\nprint(\"\\n Exercise 1 complete\")\n</code></pre>"},{"location":"Week4_Day17/#exercise-2-scaled-dot-product-attention-50-min","title":"Exercise 2: Scaled Dot-Product Attention (50 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: SCALED DOT-PRODUCT ATTENTION\")\nprint(\"=\"*70)\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention from Transformer paper\n\n    Attention(Q, K, V) = softmax(QK^T / \u221ad_k) V\n    \"\"\"\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Args:\n            query: (batch, seq_len, d_k)\n            key: (batch, seq_len, d_k)\n            value: (batch, seq_len, d_v)\n            mask: optional mask\n\n        Returns:\n            output: (batch, seq_len, d_v)\n            attention_weights: (batch, seq_len, seq_len)\n        \"\"\"\n        d_k = query.size(-1)\n\n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n\n        # Apply mask (for padding or future tokens)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        output = torch.matmul(attention_weights, value)\n\n        return output, attention_weights\n\n# Test scaled dot-product attention\nattention = ScaledDotProductAttention()\n\n# Create sample inputs\nbatch_size = 2\nseq_len = 5\nd_model = 8\n\nQ = torch.randn(batch_size, seq_len, d_model)\nK = torch.randn(batch_size, seq_len, d_model)\nV = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"\\nInput shapes:\")\nprint(f\"  Query: {Q.shape}\")\nprint(f\"  Key: {K.shape}\")\nprint(f\"  Value: {V.shape}\")\n\noutput, attn_weights = attention(Q, K, V)\n\nprint(f\"\\nOutput shapes:\")\nprint(f\"  Output: {output.shape}\")\nprint(f\"  Attention weights: {attn_weights.shape}\")\n\n# Visualize attention patterns\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nfor batch_idx in range(2):\n    ax = axes[batch_idx]\n\n    weights = attn_weights[batch_idx].detach().numpy()\n\n    im = ax.imshow(weights, cmap='YlOrRd', aspect='auto')\n    ax.set_xlabel('Key Position')\n    ax.set_ylabel('Query Position')\n    ax.set_title(f'Attention Weights (Batch {batch_idx})')\n\n    # Add values\n    for i in range(seq_len):\n        for j in range(seq_len):\n            text = ax.text(j, i, f'{weights[i, j]:.2f}',\n                          ha=\"center\", va=\"center\", color=\"black\" if weights[i, j] &lt; 0.5 else \"white\",\n                          fontsize=8)\n\n    plt.colorbar(im, ax=ax)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Each position can attend to all other positions!\")\nprint(\"\\n Exercise 2 complete\")\n</code></pre>"},{"location":"Week4_Day17/#exercise-3-multi-head-attention-60-min","title":"Exercise 3: Multi-Head Attention (60 min)","text":"<p>Multi-Head Attention: - Instead of single attention, use multiple \"heads\" - Each head learns different aspects/relationships - Heads computed in parallel - Outputs concatenated and projected</p> <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O$$ where</p> <p>$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: MULTI-HEAD ATTENTION\")\nprint(\"=\"*70)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.attention = ScaledDotProductAttention()\n\n    def split_heads(self, x):\n        \"\"\"Split into multiple heads\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        \"\"\"Combine heads back\"\"\"\n        batch_size, num_heads, seq_len, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Linear projections\n        Q = self.W_q(query)\n        K = self.W_k(key)\n        V = self.W_v(value)\n\n        # Split into heads\n        Q = self.split_heads(Q)  # (batch, heads, seq, d_k)\n        K = self.split_heads(K)\n        V = self.split_heads(V)\n\n        # Apply attention\n        output, attn_weights = self.attention(Q, K, V, mask)\n\n        # Combine heads\n        output = self.combine_heads(output)\n\n        # Final projection\n        output = self.W_o(output)\n\n        return output, attn_weights\n\n# Test multi-head attention\nd_model = 64\nnum_heads = 8\nseq_len = 10\nbatch_size = 2\n\nmha = MultiHeadAttention(d_model, num_heads)\n\nprint(f\"\\nMulti-Head Attention:\")\nprint(f\"  d_model: {d_model}\")\nprint(f\"  num_heads: {num_heads}\")\nprint(f\"  d_k (per head): {d_model // num_heads}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n\n# Test input\nX = torch.randn(batch_size, seq_len, d_model)\nprint(f\"\\nInput: {X.shape}\")\n\noutput, attn_weights = mha(X, X, X)  # Self-attention\n\nprint(f\"Output: {output.shape}\")\nprint(f\"Attention weights: {attn_weights.shape}\")\n\n# Visualize different heads\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\n\nbatch_idx = 0\nweights = attn_weights[batch_idx].detach().numpy()\n\nfor head in range(num_heads):\n    ax = axes[head]\n\n    head_weights = weights[head]\n\n    im = ax.imshow(head_weights, cmap='YlOrRd', aspect='auto')\n    ax.set_title(f'Head {head}')\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Query')\n\nplt.suptitle('Multi-Head Attention Patterns', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Different heads learn different patterns!\")\nprint(\"\\n Exercise 3 complete\")\n</code></pre>"},{"location":"Week4_Day17/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week4_Day17/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week4_Day17/#exercise-4-position-encodings-40-min","title":"Exercise 4: Position Encodings (40 min)","text":"<p>Position Encodings: - Transformers have no recurrence \u2192 no inherent position info! - Add positional information to embeddings - Use sine/cosine functions of different frequencies</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: POSITION ENCODINGS\")\nprint(\"=\"*70)\n\ndef get_positional_encoding(seq_len, d_model):\n    \"\"\"Generate positional encodings\"\"\"\n    pe = np.zeros((seq_len, d_model))\n\n    position = np.arange(0, seq_len).reshape(-1, 1)\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n\n    return pe\n\n# Generate position encodings\nseq_len = 50\nd_model = 128\n\npe = get_positional_encoding(seq_len, d_model)\n\nprint(f\"Position encodings shape: {pe.shape}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Full encoding heatmap\nim = axes[0].imshow(pe, cmap='RdBu', aspect='auto')\naxes[0].set_xlabel('Embedding Dimension')\naxes[0].set_ylabel('Position')\naxes[0].set_title('Positional Encodings')\nplt.colorbar(im, ax=axes[0])\n\n# First few dimensions over positions\nfor dim in range(8):\n    axes[1].plot(pe[:, dim], label=f'Dim {dim}', alpha=0.7)\naxes[1].set_xlabel('Position')\naxes[1].set_ylabel('Encoding Value')\naxes[1].set_title('First 8 Dimensions')\naxes[1].legend(ncol=2)\naxes[1].grid(True, alpha=0.3)\n\n# Specific positions across all dimensions\nfor pos in [0, 10, 25, 40]:\n    axes[2].plot(pe[pos], label=f'Pos {pos}', alpha=0.7)\naxes[2].set_xlabel('Dimension')\naxes[2].set_ylabel('Encoding Value')\naxes[2].set_title('Different Positions')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Position encodings are unique for each position!\")\nprint(\"   Sine/cosine allow the model to learn relative positions\")\n\nprint(\"\\n Exercise 4 complete\")\n</code></pre>"},{"location":"Week4_Day17/#exercise-5-simple-transformer-block-70-min","title":"Exercise 5: Simple Transformer Block (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: TRANSFORMER ENCODER BLOCK\")\nprint(\"=\"*70)\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"\n    Single Transformer Encoder Block\n\n    Components:\n    1. Multi-Head Self-Attention\n    2. Add &amp; Norm\n    3. Feed-Forward Network\n    4. Add &amp; Norm\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(TransformerEncoderBlock, self).__init__()\n\n        # Multi-head attention\n        self.attention = MultiHeadAttention(d_model, num_heads)\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # Layer norms\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention with residual\n        attn_output, attn_weights = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Feed-forward with residual\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n\n        return x, attn_weights\n\n# Create encoder block\nd_model = 64\nnum_heads = 8\nd_ff = 256\n\nencoder = TransformerEncoderBlock(d_model, num_heads, d_ff)\n\nprint(f\"Transformer Encoder Block:\")\nprint(f\"  d_model: {d_model}\")\nprint(f\"  num_heads: {num_heads}\")\nprint(f\"  d_ff: {d_ff}\")\nprint(f\"  Parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n\n# Test\nbatch_size = 2\nseq_len = 10\nX = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"\\nInput: {X.shape}\")\n\noutput, attn_weights = encoder(X)\n\nprint(f\"Output: {output.shape}\")\nprint(f\"Attention weights: {attn_weights.shape}\")\n\n# Visualize attention in encoder\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nweights = attn_weights[0, 0].detach().numpy()  # First batch, first head\n\nim = ax.imshow(weights, cmap='YlOrRd', aspect='auto')\nax.set_xlabel('Key Position')\nax.set_ylabel('Query Position')\nax.set_title('Self-Attention in Transformer Encoder')\n\nfor i in range(seq_len):\n    for j in range(seq_len):\n        text = ax.text(j, i, f'{weights[i, j]:.2f}',\n                      ha=\"center\", va=\"center\",\n                      color=\"black\" if weights[i, j] &lt; 0.5 else \"white\",\n                      fontsize=8)\n\nplt.colorbar(im, ax=ax)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Transformer encoder allows each position to attend to all positions!\")\nprint(\"\\n Exercise 5 complete\")\n</code></pre>"},{"location":"Week4_Day17/#mini-challenge-sequence-classification-with-transformer-70-min","title":"Mini-Challenge: Sequence Classification with Transformer (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: SEQUENCE CLASSIFICATION\")\nprint(\"=\"*70)\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"Simple Transformer for sequence classification\"\"\"\n    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_classes, max_seq_len=100):\n        super(TransformerClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n        # Positional encoding\n        self.register_buffer('pos_encoding', \n                           torch.FloatTensor(get_positional_encoding(max_seq_len, d_model)))\n\n        self.encoder = TransformerEncoderBlock(d_model, num_heads, d_ff)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, num_classes)\n        )\n\n    def forward(self, x):\n        # Embed\n        seq_len = x.size(1)\n        embedded = self.embedding(x)\n\n        # Add positional encoding\n        embedded = embedded + self.pos_encoding[:seq_len, :]\n\n        # Encode\n        encoded, attn_weights = self.encoder(embedded)\n\n        # Use mean pooling for classification\n        pooled = encoded.mean(dim=1)\n\n        # Classify\n        output = self.classifier(pooled)\n\n        return output, attn_weights\n\n# Use previous simple sentiment data\nfrom Week4_Day16 import positive_words, negative_words  # Assume available\n\ndef generate_sentence(words, n_words=5):\n    return ' '.join(np.random.choice(words, size=n_words))\n\n# Generate data\nn_samples = 1000\nsentences = []\nlabels = []\n\nfor _ in range(n_samples // 2):\n    sentences.append(generate_sentence(positive_words))\n    labels.append(1)\n    sentences.append(generate_sentence(negative_words))\n    labels.append(0)\n\n# Build vocabulary\nall_words = set(' '.join(sentences).split())\nword_to_ix = {word: i+1 for i, word in enumerate(sorted(all_words))}  # 0 for padding\nvocab_size = len(word_to_ix) + 1\n\nprint(f\"Dataset: {len(sentences)} sentences\")\nprint(f\"Vocabulary: {vocab_size} words\")\n\n# Convert to tensors\ndef sentence_to_indices(sentence, word_to_ix, max_len=10):\n    indices = [word_to_ix.get(w, 0) for w in sentence.split()[:max_len]]\n    while len(indices) &lt; max_len:\n        indices.append(0)\n    return indices\n\nX = torch.tensor([sentence_to_indices(s, word_to_ix) for s in sentences])\nY = torch.tensor(labels).long()\n\n# Create model\nmodel = TransformerClassifier(\n    vocab_size=vocab_size,\n    d_model=32,\n    num_heads=4,\n    d_ff=128,\n    num_classes=2,\n    max_seq_len=10\n)\n\nprint(f\"\\nTransformer Classifier:\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining...\")\nepochs = 50\n\nfor epoch in range(epochs):\n    model.train()\n\n    outputs, _ = model(X)\n    loss = criterion(outputs, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        with torch.no_grad():\n            predictions = torch.argmax(outputs, dim=1)\n            accuracy = (predictions == Y).float().mean()\n            print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {accuracy:.4f}\")\n\n# Final evaluation\nmodel.eval()\nwith torch.no_grad():\n    outputs, attn_weights = model(X)\n    predictions = torch.argmax(outputs, dim=1)\n    accuracy = (predictions == Y).float().mean()\n\nprint(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n\n# Test sentences\ntest_sentences = [\n    \"amazing excellent wonderful great\",\n    \"terrible horrible awful bad\"\n]\n\nprint(\"\\nTesting:\")\nmodel.eval()\nwith torch.no_grad():\n    for sent in test_sentences:\n        X_test = torch.tensor([sentence_to_indices(sent, word_to_ix)])\n        output, attn = model(X_test)\n        pred = torch.argmax(output, dim=1).item()\n        label = \"Positive\" if pred == 1 else \"Negative\"\n        conf = torch.softmax(output, dim=1)[0, pred].item()\n        print(f\"  '{sent}' \u2192 {label} ({conf:.4f})\")\n\nprint(\"\\n Transformer classifier complete!\")\nprint(\" This is a simplified version of BERT's architecture!\")\n\nprint(\"\\n Mini-challenge complete\")\n</code></pre>"},{"location":"Week4_Day17/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review attention mechanism thoroughly \u2610 Understand Transformer architecture \u2610 Connect to modern LLMs \u2610 Write daily reflection  </p>"},{"location":"Week4_Day17/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does attention differ from RNN hidden states?</li> <li>Why are Transformers so powerful?</li> <li>What role do position encodings play?</li> <li>How does multi-head attention help?</li> <li>What connections do you see to GPT/BERT?</li> </ul> <p>Next: Day 18 - NLP Fundamentals</p>"},{"location":"Week4_Day18/","title":"Week 4, Day 18: NLP Fundamentals - Text Processing &amp; Embeddings","text":""},{"location":"Week4_Day18/#daily-goals","title":"Daily Goals","text":"<ul> <li>Master text preprocessing and tokenization</li> <li>Understand word embeddings (Word2Vec, GloVe)</li> <li>Learn embedding spaces and semantic relationships</li> <li>Process text data for deep learning models</li> <li>Build text classification with embeddings</li> <li>Use pretrained embeddings</li> </ul>"},{"location":"Week4_Day18/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week4_Day18/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week4_Day18/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Word Embeddings by StatQuest (15 min) Clear explanation of embeddings</p> <p>\u2610 Watch: Word2Vec Explained (20 min) Understanding Word2Vec</p> <p>\u2610 Watch: GloVe: Global Vectors for Word Representation (15 min) Another embedding approach</p> <p>\u2610 Watch: Illustrated Word2Vec (20 min) Visual walkthrough</p> <p>\u2610 Watch: NLP Preprocessing (20 min) Text cleaning and preparation</p>"},{"location":"Week4_Day18/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 15.1-15.3 - Word Embeddings</p> <p>\u2610 Read: GloVe Paper - introduction</p> <p>\u2610 Read: PyTorch Text Tutorial</p>"},{"location":"Week4_Day18/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week4_Day18/#exercise-1-text-preprocessing-pipeline-50-min","title":"Exercise 1: Text Preprocessing Pipeline (50 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: TEXT PREPROCESSING\")\nprint(\"=\"*70)\n\n# Sample text corpus\ncorpus = \"\"\"\nNatural language processing is a subfield of artificial intelligence.\nIt focuses on enabling computers to understand human language.\nText preprocessing is crucial for NLP tasks.\nTokenization splits text into words or subwords.\nWord embeddings represent words as dense vectors.\n\"\"\"\n\nprint(f\"Original corpus:\\n{corpus}\\n\")\n\n# Step 1: Lowercase and clean\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\n\ncleaned = clean_text(corpus)\nprint(f\"After cleaning:\\n{cleaned}\\n\")\n\n# Step 2: Tokenization\ndef tokenize(text):\n    return text.split()\n\ntokens = tokenize(cleaned)\nprint(f\"Tokens: {len(tokens)} words\")\nprint(f\"First 20 tokens: {tokens[:20]}\\n\")\n\n# Step 3: Build vocabulary\ndef build_vocab(tokens, min_freq=1):\n    counter = Counter(tokens)\n    vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}\n\n    for word, freq in counter.items():\n        if freq &gt;= min_freq:\n            vocab[word] = len(vocab)\n\n    return vocab\n\nvocab = build_vocab(tokens)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Sample vocabulary: {list(vocab.items())[:15]}\\n\")\n\n# Step 4: Convert text to indices\ndef text_to_indices(text, vocab):\n    tokens = tokenize(clean_text(text))\n    return [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\n\nsample_text = \"Natural language processing uses embeddings\"\nindices = text_to_indices(sample_text, vocab)\nprint(f\"Sample: '{sample_text}'\")\nprint(f\"Indices: {indices}\\n\")\n\n# Step 5: Padding sequences\ndef pad_sequences(sequences, max_len, pad_value=0):\n    padded = []\n    for seq in sequences:\n        if len(seq) &lt; max_len:\n            seq = seq + [pad_value] * (max_len - len(seq))\n        else:\n            seq = seq[:max_len]\n        padded.append(seq)\n    return padded\n\n# Example sequences\nsequences = [\n    [5, 10, 15],\n    [3, 7, 11, 14, 18],\n    [2, 4]\n]\n\npadded = pad_sequences(sequences, max_len=6)\nprint(f\"Original sequences: {sequences}\")\nprint(f\"Padded sequences: {padded}\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nim = ax.imshow(padded, cmap='viridis', aspect='auto')\nax.set_xlabel('Position')\nax.set_ylabel('Sequence')\nax.set_title('Padded Sequences')\nplt.colorbar(im, ax=ax, label='Token Index')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Exercise 1 complete\")\n</code></pre>"},{"location":"Week4_Day18/#exercise-2-word2vec-from-scratch-70-min","title":"Exercise 2: Word2Vec from Scratch (70 min)","text":"<p>Word2Vec: Learn word embeddings from context - OPTIONAL Two architectures: 1. Skip-gram: Predict context from word 2. CBOW: Predict word from context</p> <p>We'll implement simple Skip-gram</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: WORD2VEC IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass SkipGram(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SkipGram, self).__init__()\n\n        # Target word embedding\n        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # Context word embedding\n        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, target, context):\n        # Get embeddings\n        target_embed = self.target_embeddings(target)  # (batch, embed_dim)\n        context_embed = self.context_embeddings(context)  # (batch, embed_dim)\n\n        # Dot product (similarity)\n        scores = torch.sum(target_embed * context_embed, dim=1)\n\n        # Sigmoid for binary classification\n        return torch.sigmoid(scores)\n\n# Generate training data\ndef generate_skipgram_data(corpus, vocab, window_size=2):\n    tokens = tokenize(clean_text(corpus))\n    indices = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\n\n    pairs = []\n    for i, target in enumerate(indices):\n        # Context window\n        start = max(0, i - window_size)\n        end = min(len(indices), i + window_size + 1)\n\n        for j in range(start, end):\n            if i != j:\n                pairs.append((target, indices[j], 1))  # Positive pair\n\n        # Negative sampling\n        for _ in range(2):\n            neg_context = np.random.randint(2, len(vocab))  # Random word\n            pairs.append((target, neg_context, 0))  # Negative pair\n\n    return pairs\n\n# Generate data\ntraining_pairs = generate_skipgram_data(corpus, vocab, window_size=2)\nprint(f\"Training pairs: {len(training_pairs)}\")\nprint(f\"Sample pairs: {training_pairs[:5]}\\n\")\n\n# Create model\nvocab_size = len(vocab)\nembedding_dim = 10\n\nmodel = SkipGram(vocab_size, embedding_dim)\nprint(f\"Skip-gram model:\")\nprint(f\"  Vocabulary: {vocab_size}\")\nprint(f\"  Embedding dim: {embedding_dim}\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n\n# Prepare tensors\ntargets = torch.tensor([p[0] for p in training_pairs])\ncontexts = torch.tensor([p[1] for p in training_pairs])\nlabels = torch.tensor([p[2] for p in training_pairs], dtype=torch.float)\n\n# Train\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nprint(\"Training Word2Vec...\")\nepochs = 500\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward\n    predictions = model(targets, contexts)\n    loss = criterion(predictions, labels)\n\n    # Backward\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item())\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}\")\n\n# Plot training\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Word2Vec Training')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Extract embeddings\nembeddings = model.target_embeddings.weight.data.numpy()\nprint(f\"\\nLearned embeddings shape: {embeddings.shape}\")\n\n# Find similar words\ndef find_similar(word, vocab, embeddings, top_k=3):\n    if word not in vocab:\n        return []\n\n    word_idx = vocab[word]\n    word_embed = embeddings[word_idx]\n\n    # Compute cosine similarity\n    similarities = []\n    for idx, embed in enumerate(embeddings):\n        if idx != word_idx and idx &gt; 1:  # Skip PAD, UNK, and self\n            sim = np.dot(word_embed, embed) / (np.linalg.norm(word_embed) * np.linalg.norm(embed))\n            similarities.append((idx, sim))\n\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    idx_to_word = {v: k for k, v in vocab.items()}\n    return [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n\n# Test similarity\ntest_words = ['language', 'text', 'word']\nprint(\"\\nWord similarities:\")\nfor word in test_words:\n    if word in vocab:\n        similar = find_similar(word, vocab, embeddings, top_k=3)\n        print(f\"\\n'{word}':\")\n        for similar_word, sim in similar:\n            print(f\"  {similar_word}: {sim:.4f}\")\n\nprint(\"\\n Exercise 2 complete\")\n</code></pre>"},{"location":"Week4_Day18/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week4_Day18/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week4_Day18/#exercise-3-using-pretrained-embeddings-60-min-optional","title":"Exercise 3: Using Pretrained Embeddings (60 min) - OPTIONAL","text":"<p>Note: In practice, download GloVe from https://nlp.stanford.edu/projects/glove/ For this exercise, we'll simulate the concept</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: PRETRAINED EMBEDDINGS\")\nprint(\"=\"*70)\n\n# Simulate GloVe embeddings (in practice, download from Stanford)\n\n\n# Create text classifier with embeddings\nclass EmbeddingClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_classes, pretrained_embeddings=None):\n        super(EmbeddingClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # Load pretrained if provided\n        if pretrained_embeddings is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n            self.embedding.weight.requires_grad = False  # Freeze\n\n        self.lstm = nn.LSTM(embedding_dim, 64, batch_first=True)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)\n\n        # LSTM\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Classify\n        output = self.fc(hidden.squeeze(0))\n        return output\n\n# Create sample classification dataset\npositive_sents = [\n    \"this movie is great\",\n    \"excellent film loved it\",\n    \"amazing performance wonderful\",\n    \"fantastic story brilliant acting\",\n    \"loved every minute best\"\n]\n\nnegative_sents = [\n    \"terrible movie hated it\",\n    \"awful film worst ever\",\n    \"boring plot bad acting\",\n    \"disappointing waste of time\",\n    \"horrible terrible poor\"\n]\n\nsentences = positive_sents + negative_sents\nlabels_list = [1] * len(positive_sents) + [0] * len(negative_sents)\n\n# Build vocab from this dataset\nall_tokens = []\nfor sent in sentences:\n    all_tokens.extend(tokenize(clean_text(sent)))\n\ndataset_vocab = build_vocab(all_tokens)\nprint(f\"Dataset vocabulary: {len(dataset_vocab)} words\\n\")\n\n# Convert to indices and pad\nmax_len = 8\nX_indices = [text_to_indices(sent, dataset_vocab) for sent in sentences]\nX_padded = pad_sequences(X_indices, max_len)\n\nX_tensor = torch.tensor(X_padded)\nY_tensor = torch.tensor(labels_list)\n\nprint(f\"Data shapes:\")\nprint(f\"  X: {X_tensor.shape}\")\nprint(f\"  Y: {Y_tensor.shape}\\n\")\n\n# Train without pretrained embeddings\nmodel_scratch = EmbeddingClassifier(len(dataset_vocab), embedding_dim=16, num_classes=2)\nprint(\"Model from scratch:\")\nprint(f\"  Parameters: {sum(p.numel() for p in model_scratch.parameters()):,}\")\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_scratch.parameters(), lr=0.01)\n\nprint(\"\\nTraining from scratch...\")\nfor epoch in range(100):\n    outputs = model_scratch(X_tensor)\n    loss = criterion(outputs, Y_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 20 == 0:\n        preds = torch.argmax(outputs, dim=1)\n        acc = (preds == Y_tensor).float().mean()\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\n# Compare with using our trained embeddings\npretrained_emb = embeddings[:len(dataset_vocab)]  # Use embeddings from Exercise 2\nmodel_pretrained = EmbeddingClassifier(len(dataset_vocab), embedding_dim=10, num_classes=2,\n                                      pretrained_embeddings=pretrained_emb)\n\nprint(\"\\n\\nModel with pretrained embeddings:\")\nprint(f\"  Parameters: {sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad):,} (trainable)\")\n\noptimizer_pre = torch.optim.Adam(filter(lambda p: p.requires_grad, model_pretrained.parameters()), lr=0.01)\n\nprint(\"\\nTraining with pretrained...\")\nfor epoch in range(100):\n    outputs = model_pretrained(X_tensor)\n    loss = criterion(outputs, Y_tensor)\n\n    optimizer_pre.zero_grad()\n    loss.backward()\n    optimizer_pre.step()\n\n    if (epoch + 1) % 20 == 0:\n        preds = torch.argmax(outputs, dim=1)\n        acc = (preds == Y_tensor).float().mean()\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\nprint(\"\\n Pretrained embeddings provide better starting point!\")\nprint(\"   Especially valuable with limited training data\")\n\nprint(\"\\n Exercise 3 complete\")\n</code></pre>"},{"location":"Week4_Day18/#exercise-4-embedding-space-visualization-45-min","title":"Exercise 4: Embedding Space Visualization (45 min)","text":"<p>NOTE: This is a visualisation exercise, copy this code! Similar words cluster together in embedding space! This is how models understand semantic relationships</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: VISUALIZING EMBEDDING SPACE\")\nprint(\"=\"*70)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Use embeddings from Exercise 2\nprint(f\"Embedding space: {embeddings.shape}\\n\")\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings_2d_pca = pca.fit_transform(embeddings[2:])  # Skip PAD and UNK\n\n# t-SNE to 2D\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d_tsne = tsne.fit_transform(embeddings[2:])\n\n# Get word labels\nidx_to_word = {v: k for k, v in vocab.items()}\nwords = [idx_to_word[i] for i in range(2, len(vocab))]\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# PCA\naxes[0].scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], alpha=0.6)\nfor i, word in enumerate(words):\n    axes[0].annotate(word, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]),\n                    fontsize=9, alpha=0.7)\naxes[0].set_title('Word Embeddings (PCA)')\naxes[0].set_xlabel('PC 1')\naxes[0].set_ylabel('PC 2')\naxes[0].grid(True, alpha=0.3)\n\n# t-SNE\naxes[1].scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], alpha=0.6)\nfor i, word in enumerate(words):\n    axes[1].annotate(word, (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]),\n                    fontsize=9, alpha=0.7)\naxes[1].set_title('Word Embeddings (t-SNE)')\naxes[1].set_xlabel('Dimension 1')\naxes[1].set_ylabel('Dimension 2')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\nprint(\"\\n Exercise 4 complete\")\n</code></pre>"},{"location":"Week4_Day18/#mini-challenge-text-classification-pipeline-70-min","title":"Mini-Challenge: Text Classification Pipeline (70 min)","text":"<p>NOTE: Code all of this yourself, except for the code for visualisation.</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: COMPLETE TEXT CLASSIFICATION\")\nprint(\"=\"*70)\n\n# Build complete pipeline\nclass TextClassificationPipeline:\n    def __init__(self, embedding_dim=32, hidden_dim=64):\n        self.vocab = None\n        self.model = None\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.max_len = 50\n\n    def build_vocab(self, texts, min_freq=2):\n        \"\"\"Build vocabulary from texts\"\"\"\n        tokens = []\n        for text in texts:\n            tokens.extend(tokenize(clean_text(text)))\n\n        self.vocab = build_vocab(tokens, min_freq=min_freq)\n        return self.vocab\n\n    def preprocess(self, texts):\n        \"\"\"Convert texts to padded indices\"\"\"\n        sequences = [text_to_indices(text, self.vocab) for text in texts]\n        padded = pad_sequences(sequences, self.max_len)\n        return torch.tensor(padded)\n\n    def build_model(self, num_classes):\n        \"\"\"Create classification model\"\"\"\n        self.model = EmbeddingClassifier(\n            vocab_size=len(self.vocab),\n            embedding_dim=self.embedding_dim,\n            num_classes=num_classes\n        )\n        return self.model\n\n    def train(self, texts, labels, epochs=50, lr=0.001):\n        \"\"\"Train the model\"\"\"\n        X = self.preprocess(texts)\n        Y = torch.tensor(labels)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n\n        history = {'loss': [], 'acc': []}\n\n        for epoch in range(epochs):\n            self.model.train()\n\n            outputs = self.model(X)\n            loss = criterion(outputs, Y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            preds = torch.argmax(outputs, dim=1)\n            acc = (preds == Y).float().mean().item()\n\n            history['loss'].append(loss.item())\n            history['acc'].append(acc)\n\n            if (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\n        return history\n\n    def predict(self, texts):\n        \"\"\"Predict on new texts\"\"\"\n        X = self.preprocess(texts)\n\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(X)\n            predictions = torch.argmax(outputs, dim=1)\n\n        return predictions.numpy()\n\n# Create larger dataset\npositive = [\n    \"excellent product highly recommend\",\n    \"great quality very satisfied\",\n    \"amazing service love it\",\n    \"wonderful experience best purchase\",\n    \"fantastic item exceeded expectations\"\n] * 20\n\nnegative = [\n    \"terrible quality waste money\",\n    \"awful product very disappointed\",\n    \"horrible service never again\",\n    \"worst purchase big mistake\",\n    \"poor quality not recommended\"\n] * 20\n\ntexts = positive + negative\nlabels = [1] * len(positive) + [0] * len(negative)\n\n# Shuffle\nindices = np.random.permutation(len(texts))\ntexts = [texts[i] for i in indices]\nlabels = [labels[i] for i in indices]\n\nprint(f\"Dataset: {len(texts)} samples\\n\")\n\n# Create pipeline\npipeline = TextClassificationPipeline(embedding_dim=32, hidden_dim=64)\n\n# Build vocab\nvocab = pipeline.build_vocab(texts, min_freq=2)\nprint(f\"Vocabulary: {len(vocab)} words\")\n\n# Build model\nmodel = pipeline.build_model(num_classes=2)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n\n# Train\nprint(\"Training...\")\nhistory = pipeline.train(texts, labels, epochs=50, lr=0.001)\n\n# Visualize training\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(history['loss'])\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(history['acc'])\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Test predictions\ntest_texts = [\n    \"amazing quality love this product\",\n    \"terrible experience very unhappy\",\n    \"great purchase highly satisfied\",\n    \"awful quality waste of money\"\n]\n\npredictions = pipeline.predict(test_texts)\nprint(\"\\nTest predictions:\")\nfor text, pred in zip(test_texts, predictions):\n    label = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"  '{text}' \u2192 {label}\")\n\nprint(\"\\n Complete text classification pipeline!\")\nprint(\"\\n Mini-challenge complete\")\n</code></pre>"},{"location":"Week4_Day18/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review text preprocessing steps \u2610 Understand word embeddings \u2610 Connect embeddings to Transformers \u2610 Write daily reflection  </p>"},{"location":"Week4_Day18/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do word embeddings capture semantic meaning?</li> <li>Why are embeddings better than one-hot encoding?</li> <li>What is the difference between Word2Vec and GloVe?</li> <li>How do embeddings relate to Transformer models?</li> <li>What preprocessing steps are most critical?</li> </ul> <p>Next: Day 19 - Generative Adversarial Networks</p>"},{"location":"Week4_Day19/","title":"Week 4, Day 19: Generative Adversarial Networks (GANs)","text":""},{"location":"Week4_Day19/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand GAN architecture and training dynamics</li> <li>Learn generator and discriminator roles</li> <li>Implement simple GAN from scratch</li> <li>Train GAN on MNIST</li> <li>Understand mode collapse and solutions</li> <li>Generate new images</li> </ul>"},{"location":"Week4_Day19/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week4_Day19/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week4_Day19/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: GANs Explained by Computerphile (10 min) Simple introduction to GANs</p> <p>\u2610 Watch: Generative Adversarial Networks by StatQuest (15 min) Clear breakdown of GAN components</p> <p>\u2610 Watch: GANs from Scratch by Aladdin Persson (25 min) Implementation walkthrough</p> <p>\u2610 Watch: DCGAN Tutorial (20 min) Deep Convolutional GANs</p> <p>\u2610 Watch: GAN Tips and Tricks (20 min) Making GANs train successfully</p>"},{"location":"Week4_Day19/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 20.1 - GANs</p> <p>\u2610 Read: Original GAN Paper - introduction</p> <p>\u2610 Read: DCGAN Paper - architecture guidelines</p>"},{"location":"Week4_Day19/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week4_Day19/#exercise-1-understanding-gan-components-45-min","title":"Exercise 1: Understanding GAN Components (45 min)","text":"<p>GAN Architecture:</p> <p>Generator: Noise \u2192 Fake Data - Input: Random noise vector z (latent space) - Output: Generated sample (e.g., image) - Goal: Fool the discriminator</p> <p>Discriminator: Data \u2192 Real/Fake Classification - Input: Sample (real or fake) - Output: Probability it's real - Goal: Distinguish real from fake</p> <p>Training: Minimax game - Discriminator maximizes: $\\log(D(x)) + \\log(1 - D(G(z)))$ - Generator minimizes: $\\log(1 - D(G(z)))$    or equivalently, maximizes $\\log(D(G(z)))$</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: GAN COMPONENTS\")\nprint(\"=\"*70)\n\n# Simple Generator\nclass SimpleGenerator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(SimpleGenerator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.LeakyReLU(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, output_dim),\n            nn.Tanh()  # Output in [-1, 1]\n        )\n\n    def forward(self, z):\n        return self.model(z)\n\n# Simple Discriminator\nclass SimpleDiscriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleDiscriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid()  # Output probability\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Test components\nlatent_dim = 100\ndata_dim = 784  # 28x28 images flattened\n\ngenerator = SimpleGenerator(latent_dim, data_dim)\ndiscriminator = SimpleDiscriminator(data_dim)\n\nprint(\"\\nGenerator:\")\nprint(f\"  Input: {latent_dim}-dim noise\")\nprint(f\"  Output: {data_dim}-dim data\")\nprint(f\"  Parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n\nprint(\"\\nDiscriminator:\")\nprint(f\"  Input: {data_dim}-dim data\")\nprint(f\"  Output: 1-dim probability\")\nprint(f\"  Parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n\n# Test forward pass\nz = torch.randn(4, latent_dim)  # Batch of 4 noise vectors\nfake_data = generator(z)\nprint(f\"\\nNoise shape: {z.shape}\")\nprint(f\"Generated data shape: {fake_data.shape}\")\nprint(f\"Generated data range: [{fake_data.min():.2f}, {fake_data.max():.2f}]\")\n\n# Discriminate real vs fake\nreal_data = torch.randn(4, data_dim)\nreal_pred = discriminator(real_data)\nfake_pred = discriminator(fake_data.detach())\n\nprint(f\"\\nReal data predictions: {real_pred.squeeze().tolist()}\")\nprint(f\"Fake data predictions: {fake_pred.squeeze().tolist()}\")\n\nprint(\"\\n Generator creates data, Discriminator judges it!\")\nprint(\"\\n Exercise 1 complete\")\n</code></pre>"},{"location":"Week4_Day19/#exercise-2-gan-training-loop-60-min","title":"Exercise 2: GAN Training Loop (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: GAN TRAINING LOOP\")\nprint(\"=\"*70)\n\n# Load MNIST\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n])\n\nmnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ndataloader = DataLoader(mnist, batch_size=128, shuffle=True)\n\nprint(f\"MNIST dataset: {len(mnist)} images\")\nprint(f\"Batches: {len(dataloader)}\\n\")\n\n# Initialize models\nlatent_dim = 100\ngenerator = SimpleGenerator(latent_dim, 784)\ndiscriminator = SimpleDiscriminator(784)\n\n# Loss and optimizers\ncriterion = nn.BCELoss()\nlr = 0.0002\nbeta1 = 0.5\n\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\nprint(\"Training GAN on MNIST...\")\nprint(\"(This may take a few minutes)\\n\")\n\nepochs = 10\ng_losses = []\nd_losses = []\nreal_scores = []\nfake_scores = []\n\nfor epoch in range(epochs):\n    epoch_g_loss = 0\n    epoch_d_loss = 0\n    epoch_real_score = 0\n    epoch_fake_score = 0\n\n    for i, (real_images, _) in enumerate(dataloader):\n        batch_size = real_images.size(0)\n        real_images = real_images.view(batch_size, -1)\n\n        # Labels\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        # ---------------------\n        # Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n\n        # Real images\n        real_output = discriminator(real_images)\n        d_real_loss = criterion(real_output, real_labels)\n\n        # Fake images\n        z = torch.randn(batch_size, latent_dim)\n        fake_images = generator(z).detach()\n        fake_output = discriminator(fake_images)\n        d_fake_loss = criterion(fake_output, fake_labels)\n\n        # Total discriminator loss\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        # ---------------------\n        # Train Generator\n        # ---------------------\n        optimizer_G.zero_grad()\n\n        # Generate fake images\n        z = torch.randn(batch_size, latent_dim)\n        fake_images = generator(z)\n\n        # Try to fool discriminator\n        fake_output = discriminator(fake_images)\n        g_loss = criterion(fake_output, real_labels)  # Want D to think they're real\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Track statistics\n        epoch_g_loss += g_loss.item()\n        epoch_d_loss += d_loss.item()\n        epoch_real_score += real_output.mean().item()\n        epoch_fake_score += fake_output.mean().item()\n\n    # Average for epoch\n    avg_g_loss = epoch_g_loss / len(dataloader)\n    avg_d_loss = epoch_d_loss / len(dataloader)\n    avg_real_score = epoch_real_score / len(dataloader)\n    avg_fake_score = epoch_fake_score / len(dataloader)\n\n    g_losses.append(avg_g_loss)\n    d_losses.append(avg_d_loss)\n    real_scores.append(avg_real_score)\n    fake_scores.append(avg_fake_score)\n\n    print(f\"Epoch [{epoch+1}/{epochs}] \"\n          f\"D_loss: {avg_d_loss:.4f} G_loss: {avg_g_loss:.4f} \"\n          f\"D(x): {avg_real_score:.4f} D(G(z)): {avg_fake_score:.4f}\")\n\nprint(\"\\n Exercise 2 complete\")\n</code></pre>"},{"location":"Week4_Day19/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week4_Day19/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week4_Day19/#exercise-3-visualize-generated-images-40-min","title":"Exercise 3: Visualize Generated Images (40 min)","text":"<p>Observations: - $D(x)$ should stay around 0.5-0.7 (recognizes real as real) - $D(G(z))$ should approach 0.5 (fakes become realistic) - If gap is too large, discriminator is winning - If gap is too small, generator might be winning (mode collapse)</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: VISUALIZING GENERATED IMAGES\")\nprint(\"=\"*70)\n\n# Generate samples\ngenerator.eval()\nwith torch.no_grad():\n    z = torch.randn(64, latent_dim)\n    fake_images = generator(z)\n    fake_images = fake_images.view(-1, 1, 28, 28)\n    fake_images = (fake_images + 1) / 2  # Scale to [0, 1]\n\n# Plot generated images\nfig, axes = plt.subplots(8, 8, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(64):\n    axes[idx].imshow(fake_images[idx].squeeze(), cmap='gray')\n    axes[idx].axis('off')\n\nplt.suptitle(f'Generated MNIST Digits (After {epochs} Epochs)', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Plot training curves\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Losses\naxes[0].plot(g_losses, label='Generator', linewidth=2)\naxes[0].plot(d_losses, label='Discriminator', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('GAN Training Losses')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Scores\naxes[1].plot(real_scores, label='D(x) - Real', linewidth=2)\naxes[1].plot(fake_scores, label='D(G(z)) - Fake', linewidth=2)\naxes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Target')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Discriminator Output')\naxes[1].set_title('Discriminator Predictions')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Combined\naxes[2].plot(np.array(real_scores) - np.array(fake_scores), \n            label='D(x) - D(G(z))', linewidth=2, color='purple')\naxes[2].axhline(y=0, color='r', linestyle='--', alpha=0.5)\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Score Difference')\naxes[2].set_title('Real vs Fake Gap')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Week4_Day19/#exercise-4-dcgan-implementation-80-min","title":"Exercise 4: DCGAN Implementation (80 min)","text":"<p>DCGAN Guidelines (from paper): 1. Replace pooling with strided convolutions (D) and fractional-strided convolutions (G) 2. Use BatchNorm in both G and D 3. Remove fully connected hidden layers 4. Use ReLU in G (except output uses Tanh) 5. Use LeakyReLU in D</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: DEEP CONVOLUTIONAL GAN (DCGAN)\")\nprint(\"=\"*70)\n\nclass DCGANGenerator(nn.Module):\n    def __init__(self, latent_dim, channels=1):\n        super(DCGANGenerator, self).__init__()\n\n        self.init_size = 7\n        self.l1 = nn.Sequential(\n            nn.Linear(latent_dim, 128 * self.init_size ** 2)\n        )\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),  # 7 \u2192 14\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),  # 14 \u2192 28\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self, channels=1):\n        super(DCGANDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            block.append(nn.Dropout2d(0.25))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 16, bn=False),  # 28 \u2192 14\n            *discriminator_block(16, 32),  # 14 \u2192 7\n            *discriminator_block(32, 64),  # 7 \u2192 3\n            *discriminator_block(64, 128),  # 3 \u2192 1\n        )\n\n        self.adv_layer = nn.Sequential(\n            nn.Linear(128 * 1 * 1, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        return validity\n\n# Create DCGAN\nlatent_dim = 100\ndcgan_generator = DCGANGenerator(latent_dim, channels=1)\ndcgan_discriminator = DCGANDiscriminator(channels=1)\n\nprint(f\"\\nDCGAN Generator:\")\nprint(f\"  Parameters: {sum(p.numel() for p in dcgan_generator.parameters()):,}\")\n\nprint(f\"\\nDCGAN Discriminator:\")\nprint(f\"  Parameters: {sum(p.numel() for p in dcgan_discriminator.parameters()):,}\")\n\n# Train DCGAN (fewer epochs for demo)\noptimizer_G = optim.Adam(dcgan_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(dcgan_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\nprint(\"\\nTraining DCGAN...\\n\")\nepochs = 5\n\nfor epoch in range(epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        batch_size = imgs.size(0)\n\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n\n        real_output = dcgan_discriminator(imgs)\n        d_real_loss = criterion(real_output, real_labels)\n\n        z = torch.randn(batch_size, latent_dim)\n        fake_imgs = dcgan_generator(z).detach()\n        fake_output = dcgan_discriminator(fake_imgs)\n        d_fake_loss = criterion(fake_output, fake_labels)\n\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n\n        z = torch.randn(batch_size, latent_dim)\n        fake_imgs = dcgan_generator(z)\n        fake_output = dcgan_discriminator(fake_imgs)\n        g_loss = criterion(fake_output, real_labels)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n    print(f\"Epoch [{epoch+1}/{epochs}] \"\n          f\"D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")\n\n# Generate samples with DCGAN\ndcgan_generator.eval()\nwith torch.no_grad():\n    z = torch.randn(64, latent_dim)\n    fake_images = dcgan_generator(z)\n    fake_images = (fake_images + 1) / 2  # Scale to [0, 1]\n\n# Plot\nfig, axes = plt.subplots(8, 8, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(64):\n    axes[idx].imshow(fake_images[idx].squeeze(), cmap='gray')\n    axes[idx].axis('off')\n\nplt.suptitle(f'DCGAN Generated Digits (After {epochs} Epochs)',\n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n DCGAN produces sharper images than simple GAN!\")\nprint(\"\\n Exercise 4 complete\")\n</code></pre>"},{"location":"Week4_Day19/#mini-challenge-gan-evaluation-mode-collapse-50-min","title":"Mini-Challenge: GAN Evaluation &amp; Mode Collapse (50 min)","text":"<p>Key GAN Challenges 1. Mode Collapse: Generator produces limited variety 2. Training Instability: Losses oscillate 3. Evaluation: Hard to quantify quality objectively  </p> <p>Solutions: - Use proven architectures (DCGAN) - Careful hyperparameter tuning - Label smoothing, noise injection - Wasserstein GAN loss (more stable)</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: GAN EVALUATION\")\nprint(\"=\"*70)\n\n# Evaluate diversity of generated samples\ndef evaluate_diversity(generator, latent_dim, n_samples=1000):\n    \"\"\"Check if generator produces diverse outputs\"\"\"\n    generator.eval()\n\n    generated = []\n    with torch.no_grad():\n        for _ in range(n_samples // 64):\n            z = torch.randn(64, latent_dim)\n            imgs = generator(z)\n            generated.append(imgs)\n\n    generated = torch.cat(generated, dim=0)\n\n    # Compute pairwise distances\n    generated_flat = generated.view(generated.size(0), -1)\n\n    # Random sample for efficiency\n    sample_size = 100\n    indices = np.random.choice(len(generated_flat), sample_size, replace=False)\n    sample = generated_flat[indices]\n\n    distances = []\n    for i in range(len(sample)):\n        for j in range(i+1, len(sample)):\n            dist = torch.norm(sample[i] - sample[j]).item()\n            distances.append(dist)\n\n    return np.mean(distances), np.std(distances)\n\n# Evaluate both generators\nmean_dist_simple, std_dist_simple = evaluate_diversity(generator, latent_dim)\nmean_dist_dcgan, std_dist_dcgan = evaluate_diversity(dcgan_generator, latent_dim)\n\nprint(f\"\\nDiversity Metrics:\")\nprint(f\"Simple GAN:  Mean distance = {mean_dist_simple:.4f}, Std = {std_dist_simple:.4f}\")\nprint(f\"DCGAN:       Mean distance = {mean_dist_dcgan:.4f}, Std = {std_dist_dcgan:.4f}\")\n\n# Check digit distribution\ndef check_digit_distribution(generator, latent_dim, n_samples=100):\n    \"\"\"Visually check if all digits are represented\"\"\"\n    generator.eval()\n\n    with torch.no_grad():\n        z = torch.randn(n_samples, latent_dim)\n        imgs = generator(z)\n        imgs = (imgs + 1) / 2\n\n    return imgs\n\nsimple_samples = check_digit_distribution(generator, latent_dim, 100)\ndcgan_samples = check_digit_distribution(dcgan_generator, latent_dim, 100)\n\n# Plot comparison\nfig, axes = plt.subplots(2, 10, figsize=(15, 4))\n\nfor i in range(10):\n    axes[0, i].imshow(simple_samples[i].squeeze(), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel('Simple GAN', rotation=0, ha='right', va='center')\n\n    axes[1, i].imshow(dcgan_samples[i].squeeze(), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel('DCGAN', rotation=0, ha='right', va='center')\n\nplt.suptitle('Generated Digit Diversity', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Mini-challenge complete\")\n</code></pre>"},{"location":"Week4_Day19/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review GAN architecture and training \u2610 Understand adversarial dynamics \u2610 Note common failure modes \u2610 Write daily reflection  </p>"},{"location":"Week4_Day19/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do Generator and Discriminator interact?</li> <li>Why is GAN training unstable?</li> <li>What is mode collapse and how to detect it?</li> <li>How might you apply GANs to your domain?</li> <li>What's the difference between simple GAN and DCGAN?</li> </ul> <p>Next: Day 20 - Sentiment Analysis Project</p>"},{"location":"Week4_Day20/","title":"Week 4, Day 20: Sentiment Analysis Project - Movie Reviews","text":""},{"location":"Week4_Day20/#daily-goals","title":"Daily Goals","text":"<ul> <li>Complete end-to-end sentiment analysis project</li> <li>Apply RNN/LSTM, embeddings, and attention concepts</li> <li>Achieve &gt;85% accuracy on movie review classification</li> <li>Build complete NLP pipeline from preprocessing to deployment</li> <li>Create portfolio-ready project with professional documentation</li> </ul>"},{"location":"Week4_Day20/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week4_Day20/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week4_Day20/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: Sentiment Analysis Walkthrough (15 min)</p> <p>\u2610 Optional Review: Any Week 4 videos as needed (15 min)</p>"},{"location":"Week4_Day20/#project-briefing-30-min","title":"Project Briefing (30 min)","text":"<p>SENTIMENT ANALYSIS PROJECT</p> <p>Dataset: IMDB Movie Reviews - 50,000 reviews (25,000 train, 25,000 test) - Binary classification: Positive (1) or Negative (0) - Real text data with variety and complexity</p> <p>Goal: Build sentiment classifier achieving &gt;85% accuracy</p> <p>Project Structure: Phase 1: Data Loading &amp; Exploration (30 min) Phase 2: Text Preprocessing Pipeline (45 min) Phase 3: Baseline Model (45 min) Phase 4: Advanced Model with Attention (60 min) Phase 5: Evaluation &amp; Documentation (45 min)  </p> <p>Success Criteria: - Minimum: &gt;80% test accuracy - Target: &gt;85% test accuracy - Stretch: &gt;88% test accuracy - Clean, documented code - Professional visualizations</p>"},{"location":"Week4_Day20/#phase-1-data-loading-exploration-30-min","title":"Phase 1: Data Loading &amp; Exploration (30 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: DATA LOADING &amp; EXPLORATION\")\nprint(\"=\"*70)\n\n# For this exercise, we'll use a subset of IMDB-like reviews\n# In practice, use datasets.IMDB from torchtext or download from Kaggle\n\n# Simulated movie reviews\npositive_reviews = [\n    \"This movie was absolutely fantastic! I loved every minute of it.\",\n    \"Brilliant performance by the cast. Highly recommend watching it.\",\n    \"One of the best films I've seen this year. Amazing storytelling.\",\n    \"Incredible cinematography and a compelling plot. Must watch!\",\n    \"Outstanding direction and great character development throughout.\",\n] * 100  # Duplicate for larger dataset\n\nnegative_reviews = [\n    \"Terrible movie. Complete waste of time and money.\",\n    \"The worst film I've ever seen. Poor acting and boring plot.\",\n    \"Disappointing on every level. Would not recommend to anyone.\",\n    \"Awful screenplay and terrible direction. Avoid at all costs.\",\n    \"Boring and predictable. I fell asleep halfway through.\",\n] * 100\n\n# Combine\nall_reviews = positive_reviews + negative_reviews\nall_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n\nprint(f\"Total reviews: {len(all_reviews)}\")\nprint(f\"Positive: {sum(all_labels)}\")\nprint(f\"Negative: {len(all_labels) - sum(all_labels)}\\n\")\n\n# Shuffle\nindices = np.random.permutation(len(all_reviews))\nall_reviews = [all_reviews[i] for i in indices]\nall_labels = [all_labels[i] for i in indices]\n\n# Split\ntrain_size = int(0.8 * len(all_reviews))\ntrain_reviews = all_reviews[:train_size]\ntrain_labels = all_labels[:train_size]\ntest_reviews = all_reviews[train_size:]\ntest_labels = all_labels[train_size:]\n\nprint(f\"Training set: {len(train_reviews)}\")\nprint(f\"Test set: {len(test_reviews)}\\n\")\n\n# Explore samples\nprint(\"Sample reviews:\")\nfor i in range(3):\n    sentiment = \"Positive\" if train_labels[i] == 1 else \"Negative\"\n    print(f\"\\n{sentiment}: '{train_reviews[i]}'\")\n\n# Analyze review lengths\ntrain_lengths = [len(review.split()) for review in train_reviews]\n\nplt.figure(figsize=(10, 5))\nplt.hist(train_lengths, bins=50, edgecolor='black')\nplt.xlabel('Review Length (words)')\nplt.ylabel('Count')\nplt.title('Distribution of Review Lengths')\nplt.axvline(np.mean(train_lengths), color='r', linestyle='--', \n           label=f'Mean: {np.mean(train_lengths):.1f}')\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.show()\n\nprint(f\"\\nLength statistics:\")\nprint(f\"  Mean: {np.mean(train_lengths):.1f}\")\nprint(f\"  Median: {np.median(train_lengths):.1f}\")\nprint(f\"  Max: {max(train_lengths)}\")\n\nprint(\"\\n Phase 1 complete\")\n</code></pre>"},{"location":"Week4_Day20/#phase-2-text-preprocessing-pipeline-45-min","title":"Phase 2: Text Preprocessing Pipeline (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: TEXT PREPROCESSING\")\nprint(\"=\"*70)\n\nclass TextPreprocessor:\n    def __init__(self, max_vocab_size=10000, max_len=100):\n        self.max_vocab_size = max_vocab_size\n        self.max_len = max_len\n        self.vocab = None\n        self.word_to_idx = None\n        self.idx_to_word = None\n\n    def clean_text(self, text):\n        \"\"\"Clean and normalize text\"\"\"\n        # Lowercase\n        text = text.lower()\n        # Remove special characters but keep important punctuation\n        text = re.sub(r'[^a-z\\s!?.,]', '', text)\n        return text\n\n    def tokenize(self, text):\n        \"\"\"Split text into tokens\"\"\"\n        return text.split()\n\n    def build_vocab(self, texts):\n        \"\"\"Build vocabulary from texts\"\"\"\n        # Tokenize all texts\n        all_tokens = []\n        for text in texts:\n            tokens = self.tokenize(self.clean_text(text))\n            all_tokens.extend(tokens)\n\n        # Count frequencies\n        word_freq = Counter(all_tokens)\n\n        # Keep most common words\n        most_common = word_freq.most_common(self.max_vocab_size - 2)\n\n        # Build vocab\n        self.vocab = ['&lt;PAD&gt;', '&lt;UNK&gt;'] + [word for word, _ in most_common]\n        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n\n        print(f\"Vocabulary built: {len(self.vocab)} words\")\n        return self.vocab\n\n    def text_to_sequence(self, text):\n        \"\"\"Convert text to sequence of indices\"\"\"\n        tokens = self.tokenize(self.clean_text(text))\n        sequence = [self.word_to_idx.get(token, self.word_to_idx['&lt;UNK&gt;']) \n                   for token in tokens]\n        return sequence\n\n    def pad_sequence(self, sequence):\n        \"\"\"Pad or truncate sequence to fixed length\"\"\"\n        if len(sequence) &lt; self.max_len:\n            sequence = sequence + [self.word_to_idx['&lt;PAD&gt;']] * (self.max_len - len(sequence))\n        else:\n            sequence = sequence[:self.max_len]\n        return sequence\n\n    def preprocess(self, texts):\n        \"\"\"Complete preprocessing pipeline\"\"\"\n        sequences = [self.text_to_sequence(text) for text in texts]\n        padded = [self.pad_sequence(seq) for seq in sequences]\n        return np.array(padded)\n\n# Create preprocessor\npreprocessor = TextPreprocessor(max_vocab_size=5000, max_len=50)\n\n# Build vocabulary\nvocab = preprocessor.build_vocab(train_reviews)\n\nprint(f\"\\nVocabulary sample: {vocab[:20]}\")\n\n# Preprocess data\nX_train = preprocessor.preprocess(train_reviews)\nX_test = preprocessor.preprocess(test_reviews)\n\ny_train = np.array(train_labels)\ny_test = np.array(test_labels)\n\nprint(f\"\\nPreprocessed shapes:\")\nprint(f\"  X_train: {X_train.shape}\")\nprint(f\"  y_train: {y_train.shape}\")\nprint(f\"  X_test: {X_test.shape}\")\nprint(f\"  y_test: {y_test.shape}\")\n\n# Visualize a preprocessed example\nexample_idx = 0\nexample_text = train_reviews[example_idx]\nexample_sequence = X_train[example_idx]\n\nprint(f\"\\nExample preprocessing:\")\nprint(f\"Original: '{example_text}'\")\nprint(f\"Tokens: {[preprocessor.idx_to_word[idx] for idx in example_sequence[:20]]}\")\nprint(f\"Indices: {example_sequence[:20]}\")\n\nprint(\"\\n Phase 2 complete\")\n</code></pre>"},{"location":"Week4_Day20/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week4_Day20/#phase-3-baseline-model-45-min","title":"Phase 3: Baseline Model (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: BASELINE LSTM MODEL\")\nprint(\"=\"*70)\n\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super(SentimentLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n                           batch_first=True, dropout=dropout if n_layers &gt; 1 else 0)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.dropout(self.embedding(x))\n\n        # LSTM (use final hidden state)\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Use last layer's hidden state\n        output = self.fc(self.dropout(hidden[-1]))\n        return self.sigmoid(output)\n\n# Create model\nvocab_size = len(preprocessor.vocab)\nembedding_dim = 128\nhidden_dim = 256\n\nbaseline_model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim)\n\nprint(f\"Baseline Model:\")\nprint(baseline_model)\nprint(f\"\\nParameters: {sum(p.numel() for p in baseline_model.parameters()):,}\")\n\n# Prepare data loaders\ntrain_dataset = torch.utils.data.TensorDataset(\n    torch.LongTensor(X_train),\n    torch.FloatTensor(y_train)\n)\ntest_dataset = torch.utils.data.TensorDataset(\n    torch.LongTensor(X_test),\n    torch.FloatTensor(y_test)\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Training function\ndef train_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n    best_test_acc = 0\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for inputs, labels in train_loader:\n            labels = labels.unsqueeze(1)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            predictions = (outputs &gt; 0.5).float()\n            train_correct += (predictions == labels).sum().item()\n            train_total += labels.size(0)\n\n        # Test\n        model.eval()\n        test_correct = 0\n        test_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                labels = labels.unsqueeze(1)\n                outputs = model(inputs)\n                predictions = (outputs &gt; 0.5).float()\n                test_correct += (predictions == labels).sum().item()\n                test_total += labels.size(0)\n\n        train_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        test_acc = test_correct / test_total\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n\n        if test_acc &gt; best_test_acc:\n            best_test_acc = test_acc\n            torch.save(model.state_dict(), 'best_sentiment_model.pth')\n\n        print(f\"Epoch {epoch+1}/{epochs}: \"\n              f\"Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n    return history, best_test_acc\n\nprint(\"\\nTraining baseline model...\")\nbaseline_history, baseline_best_acc = train_model(baseline_model, train_loader, test_loader, epochs=10)\n\nprint(f\"\\n Baseline Best Test Accuracy: {baseline_best_acc:.4f}\")\n\nprint(\"\\n Phase 3 complete\")\n</code></pre>"},{"location":"Week4_Day20/#phase-4-advanced-model-with-attention-60-min","title":"Phase 4: Advanced Model with Attention (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: LSTM WITH ATTENTION\")\nprint(\"=\"*70)\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        # lstm_output: (batch, seq_len, hidden_dim)\n\n        # Compute attention scores\n        attention_scores = self.attention(lstm_output)  # (batch, seq_len, 1)\n        attention_weights = torch.softmax(attention_scores, dim=1)\n\n        # Apply attention\n        attended = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_dim)\n\n        return attended, attention_weights\n\nclass SentimentLSTMWithAttention(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super(SentimentLSTMWithAttention, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n                           batch_first=True, dropout=dropout if n_layers &gt; 1 else 0,\n                           bidirectional=False)\n        self.attention = AttentionLayer(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.dropout(self.embedding(x))\n\n        # LSTM\n        lstm_out, _ = self.lstm(embedded)\n\n        # Attention\n        attended, attention_weights = self.attention(lstm_out)\n\n        # Classify\n        output = self.fc(self.dropout(attended))\n        return self.sigmoid(output), attention_weights\n\n# Create attention model\nattention_model = SentimentLSTMWithAttention(vocab_size, embedding_dim, hidden_dim)\n\nprint(f\"Attention Model:\")\nprint(attention_model)\nprint(f\"\\nParameters: {sum(p.numel() for p in attention_model.parameters()):,}\")\n\n# Modified training for attention model\ndef train_attention_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n    best_test_acc = 0\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for inputs, labels in train_loader:\n            labels = labels.unsqueeze(1)\n\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            predictions = (outputs &gt; 0.5).float()\n            train_correct += (predictions == labels).sum().item()\n            train_total += labels.size(0)\n\n        # Test\n        model.eval()\n        test_correct = 0\n        test_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                labels = labels.unsqueeze(1)\n                outputs, _ = model(inputs)\n                predictions = (outputs &gt; 0.5).float()\n                test_correct += (predictions == labels).sum().item()\n                test_total += labels.size(0)\n\n        train_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        test_acc = test_correct / test_total\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n\n        if test_acc &gt; best_test_acc:\n            best_test_acc = test_acc\n            torch.save(model.state_dict(), 'best_attention_model.pth')\n\n        print(f\"Epoch {epoch+1}/{epochs}: \"\n              f\"Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n    return history, best_test_acc\n\nprint(\"\\nTraining attention model...\")\nattention_history, attention_best_acc = train_attention_model(attention_model, train_loader, test_loader, epochs=10)\n\nprint(f\"\\n Attention Model Best Test Accuracy: {attention_best_acc:.4f}\")\nprint(f\"Improvement over baseline: +{(attention_best_acc - baseline_best_acc):.4f}\")\n\nprint(\"\\n Phase 4 complete\")\n</code></pre>"},{"location":"Week4_Day20/#phase-5-evaluation-documentation-45-min","title":"Phase 5: Evaluation &amp; Documentation (45 min)","text":"<p>KEY INSIGHTS 1. Attention mechanism improves accuracy 2. Model focuses on sentiment-heavy words 3. Bidirectional LSTM could improve further 4. Pretrained embeddings (GloVe) could help</p> <p>TECHNIQUES APPLIED - Text preprocessing pipeline - Custom vocabulary building - LSTM for sequential modeling - Attention mechanism - Proper train/test splitting - Model checkpointing</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: COMPREHENSIVE EVALUATION\")\nprint(\"=\"*70)\n\n# Load best models\nbaseline_model.load_state_dict(torch.load('best_sentiment_model.pth'))\nattention_model.load_state_dict(torch.load('best_attention_model.pth'))\n\n# Compare training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(baseline_history['train_acc'])+1)\n\n# Accuracy comparison\naxes[0].plot(epochs_range, baseline_history['test_acc'], label='Baseline', marker='o')\naxes[0].plot(epochs_range, attention_history['test_acc'], label='With Attention', marker='s')\naxes[0].axhline(y=0.85, color='r', linestyle='--', alpha=0.5, label='Target (85%)')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Test Accuracy')\naxes[0].set_title('Test Accuracy Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Loss comparison\naxes[1].plot(epochs_range, baseline_history['train_loss'], label='Baseline')\naxes[1].plot(epochs_range, attention_history['train_loss'], label='With Attention')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Training Loss')\naxes[1].set_title('Training Loss Comparison')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\nattention_model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs, _ = attention_model(inputs)\n        predictions = (outputs &gt; 0.5).squeeze().long()\n        all_preds.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n           xticklabels=['Negative', 'Positive'],\n           yticklabels=['Negative', 'Positive'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (Accuracy: {attention_best_acc:.4f})')\nplt.show()\n\n# Visualize attention weights\ndef visualize_attention(model, text, preprocessor):\n    \"\"\"Visualize what the model pays attention to\"\"\"\n    model.eval()\n\n    # Preprocess\n    sequence = preprocessor.text_to_sequence(text)\n    padded = preprocessor.pad_sequence(sequence)\n    input_tensor = torch.LongTensor([padded])\n\n    # Get prediction and attention\n    with torch.no_grad():\n        output, attention_weights = model(input_tensor)\n\n    prediction = \"Positive\" if output.item() &gt; 0.5 else \"Negative\"\n    confidence = output.item() if output.item() &gt; 0.5 else 1 - output.item()\n\n    # Get words (remove padding)\n    words = [preprocessor.idx_to_word[idx] for idx in padded if idx != 0]\n    attention = attention_weights[0, :len(words), 0].cpu().numpy()\n\n    return prediction, confidence, words, attention\n\n# Test on sample reviews\ntest_samples = [\n    \"This movie was absolutely fantastic and amazing!\",\n    \"Terrible film, complete waste of time and money.\",\n    \"Great acting but the plot was somewhat boring.\"\n]\n\nprint(\"\\nAttention Visualization:\")\nfor sample_text in test_samples:\n    pred, conf, words, attn = visualize_attention(attention_model, sample_text, preprocessor)\n\n    print(f\"\\nText: '{sample_text}'\")\n    print(f\"Prediction: {pred} (confidence: {conf:.4f})\")\n    print(\"Attention weights:\")\n\n    # Show top attended words\n    word_attention = list(zip(words, attn))\n    word_attention.sort(key=lambda x: x[1], reverse=True)\n\n    for word, weight in word_attention[:5]:\n        bar = \"\u2588\" * int(weight * 50)\n        print(f\"  {word:15s}: {weight:.4f} {bar}\")\n\n# Final report - to be printed to the screen and saved to txt file\nreport = f\"\"\"\n{'='*70}\nSENTIMENT ANALYSIS PROJECT - FINAL REPORT\n{'='*70}\n\nDATASET\n-------\nTraining samples: {len(train_reviews)}\nTest samples: {len(test_reviews)}\nVocabulary size: {len(preprocessor.vocab)}\nMax sequence length: {preprocessor.max_len}\n\nMODELS EVALUATED\n----------------\n1. Baseline LSTM\n   - Architecture: Embedding \u2192 LSTM (2 layers) \u2192 FC\n   - Parameters: {sum(p.numel() for p in baseline_model.parameters()):,}\n   - Best Test Accuracy: {baseline_best_acc:.4f}\n\n2. LSTM with Attention\n   - Architecture: Embedding \u2192 LSTM \u2192 Attention \u2192 FC\n   - Parameters: {sum(p.numel() for p in attention_model.parameters()):,}\n   - Best Test Accuracy: {attention_best_acc:.4f}\n   - Improvement: +{(attention_best_acc - baseline_best_acc):.4f}\n\nBEST MODEL: LSTM with Attention\nStatus: {' TARGET ACHIEVED (&gt;85%)' if attention_best_acc &gt; 0.85 else ' Below target'}\n\nKEY INSIGHTS\n------------\n1. Attention mechanism improves accuracy\n2. Model focuses on sentiment-heavy words\n3. Bidirectional LSTM could improve further\n4. Pretrained embeddings (GloVe) could help\n\nTECHNIQUES APPLIED\n------------------\n Text preprocessing pipeline\n Custom vocabulary building\n LSTM for sequential modeling\n Attention mechanism\n Proper train/test splitting\n Model checkpointing\n\n{'='*70}\nWeek 4 Complete! \n{'='*70}\n\"\"\"\n\nprint(report)\n\n# Save report\nwith open('sentiment_analysis_report.txt', 'w') as f:\n    f.write(report)\n\nprint(\"\\n\ud83d\udcc4 Report saved: sentiment_analysis_report.txt\")\nprint(\"\ud83d\udcbe Models saved: best_sentiment_model.pth, best_attention_model.pth\")\n\nprint(\"\\n Project complete!\")\n</code></pre>"},{"location":"Week4_Day20/#week-4-reflection-prompts-address-all","title":"Week 4 Reflection Prompts (Address All):","text":"<ul> <li>What was the most valuable thing you learned this week?</li> <li>How do RNNs differ from CNNs in what they model?</li> <li>What is the significance of attention mechanisms?</li> <li>How do GANs enable generative modeling?</li> <li>What connections do you see to modern LLMs?</li> <li>How confident do you feel about NLP now?</li> <li>What would you explore further in advanced ML?</li> </ul>"},{"location":"Week4_Day20/#week-4-achievement-checklist","title":"Week 4 Achievement Checklist:","text":"<p>\u2610 Understood RNNs and LSTMs \u2610 Implemented attention mechanism \u2610 Grasped Transformer architecture conceptually \u2610 Processed text data for NLP \u2610 Built and trained GANs \u2610 Completed sentiment analysis project &gt;85% accuracy \u2610 Created professional documentation  </p>"},{"location":"Week4_Day20/#week-4-complete","title":"Week 4 Complete!","text":"<p>Achievements Unlocked: -  Sequential modeling with RNNs/LSTMs -  Attention mechanisms understood -  Transformer architecture grasped -  NLP pipeline mastery -  Generative modeling with GANs -  Portfolio sentiment analysis project</p> <p>What You Can Now Do: - Build sequence models for text and time series - Understand modern language models architecturally - Process text data professionally - Generate images with GANs - Deploy sentiment classifiers - Understand the foundations of ChatGPT/GPT-4</p> <p>Congratulations on completing Week 4! </p> <p>You've mastered advanced deep learning topics and built impressive projects!</p> <p>Next: Week 5 Overview - Model Deployment &amp; Best Practices</p>"},{"location":"Week4_Overview/","title":"Week 4 Overview: Advanced Deep Learning - RNNs, Transformers &amp; GANs","text":""},{"location":"Week4_Overview/#introduction","title":"Introduction","text":"<p>Week 4 covers advanced deep learning architectures that power modern AI: Recurrent Neural Networks for sequences, Attention mechanisms and Transformers for language understanding, and Generative Adversarial Networks for creating new content. You'll understand the foundations of ChatGPT, image generation, and time series forecasting.</p>"},{"location":"Week4_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Understand RNNs and LSTMs for processing sequential data</li> <li>Learn attention mechanisms and self-attention fundamentals</li> <li>Grasp Transformer architecture at a conceptual level</li> <li>Explore GANs for generative modeling</li> <li>Process text data for NLP tasks and use word embeddings</li> <li>Complete sentiment analysis project on movie reviews achieving &gt;85% accuracy</li> </ul>"},{"location":"Week4_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 16: RNNs &amp; LSTMs - Sequential Data Processing</li> <li>Day 17: Attention Mechanisms &amp; Transformers</li> <li>Day 18: NLP Fundamentals - Text Processing &amp; Embeddings</li> <li>Day 19: Generative Adversarial Networks (GANs)</li> <li>Day 20: Sentiment Analysis Project - Movie Reviews</li> </ul>"},{"location":"Week5_Day21/","title":"Week 5, Day 21: Project Planning &amp; Setup","text":""},{"location":"Week5_Day21/#daily-goals","title":"Daily Goals","text":"<p>Today you'll choose your project track, define scope, explore your dataset, and create a project plan. This planning phase is crucial - a well-defined project is half-done!</p>"},{"location":"Week5_Day21/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week5_Day21/#project-track-selection-90-min","title":"Project Track Selection (90 min)","text":"<p>Choose ONE track that interests you most. Read through all three before deciding.</p>"},{"location":"Week5_Day21/#track-1-medical-image-classification","title":"Track 1: Medical Image Classification","text":"<p>Problem: Classify chest X-rays as Normal or Pneumonia</p> <p>Dataset: Chest X-Ray Images (Pneumonia) from Kaggle - ~5,800 images (train/test split provided) - 2 classes: NORMAL, PNEUMONIA - Real medical imaging data - Link: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia</p> <p>Why this project: - Practical healthcare application - Clear binary classification - Good for practicing CNNs and transfer learning - Imbalanced classes (real-world challenge)</p> <p>Recommended approach: - Start with pretrained ResNet18 or VGG16 - Use data augmentation (rotation, flip, zoom) - Handle class imbalance (weighted loss or oversampling) - Target: &gt;85% accuracy, high recall for pneumonia class</p> <p>Key challenges: - Medical images require careful preprocessing - Class imbalance (more pneumonia than normal) - Need to minimize false negatives (missing pneumonia)</p>"},{"location":"Week5_Day21/#track-2-movie-review-sentiment-analysis","title":"Track 2: Movie Review Sentiment Analysis","text":"<p>Problem: Classify movie reviews as Positive or Negative sentiment</p> <p>Dataset: IMDB Movie Reviews Dataset - 50,000 reviews (25k train, 25k test) - 2 classes: positive, negative - Real movie reviews with natural language - Available in PyTorch: <code>torchtext.datasets.IMDB</code></p> <p>Why this project: - Practical NLP application - Balanced dataset - Good for practicing RNNs/LSTMs and embeddings - Real-world text data with slang, sarcasm</p> <p>Recommended approach: - Start with LSTM + pretrained embeddings (GloVe) - Try adding attention mechanism - Experiment with different architectures (bi-directional LSTM) - Target: &gt;85% accuracy on test set</p> <p>Key challenges: - Handling variable-length sequences - Dealing with rare words and vocabulary size - Understanding context and sarcasm</p>"},{"location":"Week5_Day21/#track-3-stock-price-forecasting","title":"Track 3: Stock Price Forecasting","text":"<p>Problem: Predict next-day stock price movement (Up/Down)</p> <p>Dataset: Yahoo Finance data (use yfinance library) - Choose a stock (e.g., AAPL, GOOGL, SPY) - Download 5+ years of historical data - Features: Open, High, Low, Close, Volume - Create technical indicators as features</p> <p>Why this project: - Time series prediction - Financial application - Good for practicing feature engineering - Sequential data with LSTMs</p> <p>Recommended approach: - Engineer features (moving averages, RSI, MACD) - Use LSTM or GRU for sequential patterns - Binary classification: price goes up or down next day - Target: &gt;55% accuracy (beating random is success in finance!)</p> <p>Key challenges: - Financial data is noisy and non-stationary - Need proper train/test split (time-based, no future data in training) - Feature engineering is crucial - Don't expect high accuracy - this is a very hard problem!</p>"},{"location":"Week5_Day21/#your-decision","title":"Your Decision","text":"<p>Choose your track now. Consider: - What interests you most? - Which skills do you want to strengthen? (Vision vs NLP vs Time Series) - Which type of data do you want in your portfolio?</p> <p>Custom project? You can propose your own if you have a dataset and clear problem definition. Check with instructor first.</p>"},{"location":"Week5_Day21/#project-planning-90-min","title":"Project Planning (90 min)","text":"<p>Now that you've chosen, create your project plan. Use a text file or markdown document.</p>"},{"location":"Week5_Day21/#1-project-definition-20-min","title":"1. Project Definition (20 min)","text":"<p>Write down clearly:</p> <pre><code>PROJECT: [Your descriptive title]\n\nPROBLEM: [What are you trying to predict/classify?]\n\nWHY IT MATTERS: [Real-world application]\n\nDATASET: [Name and source]\n- Size: [Number of samples]\n- Classes: [What are you predicting?]\n- Features: [What information do you have?]\n\nSUCCESS CRITERIA:\n- Primary: [e.g., &gt;85% accuracy]\n- Secondary: [e.g., good recall on important class]\n- Deliverables: [Working code, trained model, README, results]\n</code></pre>"},{"location":"Week5_Day21/#2-technical-plan-30-min","title":"2. Technical Plan (30 min)","text":"<p>Outline your technical approach:</p> <pre><code>ARCHITECTURE:\n- Baseline: [Simple model to start]\n- Advanced: [More sophisticated approach]\n- Why: [Reasoning for choices]\n\nDATA PIPELINE:\n- Preprocessing steps: [List transformations needed]\n- Augmentation: [If applicable]\n- Train/val/test split: [Ratios]\n\nEVALUATION:\n- Metrics: [Accuracy, F1, confusion matrix, etc.]\n- Visualizations: [What graphs will you create?]\n\nTOOLS:\n- Framework: PyTorch\n- Libraries: [torchvision, matplotlib, sklearn, etc.]\n- Environment: Google Colab or local\n</code></pre> <p>Guidance by track:</p> <p>Track 1 (Medical Images): - Baseline: Simple CNN (3-4 conv layers) - Advanced: ResNet18 with transfer learning - Preprocessing: Resize to 224x224, normalize, grayscale handling - Augmentation: Random rotation (\u00b115\u00b0), horizontal flip, brightness - Split: Use provided train/test, create validation from training - Metrics: Accuracy, precision, recall (especially for pneumonia), confusion matrix, ROC curve</p> <p>Track 2 (Sentiment): - Baseline: Simple LSTM (1 layer, 128 hidden) - Advanced: Bi-directional LSTM + attention - Preprocessing: Tokenization, vocab building (10k words), padding to max_len=200 - Augmentation: Not typical for text, but can do synonym replacement - Split: Use provided train/test (50% each) - Metrics: Accuracy, F1 score, confusion matrix, example predictions</p> <p>Track 3 (Stock Price): - Baseline: Logistic regression on technical indicators - Advanced: LSTM with multiple features - Preprocessing: Technical indicators (SMA_20, SMA_50, RSI, MACD), normalization - Feature engineering: Create 10-20 technical indicators - Split: 70% train, 15% validation, 15% test (CHRONOLOGICAL ORDER!) - Metrics: Accuracy, precision, recall, profit simulation</p> <p>\u2610 Technical plan documented</p>"},{"location":"Week5_Day21/#3-milestone-timeline-20-min","title":"3. Milestone Timeline (20 min)","text":"<p>Create checkpoints for this week:</p> <pre><code>DAY 21 (Today):\n\u2610 Project chosen and planned\n\u2610 Environment set up\n\u2610 Dataset downloaded and explored\n\u2610 Baseline model architecture defined\n\nDAY 22 (Tuesday):\n\u2610 Data pipeline implemented and tested\n\u2610 Baseline model trained\n\u2610 Initial results documented\n\u2610 Identified what to improve\n\nDAY 23 (Wednesday):\n\u2610 Advanced model implemented\n\u2610 Hyperparameter tuning done\n\u2610 Performance improved over baseline\n\u2610 Mid-week check-in completed\n\nDAY 24 (Thursday):\n\u2610 Model finalized and tested thoroughly\n\u2610 Edge cases handled\n\u2610 All visualizations created\n\u2610 Results analyzed\n\nDAY 25 (Friday):\n\u2610 Code cleaned and commented\n\u2610 README.md written\n\u2610 Presentation prepared\n\u2610 Project complete and ready to demo\n</code></pre> <p>\u2610 Milestone checklist created</p>"},{"location":"Week5_Day21/#environment-setup-60-min","title":"Environment Setup (60 min)","text":"<p>Get your development environment ready.</p>"},{"location":"Week5_Day21/#1-create-project-structure-10-min","title":"1. Create Project Structure (10 min)","text":"<pre><code>my_project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/              # Original dataset\n\u2502   \u2514\u2500\u2500 processed/        # Preprocessed data\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 checkpoints/      # Saved models\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 exploration.ipynb # For EDA\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data.py          # Data loading/preprocessing\n\u2502   \u251c\u2500\u2500 model.py         # Model architecture\n\u2502   \u251c\u2500\u2500 train.py         # Training script\n\u2502   \u2514\u2500\u2500 evaluate.py      # Evaluation script\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 figures/         # Plots and visualizations\n\u2502   \u2514\u2500\u2500 metrics.txt      # Performance metrics\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Create these folders now. Don't worry about filling them yet.</p> <p>\u2610 Project structure created</p>"},{"location":"Week5_Day21/#2-install-dependencies-10-min","title":"2. Install Dependencies (10 min)","text":"<p>Create <code>requirements.txt</code>:</p> <pre><code>torch&gt;=2.0.0\ntorchvision&gt;=0.15.0  # If doing images\nnumpy&gt;=1.24.0\nmatplotlib&gt;=3.7.0\npandas&gt;=2.0.0\nscikit-learn&gt;=1.3.0\ntqdm&gt;=4.65.0\n\n# Track-specific:\n# Track 1: pillow&gt;=10.0.0\n# Track 2: torchtext&gt;=0.15.0 (or use custom tokenization)\n# Track 3: yfinance&gt;=0.2.0, ta&gt;=0.10.0 (for technical indicators)\n</code></pre> <p>Install with: <code>pip install -r requirements.txt</code></p> <p>\u2610 Dependencies installed</p>"},{"location":"Week5_Day21/#3-download-dataset-40-min","title":"3. Download Dataset (40 min)","text":"<p>Track 1 (Medical Images): - Go to Kaggle dataset page - Download chest-xray-pneumonia.zip (~2GB) - Extract to <code>data/raw/</code> - Verify structure: train/NORMAL, train/PNEUMONIA, test/NORMAL, test/PNEUMONIA</p> <p>Track 2 (Sentiment):</p> <pre><code>from torchtext.datasets import IMDB\n# This will download automatically when you use it\ntrain_iter = IMDB(split='train')\n</code></pre> <p>Or manually download from: http://ai.stanford.edu/~amaas/data/sentiment/</p> <p>Track 3 (Stock Price):</p> <pre><code>import yfinance as yf\ndata = yf.download('AAPL', start='2018-01-01', end='2023-12-31')\ndata.to_csv('data/raw/AAPL.csv')\n</code></pre> <p>\u2610 Dataset downloaded and verified</p>"},{"location":"Week5_Day21/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week5_Day21/#data-exploration-2-hours","title":"Data Exploration (2 hours)","text":"<p>Create <code>notebooks/exploration.ipynb</code> and explore your dataset. Answer these questions:</p>"},{"location":"Week5_Day21/#for-all-tracks","title":"For All Tracks:","text":"<ol> <li>Dataset Size</li> <li>How many samples total?</li> <li>How many in each class?</li> <li> <p>Is the dataset balanced?</p> </li> <li> <p>Data Quality</p> </li> <li>Any missing values?</li> <li>Any corrupted files/samples?</li> <li> <p>Data distribution issues?</p> </li> <li> <p>Sample Inspection</p> </li> <li>Look at 10-20 samples manually</li> <li>Do the labels make sense?</li> <li>What patterns do you notice?</li> </ol>"},{"location":"Week5_Day21/#track-specific-questions","title":"Track-Specific Questions:","text":"<p>Track 1 (Images): - What are the image dimensions? - Grayscale or RGB? - What does a normal X-ray look like vs pneumonia? - Are there any artifacts or text in images? - Brightness/contrast variations?</p> <pre><code># Starter code for exploration\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Count samples\nnormal_count = len(os.listdir('data/raw/train/NORMAL'))\npneumonia_count = len(os.listdir('data/raw/train/PNEUMONIA'))\nprint(f\"Normal: {normal_count}, Pneumonia: {pneumonia_count}\")\n\n# Visualize samples\nfig, axes = plt.subplots(2, 4, figsize=(15, 8))\n# Load and show 4 normal, 4 pneumonia\n# ... your code here\n</code></pre> <p>Track 2 (Text): - What's the average review length? - Vocabulary size? - Most common words? - Do reviews have structured format (e.g., star ratings in text)? - Examples of positive vs negative reviews?</p> <pre><code># Starter code\nfrom collections import Counter\n\n# Load some reviews\nreviews = []  # Load your data\nlengths = [len(r.split()) for r in reviews]\n\nprint(f\"Average length: {sum(lengths)/len(lengths):.1f} words\")\nprint(f\"Max length: {max(lengths)} words\")\n\n# Word frequency\nall_words = ' '.join(reviews).lower().split()\nword_freq = Counter(all_words)\nprint(\"Most common words:\", word_freq.most_common(20))\n</code></pre> <p>Track 3 (Time Series): - Date range? - Missing trading days? - Price range and volatility? - Any stock splits or dividends? - Trend direction over time?</p> <pre><code># Starter code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data/raw/AAPL.csv')\nprint(df.info())\nprint(df.describe())\n\n# Visualize price over time\nplt.figure(figsize=(14, 7))\nplt.plot(df['Date'], df['Close'])\nplt.title('Stock Price Over Time')\nplt.show()\n\n# Check for missing days\nprint(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\nprint(f\"Total days: {len(df)}\")\n</code></pre> <p>\u2610 Exploration notebook created with visualizations \u2610 Dataset statistics documented \u2610 Potential issues identified</p>"},{"location":"Week5_Day21/#baseline-model-definition-90-min","title":"Baseline Model Definition (90 min)","text":"<p>Don't code yet! Just plan your baseline model on paper or in comments.</p>"},{"location":"Week5_Day21/#what-is-a-baseline","title":"What is a Baseline?","text":"<p>A baseline is the simplest reasonable model for your problem. It should: - Be quick to implement and train - Give you a sense of task difficulty - Serve as a comparison point for advanced models</p>"},{"location":"Week5_Day21/#define-your-baseline-architecture","title":"Define Your Baseline Architecture","text":"<p>Track 1 (Images) - Simple CNN:</p> <pre><code># Pseudocode - don't implement yet\nclass BaselineCNN:\n    \"\"\"\n    Input: 224x224x3 image\n\n    Conv1: 32 filters, 3x3, ReLU, MaxPool2d\n    Conv2: 64 filters, 3x3, ReLU, MaxPool2d\n    Conv3: 128 filters, 3x3, ReLU, MaxPool2d\n\n    Flatten\n    FC1: 512 units, ReLU, Dropout(0.5)\n    FC2: 2 units (Normal, Pneumonia)\n\n    Output: Logits for 2 classes\n    \"\"\"\n\n# Training plan:\n# - Optimizer: Adam, lr=0.001\n# - Loss: CrossEntropyLoss (with class weights for imbalance)\n# - Epochs: 10-15\n# - Batch size: 32\n# - Expected result: 70-80% accuracy\n</code></pre> <p>Track 2 (Text) - Simple LSTM:</p> <pre><code># Pseudocode\nclass BaselineLSTM:\n    \"\"\"\n    Input: Sequence of word indices [batch, seq_len]\n\n    Embedding: vocab_size -&gt; 128 dimensions\n    LSTM: 128 hidden units, 1 layer\n\n    Take last hidden state\n    FC: 128 -&gt; 2 (positive, negative)\n\n    Output: Logits for 2 classes\n    \"\"\"\n\n# Training plan:\n# - Vocab size: 10,000 most common words\n# - Max sequence length: 200 words\n# - Optimizer: Adam, lr=0.001\n# - Loss: CrossEntropyLoss\n# - Epochs: 5-10\n# - Batch size: 64\n# - Expected result: 80-85% accuracy\n</code></pre> <p>Track 3 (Stock) - Logistic Regression:</p> <pre><code># Pseudocode\nclass BaselineLogistic:\n    \"\"\"\n    Input: Technical indicators [batch, num_features]\n\n    Features (10 indicators):\n    - SMA_5, SMA_20, SMA_50\n    - RSI\n    - MACD, MACD_signal\n    - Volume_ratio\n    - Price_change_1d, Price_change_5d\n    - Volatility\n\n    Linear: num_features -&gt; 2 (up, down)\n\n    Output: Logits for 2 classes\n    \"\"\"\n\n# Training plan:\n# - Create features from OHLCV data\n# - Normalize features\n# - Optimizer: Adam, lr=0.001\n# - Loss: CrossEntropyLoss\n# - Epochs: 50\n# - Batch size: 256\n# - Expected result: 50-55% accuracy (random is 50%!)\n</code></pre> <p>\u2610 Baseline architecture documented \u2610 Training hyperparameters planned \u2610 Expected performance estimated  </p>"},{"location":"Week5_Day21/#daily-reflection-30-min","title":"Daily Reflection (30 min)","text":"<p>Answer these questions in your project notes:</p> <ol> <li> <p>Project choice: Why did you choose this track? What excites you about it?</p> </li> <li> <p>Concerns: What worries you most about this project?</p> </li> <li> <p>Dataset insights: What surprised you about the data?</p> </li> <li> <p>Tomorrow's plan: What are the top 3 tasks for Day 22?</p> </li> <li> <p>Questions: What do you need to clarify or learn more about?</p> </li> </ol>"},{"location":"Week5_Day21/#end-of-day-21-checklist","title":"End of Day 21 Checklist","text":"<p>Before you finish today, verify you have:</p> <p>\u2610 Project track chosen \u2610 Project definition document written \u2610 Technical plan documented \u2610 Milestone timeline created \u2610 Development environment set up \u2610 Dataset downloaded and verified \u2610 Data exploration notebook with visualizations \u2610 Baseline model architecture defined \u2610 Daily reflection completed  </p>"},{"location":"Week5_Day21/#pro-tips","title":"Pro Tips","text":"<p>\ud83c\udfaf Scope management: If your dataset is huge, start with a subset (e.g., 20% of data) to iterate quickly. Scale up once things work.</p> <p>\ud83d\udcca Document everything: Keep notes on what you try and what results you get. Future-you will thank present-you!</p> <p>\ud83d\udc1b Expect bugs: You'll encounter issues. Budget time for debugging. It's part of the process!</p> <p>\ud83d\udca1 Don't compare: Your project doesn't need to beat state-of-the-art. Focus on learning and building something that works.</p> <p>\ud83e\udd1d Collaborate: Share insights with peers working on the same track. Learning together is powerful!</p>"},{"location":"Week5_Day22/","title":"Week 5, Day 22: Data Pipeline &amp; Baseline Model","text":""},{"location":"Week5_Day22/#daily-goals","title":"Daily Goals","text":"<p>Today you'll implement your data loading and preprocessing pipeline, train your baseline model, and get your first results. By end of day, you should have a working system producing metrics!</p>"},{"location":"Week5_Day22/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week5_Day22/#data-pipeline-implementation-25-hours","title":"Data Pipeline Implementation (2.5 hours)","text":"<p>Create <code>src/data.py</code> to handle all data loading and preprocessing.</p>"},{"location":"Week5_Day22/#general-structure-all-tracks","title":"General Structure (All Tracks)","text":"<p>Your data.py should have these components:</p> <pre><code># src/data.py structure\n\"\"\"\nData loading and preprocessing utilities.\n\"\"\"\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyDataset(Dataset):\n    \"\"\"Custom dataset for [your project].\"\"\"\n\n    def __init__(self, data_dir, split='train', transform=None):\n        \"\"\"\n        Args:\n            data_dir: Path to data directory\n            split: 'train', 'val', or 'test'\n            transform: Optional transforms to apply\n        \"\"\"\n        # Load file paths / data\n        # Store labels\n        pass\n\n    def __len__(self):\n        # Return dataset size\n        pass\n\n    def __getitem__(self, idx):\n        # Load and return single sample\n        # Apply transforms\n        # Return (data, label)\n        pass\n\ndef get_transforms(split='train'):\n    \"\"\"Get transforms for train/val/test.\"\"\"\n    pass\n\ndef create_dataloaders(data_dir, batch_size=32):\n    \"\"\"Create train, val, test dataloaders.\"\"\"\n    pass\n</code></pre>"},{"location":"Week5_Day22/#track-1-medical-images-specific-guidance","title":"Track 1 (Medical Images) - Specific Guidance","text":"<p>Key considerations: - Images are different sizes \u2192 resize to 224x224 - Grayscale but models expect 3 channels \u2192 convert or adjust - Class imbalance \u2192 need to handle in sampling or loss - Data augmentation crucial for generalization</p> <p>Implementation steps:</p> <ol> <li>Dataset class (30 min)</li> </ol> <pre><code>from PIL import Image\nfrom torch.utils.data import Dataset\n\nclass ChestXRayDataset(Dataset):\n    def __init__(self, data_dir, split='train', transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        # TODO: Load image paths\n        # Hint: Use os.walk or glob to find all images\n        # data_dir/train/NORMAL/*.jpeg\n        # data_dir/train/PNEUMONIA/*.jpeg\n\n        # TODO: Create labels\n        # NORMAL = 0, PNEUMONIA = 1\n\n        # Store: self.image_paths = [...]\n        #        self.labels = [...]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')  # Convert to RGB\n\n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n\n        label = self.labels[idx]\n        return image, label\n</code></pre> <ol> <li>Transforms (20 min)</li> </ol> <pre><code>from torchvision import transforms\n\ndef get_transforms(split='train'):\n    if split == 'train':\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomRotation(15),  # Augmentation\n            transforms.RandomHorizontalFlip(),  # X-rays can be flipped\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n                               std=[0.229, 0.224, 0.225])\n        ])\n    else:  # val or test\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n</code></pre> <ol> <li>Handle class imbalance (30 min)</li> </ol> <pre><code>from torch.utils.data import WeightedRandomSampler\n\n# Option 1: Weighted sampling\ndef create_balanced_sampler(dataset):\n    # Count samples per class\n    # Calculate weights (inverse of frequency)\n    # Create WeightedRandomSampler\n    # Return sampler\n    pass\n\n# Option 2: Class weights for loss\ndef calculate_class_weights(dataset):\n    # Count samples per class\n    # weights = total / (num_classes * count_per_class)\n    # Return torch.tensor([weight_normal, weight_pneumonia])\n    pass\n</code></pre> <ol> <li>Create DataLoaders (20 min)</li> </ol> <pre><code>def create_dataloaders(data_dir, batch_size=32):\n    # Create train dataset (from train folder)\n    # Split train into train/val (80/20)\n    # Create test dataset (from test folder)\n    # Return train_loader, val_loader, test_loader\n    pass\n</code></pre> <p>Testing your pipeline (30 min):</p> <pre><code># Test script\nif __name__ == \"__main__\":\n    train_loader, val_loader, test_loader = create_dataloaders('data/raw')\n\n    # Check batch\n    images, labels = next(iter(train_loader))\n    print(f\"Batch shape: {images.shape}\")  # Should be [32, 3, 224, 224]\n    print(f\"Labels shape: {labels.shape}\")  # Should be [32]\n    print(f\"Label values: {labels.unique()}\")  # Should be [0, 1]\n\n    # Visualize\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n    for i, ax in enumerate(axes.flat):\n        img = images[i].permute(1, 2, 0)  # CHW -&gt; HWC\n        # Denormalize for visualization\n        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n        ax.imshow(img)\n        ax.set_title(f\"Label: {'PNEUMONIA' if labels[i]==1 else 'NORMAL'}\")\n    plt.show()\n</code></pre> <p>\u2610 Dataset class implemented \u2610 Transforms defined \u2610 Class imbalance handled \u2610 DataLoaders created \u2610 Pipeline tested with visualizations</p>"},{"location":"Week5_Day22/#track-2-sentiment-specific-guidance","title":"Track 2 (Sentiment) - Specific Guidance","text":"<p>Key considerations: - Variable length sequences \u2192 padding needed - Large vocabulary \u2192 limit to common words - Tokenization strategy important - Can use pretrained embeddings later</p> <p>Implementation steps:</p> <ol> <li>Vocabulary building (40 min)</li> </ol> <pre><code>from collections import Counter\n\nclass Vocabulary:\n    def __init__(self, max_vocab_size=10000):\n        self.max_vocab_size = max_vocab_size\n        self.word2idx = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}\n        self.idx2word = {0: '&lt;PAD&gt;', 1: '&lt;UNK&gt;'}\n        self.word_counts = Counter()\n\n    def build_vocab(self, texts):\n        # Tokenize all texts, count words\n        # Keep top max_vocab_size words\n        # Build word2idx and idx2word mappings\n        pass\n\n    def encode(self, text, max_len=200):\n        # Tokenize text\n        # Convert to indices\n        # Pad or truncate to max_len\n        # Return list of indices\n        pass\n</code></pre> <ol> <li>Dataset class (30 min)</li> </ol> <pre><code>class IMDBDataset(Dataset):\n    def __init__(self, data_file, vocab, max_len=200):\n        # Load reviews and labels\n        # data_file should be CSV: text, label\n        self.vocab = vocab\n        self.max_len = max_len\n\n        # TODO: Load data\n        # self.reviews = [...]\n        # self.labels = [...]  # 0 for negative, 1 for positive\n\n    def __getitem__(self, idx):\n        review = self.reviews[idx]\n        label = self.labels[idx]\n\n        # Encode review\n        encoded = self.vocab.encode(review, self.max_len)\n\n        return torch.tensor(encoded), torch.tensor(label)\n</code></pre> <ol> <li>Data loading (30 min)</li> </ol> <pre><code>def load_imdb_data():\n    # Option 1: Use torchtext.datasets.IMDB\n    # Option 2: Download and parse manually\n    # Return train_texts, train_labels, test_texts, test_labels\n    pass\n\ndef create_dataloaders(data_dir, batch_size=64):\n    # Load raw data\n    # Build vocabulary on training data\n    # Create train/val/test datasets\n    # Return dataloaders\n    pass\n</code></pre> <ol> <li>Testing (30 min):</li> </ol> <pre><code>if __name__ == \"__main__\":\n    train_loader, val_loader, test_loader = create_dataloaders('data')\n\n    # Check batch\n    texts, labels = next(iter(train_loader))\n    print(f\"Text shape: {texts.shape}\")  # [batch, max_len]\n    print(f\"Labels: {labels}\")\n\n    # Decode sample\n    vocab = train_loader.dataset.vocab\n    sample_text = texts[0]\n    decoded = ' '.join([vocab.idx2word[idx.item()] for idx in sample_text if idx != 0])\n    print(f\"Sample review: {decoded[:200]}...\")\n    print(f\"Label: {'Positive' if labels[0]==1 else 'Negative'}\")\n</code></pre> <p>\u2610 Vocabulary class implemented \u2610 Tokenization working \u2610 Dataset class created \u2610 DataLoaders working \u2610 Sample reviews decoded successfully  </p>"},{"location":"Week5_Day22/#track-3-stock-price-specific-guidance","title":"Track 3 (Stock Price) - Specific Guidance","text":"<p>Key considerations: - Time-based split (NEVER shuffle time series!) - Feature engineering is crucial - Need to create technical indicators - Normalization per feature</p> <p>Implementation steps:</p> <ol> <li>Feature engineering (50 min)</li> </ol> <pre><code>import pandas as pd\n\ndef create_technical_indicators(df):\n    \"\"\"Add technical indicators to dataframe.\"\"\"\n    df = df.copy()\n\n    # TODO: Implement these indicators\n    # Simple Moving Averages\n    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n\n    # Relative Strength Index (RSI)\n    # MACD\n    # Bollinger Bands\n    # Volume indicators\n    # etc.\n\n    # Drop NaN rows (from rolling calculations)\n    df = df.dropna()\n\n    return df\n\ndef create_labels(df):\n    \"\"\"Create binary labels: 1 if price goes up next day, 0 otherwise.\"\"\"\n    # Compare tomorrow's close to today's close\n    df['Tomorrow_Close'] = df['Close'].shift(-1)\n    df['Label'] = (df['Tomorrow_Close'] &gt; df['Close']).astype(int)\n    df = df.dropna()\n    return df\n</code></pre> <ol> <li>Dataset class (30 min)</li> </ol> <pre><code>class StockDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = torch.FloatTensor(features)\n        self.labels = torch.LongTensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n</code></pre> <ol> <li>Train/Val/Test split (40 min)</li> </ol> <pre><code>def create_dataloaders(csv_path, batch_size=256):\n    # Load CSV\n    df = pd.read_csv(csv_path)\n    df = df.sort_values('Date')  # Ensure chronological order\n\n    # Create features\n    df = create_technical_indicators(df)\n    df = create_labels(df)\n\n    # Select feature columns (drop Date, OHLCV, etc.)\n    feature_cols = ['SMA_5', 'SMA_20', 'SMA_50', 'RSI', ...]  # Your indicators\n    features = df[feature_cols].values\n    labels = df['Label'].values\n\n    # Normalize features (fit on train only!)\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n\n    # CRITICAL: Time-based split (no shuffle!)\n    n = len(features)\n    train_end = int(0.7 * n)\n    val_end = int(0.85 * n)\n\n    X_train, y_train = features[:train_end], labels[:train_end]\n    X_val, y_val = features[train_end:val_end], labels[train_end:val_end]\n    X_test, y_test = features[val_end:], labels[val_end:]\n\n    # Fit scaler on train only\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n    # Create datasets and loaders\n    # Return train_loader, val_loader, test_loader, scaler\n    pass\n</code></pre> <p>\u2610 Technical indicators implemented \u2610 Labels created correctly \u2610 Time-based split implemented \u2610 Normalization applied correctly \u2610 DataLoaders created</p>"},{"location":"Week5_Day22/#baseline-model-implementation-15-hours","title":"Baseline Model Implementation (1.5 hours)","text":"<p>Create <code>src/model.py</code> with your baseline architecture.</p>"},{"location":"Week5_Day22/#track-1-images-simple-cnn","title":"Track 1 (Images) - Simple CNN","text":"<pre><code># src/model.py\nimport torch.nn as nn\n\nclass BaselineCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        # TODO: Implement architecture from Day 21 plan\n        # 3 conv layers, flatten, 2 FC layers\n        pass\n\n    def forward(self, x):\n        # TODO: Forward pass\n        pass\n\n# Test your model\nif __name__ == \"__main__\":\n    model = BaselineCNN()\n    x = torch.randn(4, 3, 224, 224)  # Batch of 4 images\n    output = model(x)\n    print(f\"Output shape: {output.shape}\")  # Should be [4, 2]\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre> <p>Implementation hints: - Use <code>nn.Conv2d</code>, <code>nn.MaxPool2d</code>, <code>nn.ReLU</code>, <code>nn.Linear</code> - Don't forget <code>nn.Flatten()</code> before FC layers - Add <code>nn.Dropout(0.5)</code> before final layer - Calculate dimensions carefully after pooling</p> <p>\u2610 CNN architecture implemented \u2610 Forward pass working \u2610 Output shape correct</p>"},{"location":"Week5_Day22/#track-2-text-simple-lstm","title":"Track 2 (Text) - Simple LSTM","text":"<pre><code>class BaselineLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=2):\n        super().__init__()\n        # TODO: Implement architecture\n        # Embedding -&gt; LSTM -&gt; take last hidden -&gt; FC\n        pass\n\n    def forward(self, x):\n        # x: [batch, seq_len]\n        # TODO: Implement forward\n        # Return logits: [batch, num_classes]\n        pass\n</code></pre> <p>Implementation hints: - <code>nn.Embedding(vocab_size, embedding_dim)</code> - <code>nn.LSTM(embedding_dim, hidden_dim, batch_first=True)</code> - Take <code>hidden[-1]</code> from LSTM output (last hidden state) - <code>nn.Linear(hidden_dim, num_classes)</code></p> <p>\u2610 LSTM architecture implemented \u2610 Embedding layer working \u2610 Last hidden state extracted correctly</p>"},{"location":"Week5_Day22/#track-3-stock-logistic-regression","title":"Track 3 (Stock) - Logistic Regression","text":"<pre><code>class BaselineLogistic(nn.Module):\n    def __init__(self, num_features, num_classes=2):\n        super().__init__()\n        self.linear = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        return self.linear(x)\n</code></pre> <p>This is simple! Logistic regression is just one linear layer.</p> <p>\u2610 Model implemented \u2610 Forward pass tested</p>"},{"location":"Week5_Day22/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week5_Day22/#training-script-15-hours","title":"Training Script (1.5 hours)","text":"<p>Create <code>src/train.py</code> - this is your main training loop.</p>"},{"location":"Week5_Day22/#general-training-structure-all-tracks","title":"General Training Structure (All Tracks)","text":"<pre><code># src/train.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, labels) in enumerate(tqdm(dataloader)):\n        data, labels = data.to(device), labels.to(device)\n\n        # Forward\n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, labels)\n\n        # Backward\n        loss.backward()\n        optimizer.step()\n\n        # Metrics\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct / total\n    return avg_loss, accuracy\n\ndef validate(model, dataloader, criterion, device):\n    \"\"\"Validate model.\"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, labels in dataloader:\n            data, labels = data.to(device), labels.to(device)\n\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct / total\n    return avg_loss, accuracy\n\ndef train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, device='cpu'):\n    \"\"\"Main training loop.\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    best_val_acc = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n        # Train\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n\n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        # Save best model\n        if val_acc &gt; best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'models/checkpoints/best_model.pth')\n            print(f\"\u2713 Saved best model (val_acc: {val_acc:.4f})\")\n\n    return history\n</code></pre>"},{"location":"Week5_Day22/#track-specific-adjustments","title":"Track-Specific Adjustments:","text":"<p>Track 1 (Images): - Use weighted loss if class imbalanced:</p> <pre><code># Calculate class weights\nclass_weights = torch.tensor([weight_normal, weight_pneumonia])\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n</code></pre> <ul> <li>Start with 10-15 epochs</li> <li>Use lr=0.001</li> </ul> <p>Track 2 (Text): - 5-10 epochs usually enough - May need gradient clipping:</p> <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n</code></pre> <p>Track 3 (Stock): - 50-100 epochs (simpler model) - lr=0.001 or 0.01 - Don't expect high accuracy! 52-55% is good.</p> <p>\u2610 Training script implemented \u2610 Validation logic working \u2610 Best model saving implemented</p>"},{"location":"Week5_Day22/#train-baseline-model-15-hours","title":"Train Baseline Model (1.5 hours)","text":"<p>Now run your training! Create a script or notebook:</p> <pre><code># train_baseline.py or notebook\nimport torch\nfrom src.data import create_dataloaders\nfrom src.model import BaselineModel  # Your model class\nfrom src.train import train_model\n\n# Setup\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Load data\ntrain_loader, val_loader, test_loader = create_dataloaders(\n    data_dir='data/raw',\n    batch_size=32  # Adjust per track\n)\n\n# Create model\nmodel = BaselineModel(...)  # Your specific parameters\nmodel = model.to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train\nhistory = train_model(\n    model, \n    train_loader, \n    val_loader,\n    num_epochs=10,  # Adjust per track\n    lr=0.001,\n    device=device\n)\n\nprint(\"\\n\u2713 Training complete!\")\nprint(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n</code></pre> <p>While training: - Monitor loss (should decrease) - Watch for overfitting (train acc &gt;&gt; val acc) - Check GPU utilization if using CUDA - Expect: 30 min - 2 hours depending on track and hardware</p> <p>\u2610 Baseline model training started \u2610 Training completing without errors \u2610 Loss decreasing over epochs \u2610 Validation accuracy reasonable</p>"},{"location":"Week5_Day22/#analyze-initial-results-1-hour","title":"Analyze Initial Results (1 hour)","text":"<p>Create <code>notebooks/results_analysis.ipynb</code>:</p>"},{"location":"Week5_Day22/#1-plot-training-curves-15-min","title":"1. Plot Training Curves (15 min)","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Assuming you saved history\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss Curves')\n\nplt.subplot(1, 2, 2)\nplt.plot(history['train_acc'], label='Train Acc')\nplt.plot(history['val_acc'], label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy Curves')\n\nplt.tight_layout()\nplt.savefig('results/figures/baseline_training_curves.png')\nplt.show()\n</code></pre>"},{"location":"Week5_Day22/#2-evaluate-on-test-set-20-min","title":"2. Evaluate on Test Set (20 min)","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Load best model\nmodel.load_state_dict(torch.load('models/checkpoints/best_model.pth'))\nmodel.eval()\n\n# Get predictions\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for data, labels in test_loader:\n        data = data.to(device)\n        outputs = model(data)\n        _, predicted = outputs.max(1)\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Metrics\nprint(classification_report(all_labels, all_preds, \n                           target_names=['Class 0', 'Class 1']))\n\n# Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('results/figures/baseline_confusion_matrix.png')\nplt.show()\n</code></pre>"},{"location":"Week5_Day22/#3-document-results-25-min","title":"3. Document Results (25 min)","text":"<p>Create <code>results/baseline_results.txt</code>:</p> <pre><code>BASELINE MODEL RESULTS\n======================\n\nArchitecture: [Your model type]\nParameters: [Number]\nTraining time: [X minutes]\n\nHyperparameters:\n- Epochs: [N]\n- Batch size: [N]\n- Learning rate: [0.00X]\n- Optimizer: Adam\n\nPerformance:\n- Train Accuracy: [XX.X%]\n- Validation Accuracy: [XX.X%]\n- Test Accuracy: [XX.X%]\n\nPer-class metrics:\n[Paste classification report]\n\nObservations:\n- [What worked well?]\n- [What issues did you notice?]\n- [Is there overfitting/underfitting?]\n\nNext steps for improvement:\n1. [Idea 1]\n2. [Idea 2]\n3. [Idea 3]\n</code></pre> <p>\u2610 Training curves plotted \u2610 Test set evaluated \u2610 Confusion matrix created \u2610 Results documented</p>"},{"location":"Week5_Day22/#daily-reflection-30-min","title":"Daily Reflection (30 min)","text":"<p>Answer these questions:</p> <ol> <li> <p>Baseline performance: How did your baseline do? Better or worse than expected?</p> </li> <li> <p>Challenges: What was hardest about today? Data pipeline or model training?</p> </li> <li> <p>Debugging: What bugs did you encounter? How did you fix them?</p> </li> <li> <p>Observations: What did the training curves tell you? Overfitting? Underfitting?</p> </li> <li> <p>Tomorrow's priorities: What will you try tomorrow to improve performance?</p> </li> </ol>"},{"location":"Week5_Day22/#end-of-day-22-checklist","title":"End of Day 22 Checklist","text":"<p>\u2610 Data pipeline implemented and tested \u2610 DataLoaders working correctly \u2610 Baseline model implemented \u2610 Training script working \u2610 Baseline model trained successfully \u2610 Results analyzed and documented \u2610 Training curves look reasonable \u2610 Test accuracy documented \u2610 Ideas for improvement identified  </p> <p>Expected Results by Track: - Track 1: 70-85% accuracy (imbalanced dataset) - Track 2: 80-85% accuracy (balanced dataset) - Track 3: 50-55% accuracy (very hard problem!)</p> <p>If below expectations: That's okay! Tomorrow you'll improve it.</p> <p>If above expectations: Great! Tomorrow you'll make it even better.</p> <p>Tomorrow (Day 23): Model iteration, experimentation, and improvement!</p>"},{"location":"Week5_Day22/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>Issue: Out of memory error - Solution: Reduce batch size, use smaller images, or use gradient accumulation</p> <p>Issue: Loss not decreasing - Solution: Check learning rate (try 0.1x or 10x), verify labels are correct, check data normalization</p> <p>Issue: Training very slow - Solution: Use GPU, reduce data size for testing, use smaller model first</p> <p>Issue: Accuracy stuck around 50% (binary classification) - Solution: Model might be predicting one class. Check class balance, try weighted loss</p> <p>Issue: Perfect training accuracy but poor validation - Solution: Overfitting! Add regularization (dropout, weight decay), more data augmentation</p>"},{"location":"Week5_Day23/","title":"Week 5, Day 23: Model Development &amp; Iteration","text":""},{"location":"Week5_Day23/#daily-goals","title":"Daily Goals","text":"<p>Improve your baseline model through experimentation, hyperparameter tuning, and advanced techniques. This is where you iterate and optimize!</p>"},{"location":"Week5_Day23/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week5_Day23/#advanced-model-implementation-25-hours","title":"Advanced Model Implementation (2.5 hours)","text":"<p>Time to upgrade from your baseline! Implement the advanced model you planned on Day 21.</p>"},{"location":"Week5_Day23/#track-1-images-transfer-learning-with-resnet18","title":"Track 1 (Images) - Transfer Learning with ResNet18","text":"<p>Why transfer learning? - Pretrained on ImageNet (1M+ images) - Already learned edge, texture, shape detectors - Just fine-tune for your specific task</p> <p>Implementation approach:</p> <pre><code># src/model.py\nimport torch.nn as nn\nfrom torchvision import models\n\nclass TransferCNN(nn.Module):\n    def __init__(self, num_classes=2, pretrained=True):\n        super().__init__()\n        # Load pretrained ResNet18\n        self.model = models.resnet18(pretrained=pretrained)\n\n        # Option 1: Freeze early layers (recommended for small datasets)\n        for param in list(self.model.parameters())[:-10]:\n            param.requires_grad = False\n\n        # Replace final layer\n        num_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre> <p>Training tips: - Use lower learning rate (0.0001) since pretrained - May converge faster (5-10 epochs) - Expected improvement: +5-10% accuracy</p> <p>\u2610 Transfer learning model implemented \u2610 Early layers frozen appropriately \u2610 Model training started</p>"},{"location":"Week5_Day23/#track-2-text-bi-directional-lstm-attention","title":"Track 2 (Text) - Bi-directional LSTM + Attention","text":"<p>Why bidirectional + attention? - Bi-directional captures context from both directions - Attention focuses on important words - State-of-the-art for sentiment (pre-Transformer era)</p> <p>Implementation approach:</p> <pre><code># Simple attention mechanism\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        # lstm_output: [batch, seq_len, hidden]\n        # Compute attention weights\n        scores = self.attention(lstm_output)  # [batch, seq_len, 1]\n        weights = torch.softmax(scores, dim=1)  # [batch, seq_len, 1]\n\n        # Weighted sum\n        context = torch.sum(weights * lstm_output, dim=1)  # [batch, hidden]\n        return context, weights\n\nclass AdvancedLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n                           bidirectional=True, batch_first=True)\n        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)  # [batch, seq, emb_dim]\n\n        # LSTM\n        lstm_out, _ = self.lstm(embedded)  # [batch, seq, hidden*2]\n\n        # Attention\n        context, attn_weights = self.attention(lstm_out)\n\n        # Classify\n        output = self.dropout(context)\n        output = self.fc(output)\n        return output\n</code></pre> <p>Training tips: - May need gradient clipping: <code>torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)</code> - Try different hidden dimensions (128, 256) - Expected improvement: +2-5% accuracy</p> <p>\u2610 Bi-directional LSTM implemented \u2610 Attention mechanism added \u2610 Model training started</p>"},{"location":"Week5_Day23/#track-3-stock-lstm-for-sequences","title":"Track 3 (Stock) - LSTM for Sequences","text":"<p>Why LSTM for stock data? - Captures temporal dependencies - Remembers long-term patterns - Better than simple features</p> <p>Implementation approach:</p> <pre><code>class StockLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_layers=2, num_classes=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n                           batch_first=True, dropout=0.2)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        # x: [batch, seq_len, features]\n        lstm_out, (hidden, _) = self.lstm(x)\n        # Take last hidden state\n        output = self.fc(hidden[-1])\n        return output\n</code></pre> <p>Data preparation changes: - Need sequences! Create sliding windows:</p> <pre><code>def create_sequences(features, labels, seq_length=10):\n    \"\"\"Create sequences of seq_length days.\"\"\"\n    X, y = [], []\n    for i in range(len(features) - seq_length):\n        X.append(features[i:i+seq_length])\n        y.append(labels[i+seq_length])\n    return np.array(X), np.array(y)\n</code></pre> <p>Training tips: - Sequence length: 5-20 days - Hidden dim: 64-128 - Be patient: improvement might be small (1-3%)</p> <p>\u2610 LSTM implemented \u2610 Sequence data prepared \u2610 Model training started</p>"},{"location":"Week5_Day23/#hyperparameter-tuning-15-hours","title":"Hyperparameter Tuning (1.5 hours)","text":"<p>Don't just use default values! Experiment systematically.</p> <p>Key hyperparameters to tune:</p> <ol> <li>Learning rate: Most important!</li> <li>Try: [0.0001, 0.001, 0.01]</li> <li> <p>Use learning rate scheduler</p> </li> <li> <p>Batch size: Affects training dynamics</p> </li> <li>Try: [16, 32, 64, 128]</li> <li> <p>Larger = faster but less stochastic</p> </li> <li> <p>Model capacity: Width and depth</p> </li> <li>Hidden dimensions: [64, 128, 256]</li> <li> <p>Number of layers: [1, 2, 3]</p> </li> <li> <p>Regularization: Prevents overfitting</p> </li> <li>Dropout: [0.2, 0.3, 0.5]</li> <li>Weight decay: [0, 1e-5, 1e-4]</li> </ol> <p>Systematic approach:</p> <pre><code># Create a simple grid search\nconfigs = [\n    {'lr': 0.001, 'batch_size': 32, 'hidden_dim': 128},\n    {'lr': 0.0001, 'batch_size': 64, 'hidden_dim': 256},\n    # ... more configs\n]\n\nresults = []\nfor config in configs:\n    print(f\"\\nTrying config: {config}\")\n    # Train model with this config\n    # Record best validation accuracy\n    results.append({'config': config, 'val_acc': val_acc})\n\n# Find best\nbest = max(results, key=lambda x: x['val_acc'])\nprint(f\"\\nBest config: {best['config']}\")\nprint(f\"Best val acc: {best['val_acc']:.4f}\")\n</code></pre> <p>\u2610 Learning rates experimented \u2610 Batch sizes tested \u2610 Model capacity varied \u2610 Best config identified</p>"},{"location":"Week5_Day23/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week5_Day23/#mid-week-check-in-optional-30-min","title":"Mid-Week Check-in (Optional, 30 min)","text":"<p>If scheduled, present to peers: - Current progress - Baseline vs advanced results - Biggest challenges - Plans for tomorrow</p>"},{"location":"Week5_Day23/#more-experimentation-25-hours","title":"More Experimentation (2.5 hours)","text":"<p>Try additional techniques based on your track:</p>"},{"location":"Week5_Day23/#all-tracks-regularization-techniques","title":"All Tracks: Regularization Techniques","text":"<ol> <li>Dropout (if not already using)</li> <li>Weight decay in optimizer:</li> </ol> <pre><code>optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n</code></pre> <ol> <li>Early stopping:</li> </ol> <pre><code>patience = 3\nbest_val_acc = 0\nepochs_without_improvement = 0\n\nfor epoch in range(max_epochs):\n    # ... training ...\n\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        epochs_without_improvement = 0\n        # Save model\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement &gt;= patience:\n            print(\"Early stopping!\")\n            break\n</code></pre>"},{"location":"Week5_Day23/#track-1-specific-data-augmentation-experiments","title":"Track 1 Specific: Data Augmentation Experiments","text":"<p>Try more aggressive augmentation: - ColorJitter with higher values - RandomAffine (scaling, rotation) - RandomErasing (cutout)</p>"},{"location":"Week5_Day23/#track-2-specific-pretrained-embeddings","title":"Track 2 Specific: Pretrained Embeddings","text":"<p>Use GloVe embeddings:</p> <pre><code># Load GloVe (can download from: https://nlp.stanford.edu/projects/glove/)\ndef load_glove_embeddings(glove_file, vocab):\n    embeddings = np.random.randn(len(vocab), 300)  # 300d GloVe\n    # ... load from file and fill embeddings matrix ...\n    return torch.FloatTensor(embeddings)\n\n# Use in model\nself.embedding = nn.Embedding.from_pretrained(glove_embeddings, freeze=False)\n</code></pre>"},{"location":"Week5_Day23/#track-3-specific-feature-engineering","title":"Track 3 Specific: Feature Engineering","text":"<p>Add more technical indicators: - Stochastic Oscillator - On-Balance Volume (OBV) - Average True Range (ATR) - Price Rate of Change</p> <p>\u2610 Regularization applied \u2610 Track-specific techniques tried \u2610 Improvements measured</p>"},{"location":"Week5_Day23/#results-comparison-analysis-1-hour","title":"Results Comparison &amp; Analysis (1 hour)","text":"<p>Create comparison table in <code>results/model_comparison.txt</code>:</p> <pre><code>MODEL COMPARISON\n================\n\nBaseline Model:\n- Architecture: [Simple CNN / Simple LSTM / Logistic]\n- Parameters: [N]\n- Train Acc: [XX.X%]\n- Val Acc: [XX.X%]\n- Test Acc: [XX.X%]\n\nAdvanced Model (Config 1):\n- Architecture: [ResNet18 / BiLSTM+Attention / LSTM]\n- Parameters: [N]\n- Train Acc: [XX.X%]\n- Val Acc: [XX.X%]\n- Test Acc: [XX.X%]\n- Improvement: +[X.X%]\n\nAdvanced Model (Config 2):\n...\n\nBest Model:\n- Configuration: [Details]\n- Test Accuracy: [XX.X%]\n- Total improvement over baseline: +[X.X%]\n\nKey insights:\n1. [What helped most?]\n2. [What didn't help?]\n3. [Any surprising findings?]\n</code></pre> <p>Create comparison visualizations:</p> <pre><code># Plot all models\nmodels = ['Baseline', 'Transfer', 'Transfer+Aug', 'Transfer+Aug+LR']\naccuracies = [0.82, 0.87, 0.89, 0.91]  # Your actual values\n\nplt.figure(figsize=(10, 6))\nplt.bar(models, accuracies)\nplt.ylabel('Test Accuracy')\nplt.title('Model Comparison')\nplt.ylim([0.5, 1.0])\nplt.axhline(y=0.85, color='r', linestyle='--', label='Target (85%)')\nplt.legend()\nplt.savefig('results/figures/model_comparison.png')\nplt.show()\n</code></pre> <p>\u2610 All models compared \u2610 Best model identified \u2610 Insights documented \u2610 Comparison visualization created</p>"},{"location":"Week5_Day23/#daily-reflection-30-min","title":"Daily Reflection (30 min)","text":"<ol> <li> <p>Progress: How much did you improve over baseline?</p> </li> <li> <p>What worked: Which technique gave the biggest boost?</p> </li> <li> <p>What didn't work: Any experiments that failed? Why?</p> </li> <li> <p>Learning: What did you learn about your problem from experimentation?</p> </li> <li> <p>Tomorrow's plan: Model is good enough? Or more tuning needed?</p> </li> </ol> <p>\u2610 Reflection completed</p>"},{"location":"Week5_Day23/#end-of-day-23-checklist","title":"End of Day 23 Checklist","text":"<p>\u2610 Advanced model implemented and trained \u2610 Hyperparameter tuning performed \u2610 Multiple configurations tested \u2610 Best model identified \u2610 Results improved over baseline \u2610 Comprehensive comparison documented \u2610 Ready for testing and refinement tomorrow</p> <p>Expected by End of Day: - Track 1: 85-90% test accuracy - Track 2: 85-88% test accuracy - Track 3: 52-56% test accuracy</p> <p>Tomorrow (Day 24): Final testing, edge case handling, and optimization!</p>"},{"location":"Week5_Day24/","title":"Week 5, Day 24: Testing &amp; Refinement","text":""},{"location":"Week5_Day24/#daily-goals","title":"Daily Goals","text":"<p>Thoroughly test your model, handle edge cases, analyze errors, and make final optimizations. Today is about robustness and polish!</p>"},{"location":"Week5_Day24/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week5_Day24/#comprehensive-evaluation-2-hours","title":"Comprehensive Evaluation (2 hours)","text":"<p>Go beyond simple accuracy. Understand where your model succeeds and fails.</p>"},{"location":"Week5_Day24/#1-detailed-metrics-45-min","title":"1. Detailed Metrics (45 min)","text":"<p>Create <code>src/evaluate.py</code> with comprehensive evaluation:</p> <pre><code>from sklearn.metrics import (classification_report, confusion_matrix, \n                            roc_curve, auc, precision_recall_curve)\nimport numpy as np\n\ndef comprehensive_evaluation(model, test_loader, device, class_names):\n    \"\"\"Perform thorough evaluation.\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for data, labels in test_loader:\n            data = data.to(device)\n            outputs = model(data)\n            probs = torch.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    # Convert to numpy\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n\n    # Metrics\n    print(\"=\"*60)\n    print(\"CLASSIFICATION REPORT\")\n    print(\"=\"*60)\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n\n    # Per-class accuracy\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    for i, acc in enumerate(per_class_acc):\n        print(f\"{class_names[i]} Accuracy: {acc:.4f}\")\n\n    return {\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs,\n        'confusion_matrix': cm\n    }\n</code></pre> <p>Track-specific considerations: - Track 1: Focus on recall for pneumonia (minimize false negatives) - Track 2: Check F1-score (balanced precision/recall) - Track 3: Check precision (avoid false buy signals)</p> <p>\u2610 Comprehensive metrics calculated \u2610 Per-class performance analyzed \u2610 Confusion matrix examined</p>"},{"location":"Week5_Day24/#2-error-analysis-75-min","title":"2. Error Analysis (75 min)","text":"<p>Find and understand your model's mistakes:</p> <pre><code>def analyze_errors(model, test_loader, device, num_errors=20):\n    \"\"\"Find and visualize misclassified examples.\"\"\"\n    model.eval()\n    errors = []\n\n    with torch.no_grad():\n        for data, labels in test_loader:\n            data_device = data.to(device)\n            outputs = model(data_device)\n            probs = torch.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n\n            # Find errors\n            incorrect = predicted.cpu() != labels\n            for i in range(len(incorrect)):\n                if incorrect[i]:\n                    errors.append({\n                        'data': data[i],\n                        'true_label': labels[i].item(),\n                        'pred_label': predicted[i].item(),\n                        'confidence': probs[i].max().item(),\n                        'true_prob': probs[i, labels[i]].item()\n                    })\n\n                    if len(errors) &gt;= num_errors:\n                        return errors\n\n    return errors\n</code></pre> <p>Analyze patterns in errors: - Are errors random or systematic? - High confidence mistakes (model is wrong but sure)? - Borderline cases (model is uncertain)? - Any class-specific patterns?</p> <p>Track 1 (Images): Visualize misclassified X-rays - Are they actually ambiguous to humans? - Image quality issues? - Rare medical conditions?</p> <p>Track 2 (Text): Look at misclassified reviews - Sarcasm model didn't catch? - Mixed sentiment reviews? - Short reviews vs long?</p> <p>Track 3 (Stock): Analyze wrong predictions - During high volatility periods? - Around major news events? - Trend changes?</p> <p>\u2610 Errors collected and analyzed \u2610 Patterns identified \u2610 Examples visualized</p>"},{"location":"Week5_Day24/#edge-case-testing-1-hour","title":"Edge Case Testing (1 hour)","text":"<p>Test your model on challenging cases:</p>"},{"location":"Week5_Day24/#all-tracks-robustness-tests","title":"All Tracks: Robustness Tests","text":"<ol> <li>Confidence calibration: Are confident predictions actually more accurate?</li> </ol> <pre><code># Group by confidence\nlow_conf = probs &lt; 0.6\nmid_conf = (probs &gt;= 0.6) &amp; (probs &lt; 0.8)\nhigh_conf = probs &gt;= 0.8\n\n# Check accuracy in each group\n</code></pre> <ol> <li> <p>Class balance sensitivity: How does model do on minority class?</p> </li> <li> <p>Outlier detection: Can model identify \"weird\" inputs?</p> </li> </ol>"},{"location":"Week5_Day24/#track-specific-edge-cases","title":"Track-Specific Edge Cases","text":"<p>Track 1:  - Very low quality images - Images with artifacts or text - Edge of class boundary (mild pneumonia)</p> <p>Track 2: - Very short reviews (&lt; 10 words) - Very long reviews (&gt; 500 words) - Reviews with mixed sentiment</p> <p>Track 3: - High volatility periods - Market crashes - Low volume days</p> <p>\u2610 Edge cases tested \u2610 Model behavior documented \u2610 Failure modes identified</p>"},{"location":"Week5_Day24/#model-optimization-1-hour","title":"Model Optimization (1 hour)","text":"<p>Final tweaks based on evaluation:</p>"},{"location":"Week5_Day24/#option-1-ensemble-models-if-time-permits","title":"Option 1: Ensemble Models (if time permits)","text":"<p>Train 2-3 models and average predictions:</p> <pre><code>models = [model1, model2, model3]\nensemble_probs = np.mean([get_probs(m, data) for m in models], axis=0)\n</code></pre> <p>Typically gives +1-2% accuracy boost.</p>"},{"location":"Week5_Day24/#option-2-threshold-tuning","title":"Option 2: Threshold Tuning","text":"<p>For imbalanced problems, tune classification threshold:</p> <pre><code># Find optimal threshold\nfrom sklearn.metrics import f1_score\n\nthresholds = np.arange(0.3, 0.7, 0.05)\nbest_threshold = 0.5\nbest_f1 = 0\n\nfor threshold in thresholds:\n    preds = (probs[:, 1] &gt; threshold).astype(int)\n    f1 = f1_score(labels, preds)\n    if f1 &gt; best_f1:\n        best_f1 = f1\n        best_threshold = threshold\n</code></pre>"},{"location":"Week5_Day24/#option-3-post-processing","title":"Option 3: Post-processing","text":"<p>Add rules based on domain knowledge: - Track 1: If very uncertain, flag for human review - Track 2: Boost sentiment for words like \"amazing\", \"terrible\" - Track 3: Don't trade on low volume days</p> <p>\u2610 Optimization attempted \u2610 Final model performance measured \u2610 Improvement documented</p>"},{"location":"Week5_Day24/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week5_Day24/#final-model-training-15-hours","title":"Final Model Training (1.5 hours)","text":"<p>Train your final model with best configuration on ALL available data:</p> <pre><code># Combine train + validation for final training\nfinal_train_data = train_data + val_data\n\n# Train from scratch with best config\nfinal_model = BestModel(**best_config)\ntrain_model(final_model, final_train_loader, num_epochs=best_epochs)\n\n# Evaluate on test set ONCE\nfinal_test_accuracy = evaluate(final_model, test_loader)\n\n# Save final model\ntorch.save(final_model.state_dict(), 'models/final_model.pth')\n</code></pre> <p>\u2610 Final model trained \u2610 Test set evaluated (ONCE only!) \u2610 Final model saved</p>"},{"location":"Week5_Day24/#create-final-visualizations-15-hours","title":"Create Final Visualizations (1.5 hours)","text":"<p>Create a comprehensive results document with visualizations:</p>"},{"location":"Week5_Day24/#1-performance-summary-figure","title":"1. Performance Summary Figure","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Training curves\naxes[0, 0].plot(history['train_loss'], label='Train')\naxes[0, 0].plot(history['val_loss'], label='Val')\naxes[0, 0].set_title('Loss Curves')\naxes[0, 0].legend()\n\n# 2. Accuracy curves\naxes[0, 1].plot(history['train_acc'], label='Train')\naxes[0, 1].plot(history['val_acc'], label='Val')\naxes[0, 1].set_title('Accuracy Curves')\naxes[0, 1].legend()\n\n# 3. Confusion matrix\nimport seaborn as sns\nsns.heatmap(confusion_matrix, annot=True, fmt='d', ax=axes[1, 0])\naxes[1, 0].set_title('Confusion Matrix')\n\n# 4. Model comparison\nmodels = ['Baseline', 'Advanced', 'Optimized', 'Final']\naccs = [baseline_acc, advanced_acc, optimized_acc, final_acc]\naxes[1, 1].bar(models, accs)\naxes[1, 1].set_title('Model Evolution')\naxes[1, 1].set_ylim([0.5, 1.0])\n\nplt.suptitle('Final Project Results', fontsize=16)\nplt.tight_layout()\nplt.savefig('results/figures/final_results.png', dpi=300)\n</code></pre>"},{"location":"Week5_Day24/#2-additional-visualizations","title":"2. Additional Visualizations","text":"<ul> <li>ROC curve (if binary classification)</li> <li>Precision-recall curve</li> <li>Per-class performance bar chart</li> <li>Sample predictions (correct and incorrect)</li> </ul> <p>\u2610 All visualizations created \u2610 High-quality figures saved \u2610 Results presentation-ready</p>"},{"location":"Week5_Day24/#code-cleanup-1-hour","title":"Code Cleanup (1 hour)","text":"<p>Make your code professional:</p>"},{"location":"Week5_Day24/#1-code-organization","title":"1. Code Organization","text":"<p>Ensure clean structure:</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data.py          # \u2713 Clean, commented\n\u2502   \u251c\u2500\u2500 model.py         # \u2713 Clear class definitions\n\u2502   \u251c\u2500\u2500 train.py         # \u2713 Reusable training functions\n\u2502   \u2514\u2500\u2500 evaluate.py      # \u2713 Comprehensive evaluation\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 exploration.ipynb\n\u2502   \u2514\u2500\u2500 results_analysis.ipynb\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 figures/         # All visualizations\n\u2502   \u2514\u2500\u2500 metrics.txt      # Numerical results\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 final_model.pth\n\u2514\u2500\u2500 README.md            # TODO: Tomorrow\n</code></pre>"},{"location":"Week5_Day24/#2-add-docstrings","title":"2. Add Docstrings","text":"<pre><code>def train_model(model, train_loader, val_loader, ...):\n    \"\"\"\n    Train a classification model.\n\n    Args:\n        model: PyTorch model to train\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        ...\n\n    Returns:\n        dict: Training history with keys 'train_loss', 'train_acc',\n              'val_loss', 'val_acc'\n\n    Example:\n        &gt;&gt;&gt; history = train_model(model, train_loader, val_loader)\n        &gt;&gt;&gt; plot_curves(history)\n    \"\"\"\n</code></pre>"},{"location":"Week5_Day24/#3-remove-dead-code","title":"3. Remove Dead Code","text":"<ul> <li>Delete commented-out experiments</li> <li>Remove unused imports</li> <li>Clean up debug print statements</li> </ul>"},{"location":"Week5_Day24/#4-add-comments","title":"4. Add Comments","text":"<pre><code># Load pretrained model and freeze early layers\nmodel = models.resnet18(pretrained=True)\nfor param in list(model.parameters())[:-10]:\n    param.requires_grad = False  # Freeze for transfer learning\n\n# Replace final layer for binary classification\nmodel.fc = nn.Linear(model.fc.in_features, 2)\n</code></pre> <p>\u2610 Code organized \u2610 Docstrings added \u2610 Comments clear and helpful \u2610 Dead code removed</p>"},{"location":"Week5_Day24/#daily-reflection-30-min","title":"Daily Reflection (30 min)","text":"<ol> <li> <p>Final performance: Did you meet your success criteria (&gt;80-85%)?</p> </li> <li> <p>Biggest improvement: What technique gave the largest boost?</p> </li> <li> <p>Challenges overcome: What was hardest and how did you solve it?</p> </li> <li> <p>Model limitations: What does your model still struggle with?</p> </li> <li> <p>Real-world readiness: Could this model be used in practice? What would be needed?</p> </li> </ol>"},{"location":"Week5_Day24/#end-of-day-24-checklist","title":"End of Day 24 Checklist","text":"<p>\u2610 Comprehensive evaluation completed \u2610 Error analysis performed \u2610 Edge cases tested \u2610 Final model trained and saved \u2610 All visualizations created \u2610 Code cleaned and documented \u2610 Results folder organized \u2610 Ready for documentation tomorrow</p> <p>Final Performance Target: - Track 1: &gt;85% test accuracy - Track 2: &gt;85% test accuracy - Track 3: &gt;52% test accuracy</p> <p>Tomorrow (Day 25): Documentation, README, and presentation preparation - make it portfolio-ready!</p>"},{"location":"Week5_Day25/","title":"Week 5, Day 25: Documentation &amp; Presentation","text":""},{"location":"Week5_Day25/#daily-goals","title":"Daily Goals","text":"<p>Make your project portfolio-ready! Today you'll write professional documentation, create a presentation, and celebrate your accomplishment!</p>"},{"location":"Week5_Day25/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week5_Day25/#professional-readme-2-hours","title":"Professional README (2 hours)","text":"<p>Your README is the first thing people see. Make it count!</p>"},{"location":"Week5_Day25/#readmemd-structure","title":"README.md Structure","text":"<pre><code># [Your Project Title]\n\nBrief, compelling description of what your project does.\n\n![Results Visualization](results/figures/final_results.png)\n\n## Overview\n\n**Problem**: [What problem are you solving?]\n\n**Solution**: [Your approach in 2-3 sentences]\n\n**Results**: [Your best metrics in bold]\n- **Test Accuracy**: XX.X%\n- **[Key Metric]**: XX.X%\n\n## Motivation\n\nWhy is this problem interesting/important? (2-3 paragraphs)\n\n## \ud83d\udcca Dataset\n\n- **Source**: [Link to dataset]\n- **Size**: X,XXX samples\n- **Classes**: [Class names and distributions]\n- **Split**: X% train, X% validation, X% test\n\n[Optional: Show sample images/text]\n\n## Architecture\n\nBrief description of your model architecture.\n\n**Baseline Model**:\n- [Simple description]\n- Parameters: X,XXX\n- Accuracy: XX.X%\n\n**Final Model**:\n- [Description with key innovations]\n- Parameters: X,XXX\n- Accuracy: XX.X% (+X.X% improvement)\n\n[Optional: Architecture diagram]\n\n## \ud83d\udd2c Experiments\n\n### Techniques Tried:\n1. **Transfer Learning** \u2192 +X% improvement\n2. **Data Augmentation** \u2192 +X% improvement\n3. **Hyperparameter Tuning** \u2192 +X% improvement\n\n### What Worked:\n- [Technique 1]: [Why it helped]\n- [Technique 2]: [Why it helped]\n\n### What Didn't Work:\n- [Technique]: [Why it failed]\n\n## Results\n\n### Final Performance:\n\\```\nTest Accuracy: XX.X%\nPrecision: XX.X%\nRecall: XX.X%\nF1-Score: XX.X%\n\\```\n\n### Per-Class Performance:\n| Class | Accuracy | Precision | Recall |\n|-------|----------|-----------|--------|\n| Class 0 | XX.X% | XX.X% | XX.X% |\n| Class 1 | XX.X% | XX.X% | XX.X% |\n\n### Visualizations:\n- [Link to confusion matrix]\n- [Link to training curves]\n- [Link to sample predictions]\n\n## Getting Started\n\n### Prerequisites\n\\```bash\nPython 3.9+\nPyTorch 2.0+\ntorchvision\nnumpy\nmatplotlib\n\\```\n\n### Installation\n\\```bash\ngit clone [your-repo]\ncd [your-project]\npip install -r requirements.txt\n\\```\n\n### Download Dataset\n[Instructions for getting data]\n\n### Training\n\\```bash\npython src/train.py --epochs 15 --lr 0.001 --batch-size 32\n\\```\n\n### Evaluation\n\\```bash\npython src/evaluate.py --model models/final_model.pth\n\\```\n\n## Key Learnings\n\n1. [Important insight 1]\n2. [Important insight 2]\n3. [Important insight 3]\n\n## Future Work\n\n- [ ] Try transformer-based models\n- [ ] Experiment with semi-supervised learning\n- [ ] Deploy as web application\n- [ ] Collect more training data\n\n## References\n\n- [Dataset source]\n- [Key papers or tutorials used]\n- [Pretrained models used]\n\n## \ud83d\udc64 Author\n\n[Your Name]  \n[Your LinkedIn/GitHub]  \nProject for: 5-Week ML Training Program\n\n## License\n\n[Choose license: MIT, Apache 2.0, etc.]\n</code></pre>"},{"location":"Week5_Day25/#readme-writing-tips","title":"README Writing Tips:","text":"<ol> <li>Be concise: Developers skim - use bullets and short paragraphs</li> <li>Show results early: Put your best metrics near the top</li> <li>Include visuals: One picture &gt; 1000 words</li> <li>Make it runnable: Clear setup instructions</li> <li>Tell a story: What did you learn? What would you do differently?</li> </ol> <p>\u2610 README.md written \u2610 All sections complete \u2610 Links and images work \u2610 Professional and clear</p>"},{"location":"Week5_Day25/#code-documentation-1-hour","title":"Code Documentation (1 hour)","text":"<p>Add final polish to your code:</p>"},{"location":"Week5_Day25/#1-module-docstrings","title":"1. Module Docstrings","text":"<pre><code>\"\"\"\ndata.py - Data Loading and Preprocessing\n\nThis module handles all data loading, preprocessing, and augmentation\nfor the [Your Project] classification task.\n\nClasses:\n    MyDataset: Custom PyTorch Dataset for loading [your data]\n\nFunctions:\n    get_transforms: Returns train/val transforms\n    create_dataloaders: Creates train/val/test DataLoaders\n\nExample:\n    &gt;&gt;&gt; train_loader, val_loader, test_loader = create_dataloaders('data/raw')\n    &gt;&gt;&gt; for images, labels in train_loader:\n    ...     # Training code\n\"\"\"\n</code></pre>"},{"location":"Week5_Day25/#2-configuration-file","title":"2. Configuration File","text":"<p>Create <code>config.py</code> for all hyperparameters:</p> <pre><code>\"\"\"Configuration file for all hyperparameters.\"\"\"\n\nclass Config:\n    # Data\n    DATA_DIR = 'data/raw'\n    BATCH_SIZE = 32\n    NUM_WORKERS = 4\n\n    # Model\n    MODEL_TYPE = 'resnet18'\n    NUM_CLASSES = 2\n    PRETRAINED = True\n\n    # Training\n    NUM_EPOCHS = 15\n    LEARNING_RATE = 0.001\n    WEIGHT_DECAY = 1e-5\n\n    # Paths\n    CHECKPOINT_DIR = 'models/checkpoints'\n    RESULTS_DIR = 'results'\n</code></pre>"},{"location":"Week5_Day25/#3-requirements-file","title":"3. Requirements File","text":"<p>Create complete <code>requirements.txt</code>:</p> <pre><code>torch==2.0.1\ntorchvision==0.15.2\nnumpy==1.24.3\nmatplotlib==3.7.1\npandas==2.0.3\nscikit-learn==1.3.0\ntqdm==4.65.0\npillow==10.0.0  # If using images\nseaborn==0.12.2  # For visualizations\n</code></pre> <p>\u2610 All files have module docstrings \u2610 Config file created \u2610 Requirements.txt complete</p>"},{"location":"Week5_Day25/#results-documentation-1-hour","title":"Results Documentation (1 hour)","text":"<p>Create a comprehensive results document:</p>"},{"location":"Week5_Day25/#resultsresultsmd","title":"results/RESULTS.md","text":"<pre><code># Project Results Summary\n\n## Methodology\n\n### Data Preprocessing\n- [Describe steps]\n- [Any challenges encountered]\n\n### Model Development\n1. **Baseline**: [Description] \u2192 XX.X% accuracy\n2. **Iteration 1**: [Changes] \u2192 XX.X% accuracy\n3. **Iteration 2**: [Changes] \u2192 XX.X% accuracy\n4. **Final Model**: [Description] \u2192 **XX.X% accuracy**\n\n### Training Details\n- Optimizer: Adam (lr=0.001, weight_decay=1e-5)\n- Batch size: 32\n- Epochs: 15 (with early stopping)\n- Hardware: [CPU/GPU details]\n- Training time: ~X hours\n\n## Quantitative Results\n\n### Overall Performance\n\\```\nTest Set Metrics:\n  Accuracy:  XX.X%\n  Precision: XX.X%\n  Recall:    XX.X%\n  F1-Score:  XX.X%\n\\```\n\n### Confusion Matrix\n\\```\n              Predicted\n              Class 0  Class 1\nActual Class 0   XXX      XX\n       Class 1    XX     XXX\n\\```\n\n### Per-Class Analysis\n- **Class 0**: [Analysis]\n- **Class 1**: [Analysis]\n\n## Qualitative Analysis\n\n### Strengths\n- [What model does well]\n- [Examples of correct predictions]\n\n### Limitations\n- [What model struggles with]\n- [Examples of failure cases]\n\n### Error Analysis\n- [Common error patterns]\n- [Why model makes these mistakes]\n\n## Comparison with Baselines\n\n| Model | Accuracy | Parameters | Notes |\n|-------|----------|------------|-------|\n| Random | 50.0% | - | Baseline |\n| Logistic Regression | XX.X% | XXX | Simple baseline |\n| Simple CNN/LSTM | XX.X% | X,XXX | Our baseline |\n| **Final Model** | **XX.X%** | **X,XXX** | **Best** |\n\n## Lessons Learned\n\n1. **What worked**: [Key insight]\n2. **What didn't work**: [Failed approach]\n3. **Surprises**: [Unexpected finding]\n\n## Future Improvements\n\nGiven more time/resources:\n- [Improvement 1]\n- [Improvement 2]\n- [Improvement 3]\n</code></pre> <p>\u2610 Results document written \u2610 All metrics included \u2610 Analysis thoughtful and honest</p>"},{"location":"Week5_Day25/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week5_Day25/#presentation-preparation-2-hours","title":"Presentation Preparation (2 hours)","text":""},{"location":"Week5_Day25/#final-testing-30-min","title":"Final Testing (30 min)","text":"<p>Make sure everything works from scratch:</p> <pre><code># Clone your repo (if using git)\ngit clone [your-repo]\ncd [project]\n\n# Install dependencies\npip install -r requirements.txt\n\n# Download data (or verify it's available)\n# ...\n\n# Run training (on small subset to verify)\npython src/train.py --epochs 2  # Quick test\n\n# Run evaluation\npython src/evaluate.py --model models/final_model.pth\n\n# Success? \u2713\n</code></pre> <p>\u2610 End-to-end workflow tested \u2610 All scripts run without errors \u2610 Documentation accurate</p>"},{"location":"Week5_Day25/#project-submission-30-min","title":"Project Submission (30 min)","text":"<p>Prepare final deliverables:</p>"},{"location":"Week5_Day25/#checklist","title":"Checklist:","text":"<p>\u2610 Code is clean and documented \u2610 README.md is complete and professional \u2610 requirements.txt is accurate \u2610 Final model is saved \u2610 Results are documented with figures \u2610 Presentation is ready \u2610 (Optional) GitHub repo created</p>"},{"location":"Week5_Day25/#github-if-applicable","title":"GitHub (if applicable):","text":"<pre><code>git init\ngit add .\ngit commit -m \"Initial commit: [Project Name]\"\ngit branch -M main\ngit remote add origin [your-repo-url]\ngit push -u origin main\n</code></pre> <p>Make sure <code>.gitignore</code> excludes:</p> <pre><code>data/\nmodels/*.pth\n__pycache__/\n*.pyc\n.ipynb_checkpoints/\n</code></pre> <p>\u2610 Project ready for submission</p>"},{"location":"Week5_Day25/#celebration-reflection-30-min","title":"Celebration &amp; Reflection (30 min)","text":"<p>Congratulations! You've completed a full ML project!</p>"},{"location":"Week5_Day25/#final-reflection","title":"Final Reflection:","text":"<ol> <li> <p>Achievement: What are you most proud of?</p> </li> <li> <p>Growth: How have your ML skills developed?</p> </li> <li> <p>Challenges: What was hardest? How did you overcome it?</p> </li> <li> <p>Technical learning: What's the most important technical concept you learned?</p> </li> <li> <p>Process learning: What did you learn about the ML development process?</p> </li> <li> <p>Next project: What would you like to build next?</p> </li> </ol>"},{"location":"Week5_Day25/#end-of-week-5-project-complete","title":"End of Week 5 - Project Complete!","text":""},{"location":"Week5_Day25/#what-youve-accomplished","title":"What You've Accomplished:","text":"<ul> <li>Day 21: Planned comprehensive ML project</li> <li>Day 22: Built data pipeline and baseline model</li> <li>Day 23: Iterated and improved model</li> <li>Day 24: Tested thoroughly and refined</li> <li>Day 25: Documented professionally and presented</li> </ul>"},{"location":"Week5_Day25/#your-portfolio-now-includes","title":"Your Portfolio Now Includes:","text":"<p>Complete ML project with: - Working code - Trained models - Comprehensive documentation - Results visualization - Presentation</p>"},{"location":"Week5_Day25/#skills-demonstrated","title":"Skills Demonstrated:","text":"<ul> <li>Problem formulation</li> <li>Data preprocessing and augmentation</li> <li>Model architecture design</li> <li>Training and optimization</li> <li>Hyperparameter tuning</li> <li>Error analysis</li> <li>Professional documentation</li> <li>Technical presentation</li> </ul>"},{"location":"Week5_Day25/#optional-week-6","title":"Optional Week 6:","text":"<ul> <li>Continue with Week 6 (Production ML) to learn deployment</li> <li>Deploy this project as an API</li> <li>Add monitoring and logging</li> <li>Make it production-ready</li> </ul>"},{"location":"Week5_Day25/#you-did-it","title":"You Did It!","text":"<p>You've gone from choosing a dataset to having a complete, documented, presentation-ready ML project. This is a significant achievement!</p> <p>Remember: - Perfect is the enemy of done - Real projects teach more than tutorials - Your first project won't be your last - The ML community is here to help</p> <p>Congratulations on completing Week 5! </p> <p>Whether you continue to Week 6 or take what you've learned into new projects, you now have the skills and confidence to build real ML systems.</p> <p>Keep learning, keep building, and most importantly - keep sharing what you create!</p> <p>See you in Week 6 (optional) or in your next ML adventure! </p>"},{"location":"Week5_Overview/","title":"Week 5 Overview: Capstone Project Development","text":""},{"location":"Week5_Overview/#introduction","title":"Introduction","text":"<p>Week 5 is dedicated to building your capstone project - a complete machine learning system that demonstrates everything you've learned. You'll choose a project track, develop your solution iteratively, and present a portfolio-ready project. This week emphasizes independent work with structured guidance and checkpoints.</p>"},{"location":"Week5_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Plan and scope a realistic ML project with clear success criteria</li> <li>Build a complete end-to-end ML pipeline from data to evaluation</li> <li>Apply techniques from Weeks 1-4 to a real problem</li> <li>Debug and iterate on model performance systematically</li> <li>Document your work professionally for your portfolio</li> <li>Present your project and results confidently</li> </ul>"},{"location":"Week5_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 21: Project Planning &amp; Setup - Choose track, define scope, explore data</li> <li>Day 22: Data Pipeline &amp; Baseline Model - Preprocessing, simple model, initial results</li> <li>Day 23: Model Development &amp; Iteration - Advanced architectures, experimentation</li> <li>Day 24: Testing &amp; Refinement - Comprehensive evaluation, optimization, edge cases</li> <li>Day 25: Documentation &amp; Presentation - Polish code, create README, present project</li> </ul>"},{"location":"Week5_Overview/#project-tracks","title":"Project Tracks","text":"<p>Choose one track or propose your own (with instructor approval):</p> <p>Track 1: Computer Vision - Image classification on domain-specific dataset Track 2: Natural Language Processing - Text classification with modern techniques Track 3: Time Series Forecasting - Sequential prediction on real-world data</p> <p>Each track includes suggested datasets, architecture recommendations, and success criteria.</p>"},{"location":"Week5_Overview/#success-criteria","title":"Success Criteria","text":"<p>Your project should demonstrate: - Clean, well-organized code structure - Proper train/validation/test splits - Model achieving reasonable performance (&gt;80% accuracy or appropriate metric) - Comprehensive evaluation with visualizations - Professional documentation (README with results) - Working code that runs without errors</p> <p>Ready to start? Begin with Day 21: Project Planning</p>"},{"location":"Week6_Day26/","title":"Week 6, Day 26: Model Optimization - Speed, Size, and Efficiency","text":""},{"location":"Week6_Day26/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand model optimization techniques for production</li> <li>Learn quantization (FP32 \u2192 INT8) to reduce model size</li> <li>Explore model pruning to remove unnecessary weights</li> <li>Export models to ONNX format for cross-platform deployment</li> <li>Measure inference speed and model size improvements</li> <li>Optimize your Week 5 project model</li> </ul>"},{"location":"Week6_Day26/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week6_Day26/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week6_Day26/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Model Optimization Overview by Google Cloud (15 min)</p> <p>\u2610 Watch: PyTorch Quantization by PyTorch (25 min)</p> <p>\u2610 Watch: ONNX Explained by Microsoft (20 min)</p> <p>\u2610 Watch: Model Pruning Tutorial by PyTorch (30 min)</p>"},{"location":"Week6_Day26/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: PyTorch Quantization Docs</p> <p>\u2610 Read: ONNX Introduction</p>"},{"location":"Week6_Day26/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week6_Day26/#exercise-1-baseline-model-benchmarking-30-min","title":"Exercise 1: Baseline Model Benchmarking (30 min)","text":"<p>First, load your Week 5 project model and establish baseline metrics:</p> <pre><code>import torch\nimport torch.nn as nn\nimport time\nimport os\n\n# Load your Week 5 model (adjust path and architecture)\n# Example for Track 1 (Medical Images):\nclass YourModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Your architecture from Week 5\n        pass\n\n    def forward(self, x):\n        # Your forward pass\n        pass\n\n# Load trained model\nmodel = YourModel()\nmodel.load_state_dict(torch.load('your_week5_model.pth'))\nmodel.eval()\n\n# Measure baseline metrics\ndef benchmark_model(model, input_shape, device='cpu', num_runs=100):\n    \"\"\"Benchmark model size and inference speed\"\"\"\n    model = model.to(device)\n    model.eval()\n\n    # Create dummy input\n    dummy_input = torch.randn(1, *input_shape).to(device)\n\n    # Warmup\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(dummy_input)\n\n    # Measure inference time\n    start = time.time()\n    with torch.no_grad():\n        for _ in range(num_runs):\n            _ = model(dummy_input)\n    end = time.time()\n\n    avg_time = (end - start) / num_runs * 1000  # ms\n\n    # Measure model size\n    torch.save(model.state_dict(), 'temp_model.pth')\n    size_mb = os.path.getsize('temp_model.pth') / (1024 * 1024)\n    os.remove('temp_model.pth')\n\n    return {\n        'avg_inference_time_ms': avg_time,\n        'model_size_mb': size_mb,\n        'num_parameters': sum(p.numel() for p in model.parameters())\n    }\n\n# Benchmark your model\n# Adjust input_shape based on your track:\n# Track 1: (3, 224, 224) for images\n# Track 2: (200,) for text sequences\n# Track 3: (10,) for stock features\ninput_shape = (3, 224, 224)  # Adjust this!\n\nbaseline = benchmark_model(model, input_shape)\nprint(\"\\nBaseline Model Metrics:\")\nprint(f\"  Inference Time: {baseline['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Model Size: {baseline['model_size_mb']:.2f} MB\")\nprint(f\"  Parameters: {baseline['num_parameters']:,}\")\n</code></pre> <p>Expected: Baseline metrics established for comparison</p>"},{"location":"Week6_Day26/#exercise-2-dynamic-quantization-40-min","title":"Exercise 2: Dynamic Quantization (40 min)","text":"<p>Apply dynamic quantization to reduce model size:</p> <pre><code>import torch.quantization as quantization\n\n# Dynamic quantization (easiest, works for most models)\nquantized_model = quantization.quantize_dynamic(\n    model,\n    {nn.Linear, nn.LSTM},  # Layers to quantize\n    dtype=torch.qint8\n)\n\n# Benchmark quantized model\nquantized_metrics = benchmark_model(quantized_model, input_shape)\n\nprint(\"\\nQuantized Model Metrics:\")\nprint(f\"  Inference Time: {quantized_metrics['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Model Size: {quantized_metrics['model_size_mb']:.2f} MB\")\nprint(f\"  Parameters: {quantized_metrics['num_parameters']:,}\")\n\nprint(\"\\nImprovements:\")\nprint(f\"  Speed: {baseline['avg_inference_time_ms']/quantized_metrics['avg_inference_time_ms']:.2f}x faster\")\nprint(f\"  Size: {baseline['model_size_mb']/quantized_metrics['model_size_mb']:.2f}x smaller\")\n\n# Save quantized model\ntorch.save(quantized_model.state_dict(), 'quantized_model.pth')\n</code></pre> <p>Expected: 2-4x reduction in model size, possible speed improvement on CPU</p>"},{"location":"Week6_Day26/#exercise-3-model-pruning-50-min","title":"Exercise 3: Model Pruning (50 min)","text":"<p>Remove less important weights to reduce model size:</p> <pre><code>import torch.nn.utils.prune as prune\n\n# Load fresh model\nmodel_prune = YourModel()\nmodel_prune.load_state_dict(torch.load('your_week5_model.pth'))\n\n# Apply L1 unstructured pruning to all linear/conv layers\ndef apply_pruning(model, amount=0.3):\n    \"\"\"Apply pruning to all Conv2d and Linear layers\"\"\"\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.Conv2d, nn.Linear)):\n            prune.l1_unstructured(module, name='weight', amount=amount)\n            # Make pruning permanent\n            prune.remove(module, 'weight')\n    return model\n\n# Prune 30% of weights\nmodel_pruned = apply_pruning(model_prune, amount=0.3)\n\n# Benchmark pruned model\npruned_metrics = benchmark_model(model_pruned, input_shape)\n\nprint(\"\\nPruned Model Metrics (30% weights removed):\")\nprint(f\"  Inference Time: {pruned_metrics['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Model Size: {pruned_metrics['model_size_mb']:.2f} MB\")\n\n# Check sparsity\ndef check_sparsity(model):\n    \"\"\"Calculate percentage of zero weights\"\"\"\n    zeros = 0\n    total = 0\n    for param in model.parameters():\n        zeros += torch.sum(param == 0).item()\n        total += param.numel()\n    return 100 * zeros / total\n\nsparsity = check_sparsity(model_pruned)\nprint(f\"  Sparsity: {sparsity:.1f}% zeros\")\n</code></pre> <p>Expected: Model size reduction with minimal accuracy loss</p>"},{"location":"Week6_Day26/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week6_Day26/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week6_Day26/#exercise-4-onnx-export-50-min","title":"Exercise 4: ONNX Export (50 min)","text":"<p>Export your model to ONNX for cross-platform deployment:</p> <pre><code>import onnx\nimport onnxruntime\n\n# Export to ONNX\ndummy_input = torch.randn(1, *input_shape)\n\ntorch.onnx.export(\n    model,                          # Model\n    dummy_input,                    # Example input\n    \"model.onnx\",                   # Output file\n    export_params=True,             # Store trained weights\n    opset_version=11,               # ONNX version\n    input_names=['input'],          # Input name\n    output_names=['output'],        # Output name\n    dynamic_axes={\n        'input': {0: 'batch_size'},\n        'output': {0: 'batch_size'}\n    }\n)\n\nprint(\"Model exported to ONNX!\")\n\n# Verify ONNX model\nonnx_model = onnx.load(\"model.onnx\")\nonnx.checker.check_model(onnx_model)\nprint(\"ONNX model is valid!\")\n\n# Benchmark ONNX runtime\nort_session = onnxruntime.InferenceSession(\"model.onnx\")\n\ndef benchmark_onnx(session, input_shape, num_runs=100):\n    \"\"\"Benchmark ONNX model\"\"\"\n    dummy_input = torch.randn(1, *input_shape).numpy()\n\n    # Warmup\n    for _ in range(10):\n        _ = session.run(None, {'input': dummy_input})\n\n    # Measure\n    start = time.time()\n    for _ in range(num_runs):\n        _ = session.run(None, {'input': dummy_input})\n    end = time.time()\n\n    avg_time = (end - start) / num_runs * 1000\n\n    size_mb = os.path.getsize(\"model.onnx\") / (1024 * 1024)\n\n    return {'avg_inference_time_ms': avg_time, 'model_size_mb': size_mb}\n\nonnx_metrics = benchmark_onnx(ort_session, input_shape)\n\nprint(\"\\nONNX Model Metrics:\")\nprint(f\"  Inference Time: {onnx_metrics['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Model Size: {onnx_metrics['model_size_mb']:.2f} MB\")\n</code></pre> <p>Expected: ONNX model with comparable or better inference speed</p>"},{"location":"Week6_Day26/#exercise-5-combined-optimization-90-min","title":"Exercise 5: Combined Optimization (90 min)","text":"<p>Combine techniques for maximum optimization:</p> <pre><code># 1. Start with pruned model\nmodel_optimized = YourModel()\nmodel_optimized.load_state_dict(torch.load('your_week5_model.pth'))\nmodel_optimized = apply_pruning(model_optimized, amount=0.4)\n\n# 2. Apply quantization\nmodel_optimized = quantization.quantize_dynamic(\n    model_optimized,\n    {nn.Linear, nn.Conv2d},\n    dtype=torch.qint8\n)\n\n# 3. Export to ONNX (quantized ONNX)\ntorch.onnx.export(\n    model_optimized,\n    torch.randn(1, *input_shape),\n    \"model_optimized.onnx\",\n    opset_version=11\n)\n\n# Benchmark final model\nfinal_session = onnxruntime.InferenceSession(\"model_optimized.onnx\")\nfinal_metrics = benchmark_onnx(final_session, input_shape)\n\nprint(\"\\n=== OPTIMIZATION SUMMARY ===\")\nprint(f\"\\nBaseline:\")\nprint(f\"  Time: {baseline['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Size: {baseline['model_size_mb']:.2f} MB\")\n\nprint(f\"\\nFinal Optimized (Pruned + Quantized + ONNX):\")\nprint(f\"  Time: {final_metrics['avg_inference_time_ms']:.2f} ms\")\nprint(f\"  Size: {final_metrics['model_size_mb']:.2f} MB\")\n\nprint(f\"\\nTotal Improvements:\")\nprint(f\"  Speed: {baseline['avg_inference_time_ms']/final_metrics['avg_inference_time_ms']:.2f}x faster\")\nprint(f\"  Size: {baseline['model_size_mb']/final_metrics['model_size_mb']:.2f}x smaller\")\n\n# Create comparison visualization\nimport matplotlib.pyplot as plt\n\nmetrics = ['Baseline', 'Quantized', 'Pruned', 'ONNX', 'Combined']\ntimes = [baseline['avg_inference_time_ms'], \n         quantized_metrics['avg_inference_time_ms'],\n         pruned_metrics['avg_inference_time_ms'],\n         onnx_metrics['avg_inference_time_ms'],\n         final_metrics['avg_inference_time_ms']]\n\nsizes = [baseline['model_size_mb'],\n         quantized_metrics['model_size_mb'],\n         pruned_metrics['model_size_mb'],\n         onnx_metrics['model_size_mb'],\n         final_metrics['model_size_mb']]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.bar(metrics, times, color=['gray', 'blue', 'green', 'orange', 'red'])\nax1.set_ylabel('Inference Time (ms)')\nax1.set_title('Model Inference Speed')\nax1.grid(True, alpha=0.3)\n\nax2.bar(metrics, sizes, color=['gray', 'blue', 'green', 'orange', 'red'])\nax2.set_ylabel('Model Size (MB)')\nax2.set_title('Model Size')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('optimization_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n</code></pre> <p>Goal: 3-5x speed improvement, 4-8x size reduction, &lt;2% accuracy loss</p>"},{"location":"Week6_Day26/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Compare optimization techniques (which worked best for your model?) \u2610 Consider trade-offs between speed, size, and accuracy \u2610 Write daily reflection (choose 2-3 prompts) \u2610 Save all optimized models for tomorrow's deployment  </p>"},{"location":"Week6_Day26/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What optimization technique gave the best results for your model?</li> <li>How much did you improve inference speed and model size?</li> <li>What trade-offs did you observe between optimization and accuracy?</li> <li>Which optimization would be most important for your use case?</li> <li>How does quantization affect different types of layers differently?</li> <li>What challenges did you face during ONNX export?</li> </ul>"},{"location":"Week6_Day26/#summary-checklist","title":"Summary Checklist","text":"<p>By end of day, you should have:</p> <ul> <li>\u2610 Baseline model benchmarks  </li> <li>\u2610 Quantized model (2-4x smaller)  </li> <li>\u2610 Pruned model (with sparsity metrics)  </li> <li>\u2610 ONNX exported model  </li> <li>\u2610 Combined optimized model (3-5x improvements)  </li> <li>\u2610 Comparison visualization saved  </li> </ul> <p>Next: Day 27 - Model Deployment</p>"},{"location":"Week6_Day27/","title":"Week 6, Day 27: Model Deployment - APIs, Docker, and Cloud","text":""},{"location":"Week6_Day27/#daily-goals","title":"Daily Goals","text":"<ul> <li>Build a REST API for your model using FastAPI</li> <li>Containerize your application with Docker</li> <li>Deploy to a cloud platform (optional)</li> <li>Create prediction endpoints for your Week 5 project</li> <li>Test API with various clients</li> <li>Understand production deployment workflows</li> </ul>"},{"location":"Week6_Day27/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week6_Day27/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week6_Day27/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: FastAPI in 45 Minutes by freeCodeCamp (45 min)</p> <p>\u2610 Watch: Docker in 100 Seconds by Fireship (2 min)</p> <p>\u2610 Watch: Docker Tutorial for Beginners by Programming with Mosh (43 min - watch first 25 min)</p>"},{"location":"Week6_Day27/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: FastAPI Documentation</p> <p>\u2610 Read: Docker Getting Started</p>"},{"location":"Week6_Day27/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week6_Day27/#exercise-1-simple-fastapi-server-30-min","title":"Exercise 1: Simple FastAPI Server (30 min)","text":"<p>Create a basic API server:</p> <pre><code># app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport torch\nimport numpy as np\n\napp = FastAPI(title=\"ML Model API\", version=\"1.0\")\n\n# Health check endpoint\n@app.get(\"/\")\nasync def root():\n    return {\n        \"message\": \"ML Model API\",\n        \"version\": \"1.0\",\n        \"status\": \"healthy\"\n    }\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": True}\n\n# Load model at startup\n@app.on_event(\"startup\")\nasync def load_model():\n    global model\n    # Load your optimized model from Day 26\n    model = torch.jit.load(\"model_optimized.pth\")\n    model.eval()\n    print(\"Model loaded successfully!\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Run the server:</p> <pre><code>pip install fastapi uvicorn python-multipart\npython app.py\n</code></pre> <p>Visit <code>http://localhost:8000/docs</code> to see auto-generated API documentation!</p> <p>Expected: Working API server with health check endpoint</p>"},{"location":"Week6_Day27/#exercise-2-add-prediction-endpoint-45-min","title":"Exercise 2: Add Prediction Endpoint (45 min)","text":"<p>Add prediction functionality based on your Week 5 track:</p> <p>Track 1: Image Classification (Medical Images)</p> <pre><code>from fastapi import File, UploadFile\nfrom PIL import Image\nimport io\nfrom torchvision import transforms\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nclass PredictionResponse(BaseModel):\n    class_name: str\n    confidence: float\n    all_probabilities: dict\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_image(file: UploadFile = File(...)):\n    \"\"\"Predict class of uploaded image\"\"\"\n    # Read image\n    contents = await file.read()\n    image = Image.open(io.BytesIO(contents)).convert('RGB')\n\n    # Preprocess\n    input_tensor = transform(image).unsqueeze(0)\n\n    # Predict\n    with torch.no_grad():\n        output = model(input_tensor)\n        probabilities = torch.softmax(output, dim=1)[0]\n\n    # Get prediction\n    class_idx = torch.argmax(probabilities).item()\n    class_names = ['NORMAL', 'PNEUMONIA']  # Adjust for your classes\n\n    return {\n        \"class_name\": class_names[class_idx],\n        \"confidence\": float(probabilities[class_idx]),\n        \"all_probabilities\": {\n            class_names[i]: float(probabilities[i]) \n            for i in range(len(class_names))\n        }\n    }\n</code></pre> <p>Track 2: Text Classification (Sentiment Analysis)</p> <pre><code>class TextInput(BaseModel):\n    text: str\n\nclass PredictionResponse(BaseModel):\n    sentiment: str\n    confidence: float\n    all_probabilities: dict\n\n# Load vocabulary from Week 5\nimport pickle\nwith open('vocabulary.pkl', 'rb') as f:\n    vocab = pickle.load(f)\n\ndef preprocess_text(text, vocab, max_len=200):\n    \"\"\"Convert text to token indices\"\"\"\n    tokens = text.lower().split()\n    indices = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\n    # Pad/truncate\n    if len(indices) &lt; max_len:\n        indices += [vocab['&lt;PAD&gt;']] * (max_len - len(indices))\n    else:\n        indices = indices[:max_len]\n    return torch.LongTensor(indices).unsqueeze(0)\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_sentiment(input_data: TextInput):\n    \"\"\"Predict sentiment of text\"\"\"\n    # Preprocess\n    input_tensor = preprocess_text(input_data.text, vocab)\n\n    # Predict\n    with torch.no_grad():\n        output = model(input_tensor)\n        probabilities = torch.softmax(output, dim=1)[0]\n\n    sentiments = ['NEGATIVE', 'POSITIVE']\n    class_idx = torch.argmax(probabilities).item()\n\n    return {\n        \"sentiment\": sentiments[class_idx],\n        \"confidence\": float(probabilities[class_idx]),\n        \"all_probabilities\": {\n            sentiments[i]: float(probabilities[i]) \n            for i in range(len(sentiments))\n        }\n    }\n</code></pre> <p>Track 3: Time Series (Stock Prediction)</p> <pre><code>class StockInput(BaseModel):\n    features: list[float]  # Last N days of features\n\nclass PredictionResponse(BaseModel):\n    prediction: str  # \"UP\" or \"DOWN\"\n    confidence: float\n    predicted_return: float\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_stock(input_data: StockInput):\n    \"\"\"Predict stock movement\"\"\"\n    # Preprocess\n    input_tensor = torch.FloatTensor(input_data.features).unsqueeze(0)\n\n    # Predict\n    with torch.no_grad():\n        output = model(input_tensor)\n        probabilities = torch.softmax(output, dim=1)[0]\n\n    class_idx = torch.argmax(probabilities).item()\n    directions = ['DOWN', 'UP']\n\n    return {\n        \"prediction\": directions[class_idx],\n        \"confidence\": float(probabilities[class_idx]),\n        \"predicted_return\": float(output[0][class_idx])\n    }\n</code></pre> <p>Test your endpoint:</p> <pre><code># Test with curl (adjust for your track)\n# Track 1 (Image):\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -F \"file=@test_image.jpg\"\n\n# Track 2 (Text):\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"text\": \"This product is amazing!\"}'\n\n# Track 3 (Stock):\ncurl -X POST \"http://localhost:8000/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"features\": [1.2, 3.4, 2.1, 4.5, 1.8]}'\n</code></pre> <p>Expected: Working prediction endpoint returning JSON responses</p>"},{"location":"Week6_Day27/#exercise-3-add-batch-prediction-45-min","title":"Exercise 3: Add Batch Prediction (45 min)","text":"<p>Support multiple predictions in one request:</p> <pre><code># Track 1: Batch image prediction\n@app.post(\"/predict/batch\")\nasync def predict_batch_images(files: list[UploadFile] = File(...)):\n    \"\"\"Predict multiple images at once\"\"\"\n    results = []\n\n    for file in files:\n        contents = await file.read()\n        image = Image.open(io.BytesIO(contents)).convert('RGB')\n        input_tensor = transform(image).unsqueeze(0)\n\n        with torch.no_grad():\n            output = model(input_tensor)\n            probabilities = torch.softmax(output, dim=1)[0]\n\n        class_idx = torch.argmax(probabilities).item()\n\n        results.append({\n            \"filename\": file.filename,\n            \"class_name\": class_names[class_idx],\n            \"confidence\": float(probabilities[class_idx])\n        })\n\n    return {\"predictions\": results, \"count\": len(results)}\n\n# Track 2: Batch text prediction\nclass BatchTextInput(BaseModel):\n    texts: list[str]\n\n@app.post(\"/predict/batch\")\nasync def predict_batch_texts(input_data: BatchTextInput):\n    \"\"\"Predict multiple texts at once\"\"\"\n    results = []\n\n    for text in input_data.texts:\n        input_tensor = preprocess_text(text, vocab)\n\n        with torch.no_grad():\n            output = model(input_tensor)\n            probabilities = torch.softmax(output, dim=1)[0]\n\n        class_idx = torch.argmax(probabilities).item()\n\n        results.append({\n            \"text\": text[:50] + \"...\" if len(text) &gt; 50 else text,\n            \"sentiment\": sentiments[class_idx],\n            \"confidence\": float(probabilities[class_idx])\n        })\n\n    return {\"predictions\": results, \"count\": len(results)}\n</code></pre> <p>Expected: Efficient batch processing endpoint</p>"},{"location":"Week6_Day27/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week6_Day27/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week6_Day27/#exercise-4-dockerize-your-application-60-min","title":"Exercise 4: Dockerize Your Application (60 min)","text":"<p>Create a Docker container for your API:</p> <pre><code># Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY app.py .\nCOPY model_optimized.pth .\nCOPY vocabulary.pkl .  # If using Track 2\n\n# Expose port\nEXPOSE 8000\n\n# Run application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <pre><code># requirements.txt\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\ntorch==2.1.0\ntorchvision==0.16.0  # If using Track 1\npillow==10.1.0  # If using Track 1\npython-multipart==0.0.6\npydantic==2.5.0\nnumpy==1.24.3\n</code></pre> <p>Build and run:</p> <pre><code># Build image\ndocker build -t ml-api:v1 .\n\n# Run container\ndocker run -d -p 8000:8000 --name ml-api ml-api:v1\n\n# Test\ncurl http://localhost:8000/health\n\n# View logs\ndocker logs ml-api\n\n# Stop container\ndocker stop ml-api\ndocker rm ml-api\n</code></pre> <p>Expected: Containerized API running in Docker</p>"},{"location":"Week6_Day27/#exercise-5-docker-compose-setup-45-min","title":"Exercise 5: Docker Compose Setup (45 min)","text":"<p>Create multi-container setup with monitoring:</p> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_PATH=/app/model_optimized.pth\n    volumes:\n      - ./logs:/app/logs\n    restart: unless-stopped\n\n  # Optional: Add Prometheus for monitoring\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n\n  # Optional: Add Grafana for visualization\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana-storage:/var/lib/grafana\n\nvolumes:\n  grafana-storage:\n</code></pre> <pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'fastapi'\n    static_configs:\n      - targets: ['api:8000']\n</code></pre> <p>Run:</p> <pre><code>docker-compose up -d\ndocker-compose logs -f api\ndocker-compose down\n</code></pre> <p>Expected: Multi-container deployment with monitoring</p>"},{"location":"Week6_Day27/#exercise-6-load-testing-45-min","title":"Exercise 6: Load Testing (45 min)","text":"<p>Test your API's performance under load:</p> <pre><code># test_load.py\nimport requests\nimport time\nimport concurrent.futures\nimport numpy as np\n\nAPI_URL = \"http://localhost:8000/predict\"\n\ndef make_prediction():\n    \"\"\"Make a single prediction request\"\"\"\n    # Adjust based on your track\n    # Track 1 (Image):\n    # files = {'file': open('test_image.jpg', 'rb')}\n    # response = requests.post(API_URL, files=files)\n\n    # Track 2 (Text):\n    data = {\"text\": \"This is a test sentence for prediction\"}\n    response = requests.post(API_URL, json=data)\n\n    # Track 3 (Stock):\n    # data = {\"features\": [1.2, 3.4, 2.1, 4.5, 1.8]}\n    # response = requests.post(API_URL, json=data)\n\n    return response.status_code, response.elapsed.total_seconds()\n\ndef load_test(num_requests=100, num_workers=10):\n    \"\"\"Run load test with concurrent requests\"\"\"\n    print(f\"Running load test: {num_requests} requests with {num_workers} workers\")\n\n    start_time = time.time()\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(make_prediction) for _ in range(num_requests)]\n        results = [f.result() for f in concurrent.futures.as_completed(futures)]\n\n    end_time = time.time()\n\n    # Calculate metrics\n    status_codes = [r[0] for r in results]\n    response_times = [r[1] for r in results]\n\n    total_time = end_time - start_time\n    successful = sum(1 for code in status_codes if code == 200)\n\n    print(f\"\\n=== Load Test Results ===\")\n    print(f\"Total Requests: {num_requests}\")\n    print(f\"Successful: {successful} ({successful/num_requests*100:.1f}%)\")\n    print(f\"Failed: {num_requests - successful}\")\n    print(f\"Total Time: {total_time:.2f}s\")\n    print(f\"Requests/sec: {num_requests/total_time:.2f}\")\n    print(f\"\\nResponse Times:\")\n    print(f\"  Min: {min(response_times)*1000:.2f}ms\")\n    print(f\"  Max: {max(response_times)*1000:.2f}ms\")\n    print(f\"  Mean: {np.mean(response_times)*1000:.2f}ms\")\n    print(f\"  Median: {np.median(response_times)*1000:.2f}ms\")\n    print(f\"  P95: {np.percentile(response_times, 95)*1000:.2f}ms\")\n    print(f\"  P99: {np.percentile(response_times, 99)*1000:.2f}ms\")\n\nif __name__ == \"__main__\":\n    # Warm up\n    print(\"Warming up...\")\n    for _ in range(5):\n        make_prediction()\n\n    # Run tests\n    load_test(num_requests=100, num_workers=5)\n    load_test(num_requests=500, num_workers=10)\n</code></pre> <p>Run:</p> <pre><code>python test_load.py\n</code></pre> <p>Expected: Performance metrics (aim for &gt;10 requests/sec, &lt;100ms p95 latency)</p>"},{"location":"Week6_Day27/#exercise-7-create-deployment-documentation-30-min","title":"Exercise 7: Create Deployment Documentation (30 min)","text":"<p>Document your deployment:</p> <pre><code># ML Model API - Deployment Guide\n\n## Quick Start\n\n### Local Development\n```bash\npython app.py\n</code></pre>"},{"location":"Week6_Day27/#docker-deployment","title":"Docker Deployment","text":"<pre><code>docker build -t ml-api:v1 .\ndocker run -p 8000:8000 ml-api:v1\n</code></pre>"},{"location":"Week6_Day27/#docker-compose-with-monitoring","title":"Docker Compose (with monitoring)","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"Week6_Day27/#api-endpoints","title":"API Endpoints","text":""},{"location":"Week6_Day27/#health-check","title":"Health Check","text":"<pre><code>GET /health\n</code></pre>"},{"location":"Week6_Day27/#single-prediction","title":"Single Prediction","text":"<pre><code>POST /predict\nContent-Type: application/json\nBody: {\n  \"text\": \"Your input here\"\n}\n</code></pre>"},{"location":"Week6_Day27/#batch-prediction","title":"Batch Prediction","text":"<pre><code>POST /predict/batch\nContent-Type: application/json\nBody: {\n  \"texts\": [\"text1\", \"text2\", \"text3\"]\n}\n</code></pre>"},{"location":"Week6_Day27/#performance","title":"Performance","text":"<ul> <li>Throughput: ~15 requests/second</li> <li>P95 Latency: &lt;80ms</li> <li>Model Size: 12MB (optimized)</li> <li>Container Size: 850MB</li> </ul>"},{"location":"Week6_Day27/#monitoring","title":"Monitoring","text":"<ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> </ul>"},{"location":"Week6_Day27/#cloud-deployment","title":"Cloud Deployment","text":""},{"location":"Week6_Day27/#aws-ec2","title":"AWS EC2","text":"<pre><code># Install Docker on EC2\n# Copy files to EC2\n# Run docker-compose\n</code></pre>"},{"location":"Week6_Day27/#google-cloud-run","title":"Google Cloud Run","text":"<pre><code>gcloud run deploy ml-api --source . --region us-central1\n</code></pre>"},{"location":"Week6_Day27/#heroku","title":"Heroku","text":"<pre><code>heroku container:push web\nheroku container:release web\n</code></pre> <p>Save as <code>DEPLOYMENT.md</code></p>"},{"location":"Week6_Day27/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Test all API endpoints thoroughly \u2610 Review Docker setup and understand each component \u2610 Write daily reflection (choose 2-3 prompts) \u2610 Document any deployment issues encountered</p>"},{"location":"Week6_Day27/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most challenging part of building the API?</li> <li>How did FastAPI's automatic documentation help development?</li> <li>What benefits does Docker provide for deployment?</li> <li>How did your API perform under load? Any bottlenecks?</li> <li>What would you improve in your API design?</li> <li>How would you handle scaling to 1000+ requests/second?</li> </ul>"},{"location":"Week6_Day27/#summary-checklist","title":"Summary Checklist","text":"<p>By end of day, you should have: - Working FastAPI server - Prediction endpoint for your model - Batch prediction support - Dockerized application - Docker Compose setup with monitoring - Load test results - Deployment documentation</p> <p>Next: Day 28 - MLOps Fundamentals</p>"},{"location":"Week6_Day28/","title":"Week 6, Day 28: MLOps Fundamentals - Tracking, Versioning, and Pipelines","text":""},{"location":"Week6_Day28/#daily-goals","title":"Daily Goals","text":"<ul> <li>Set up experiment tracking with MLflow or Weights &amp; Biases</li> <li>Implement model versioning and registry</li> <li>Create automated training pipelines</li> <li>Track hyperparameters, metrics, and artifacts</li> <li>Build reproducible ML workflows</li> <li>Understand ML lifecycle management</li> </ul>"},{"location":"Week6_Day28/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week6_Day28/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week6_Day28/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: MLOps Explained by IBM Technology (8 min)</p> <p>\u2610 Watch: MLflow Tutorial by DataTalks.Club (45 min)</p> <p>\u2610 Watch: Weights &amp; Biases Tutorial by Weights &amp; Biases (37 min)</p>"},{"location":"Week6_Day28/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: MLflow Documentation</p> <p>\u2610 Read: W&amp;B Quickstart</p>"},{"location":"Week6_Day28/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week6_Day28/#exercise-1-set-up-experiment-tracking-45-min","title":"Exercise 1: Set Up Experiment Tracking (45 min)","text":"<p>Choose one tool and set it up:</p> <p>Option A: MLflow</p> <pre><code>import mlflow\nimport mlflow.pytorch\n\n# Start MLflow tracking\nmlflow.set_experiment(\"week5_project_optimization\")\n\n# Training loop with tracking\nwith mlflow.start_run(run_name=\"baseline_model\"):\n    # Log parameters\n    mlflow.log_param(\"learning_rate\", 0.001)\n    mlflow.log_param(\"batch_size\", 32)\n    mlflow.log_param(\"architecture\", \"ResNet18\")\n\n    # Training code here...\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train_epoch(model, train_loader)\n        val_loss, val_acc = validate(model, val_loader)\n\n        # Log metrics\n        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n\n    # Log model\n    mlflow.pytorch.log_model(model, \"model\")\n\n    # Log artifacts\n    mlflow.log_artifact(\"training_plot.png\")\n    mlflow.log_artifact(\"confusion_matrix.png\")\n\n# View results\n# mlflow ui --port 5000\n</code></pre> <p>Option B: Weights &amp; Biases</p> <pre><code>import wandb\n\n# Initialize W&amp;B\nwandb.init(\n    project=\"week5-ml-project\",\n    config={\n        \"learning_rate\": 0.001,\n        \"batch_size\": 32,\n        \"architecture\": \"ResNet18\",\n        \"epochs\": 20\n    }\n)\n\n# Training loop\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader)\n    val_loss, val_acc = validate(model, val_loader)\n\n    # Log metrics\n    wandb.log({\n        \"epoch\": epoch,\n        \"train_loss\": train_loss,\n        \"train_acc\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc\n    })\n\n# Log model\nwandb.save(\"model.pth\")\n\n# Log images\nwandb.log({\"confusion_matrix\": wandb.Image(\"confusion_matrix.png\")})\n\nwandb.finish()\n</code></pre> <p>Expected: Experiments tracked with metrics visible in web UI</p>"},{"location":"Week6_Day28/#exercise-2-model-versioning-45-min","title":"Exercise 2: Model Versioning (45 min)","text":"<p>Implement proper model versioning:</p> <pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\n# Register model\nmodel_name = \"week5_sentiment_classifier\"  # Adjust for your track\n\n# Log and register model\nwith mlflow.start_run():\n    mlflow.pytorch.log_model(\n        model,\n        \"model\",\n        registered_model_name=model_name\n    )\n\n    # Add model metadata\n    run_id = mlflow.active_run().info.run_id\n\n# Transition model to production\nmodel_version = 1\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_version,\n    stage=\"Production\"\n)\n\n# Add model description\nclient.update_registered_model(\n    name=model_name,\n    description=\"Sentiment classifier trained on IMDB dataset. Achieves 87% accuracy.\"\n)\n\n# Load production model\nproduction_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n</code></pre> <p>Expected: Model registered with version control</p>"},{"location":"Week6_Day28/#exercise-3-automated-training-pipeline-30-min","title":"Exercise 3: Automated Training Pipeline (30 min)","text":"<p>Create a reproducible training script:</p> <pre><code># train_pipeline.py\nimport argparse\nimport yaml\nimport mlflow\n\ndef load_config(config_path):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef train_pipeline(config):\n    \"\"\"Complete training pipeline\"\"\"\n\n    # Set up experiment\n    mlflow.set_experiment(config['experiment_name'])\n\n    with mlflow.start_run():\n        # Log all config\n        mlflow.log_params(config)\n\n        # 1. Load data\n        train_loader, val_loader, test_loader = load_data(config)\n\n        # 2. Create model\n        model = create_model(config)\n\n        # 3. Train\n        best_model, history = train_model(\n            model, train_loader, val_loader, config\n        )\n\n        # 4. Evaluate\n        test_metrics = evaluate(best_model, test_loader)\n        mlflow.log_metrics(test_metrics)\n\n        # 5. Save model\n        mlflow.pytorch.log_model(best_model, \"model\")\n\n        # 6. Log artifacts\n        save_plots(history)\n        mlflow.log_artifact(\"training_history.png\")\n\n        return test_metrics\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default='config.yaml')\n    args = parser.parse_args()\n\n    config = load_config(args.config)\n    metrics = train_pipeline(config)\n    print(f\"Final test accuracy: {metrics['test_accuracy']:.4f}\")\n</code></pre> <pre><code># config.yaml\nexperiment_name: \"week5_project\"\ndata:\n  dataset_path: \"./data\"\n  train_split: 0.7\n  val_split: 0.15\n  test_split: 0.15\nmodel:\n  architecture: \"resnet18\"\n  pretrained: true\n  num_classes: 2\ntraining:\n  batch_size: 32\n  learning_rate: 0.001\n  num_epochs: 20\n  optimizer: \"adam\"\n  scheduler: \"cosine\"\n</code></pre> <p>Run pipeline:</p> <pre><code>python train_pipeline.py --config config.yaml\n</code></pre> <p>Expected: Fully automated, reproducible training run</p>"},{"location":"Week6_Day28/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week6_Day28/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week6_Day28/#exercise-4-hyperparameter-optimization-60-min","title":"Exercise 4: Hyperparameter Optimization (60 min)","text":"<p>Use MLflow to track hyperparameter sweeps:</p> <pre><code>import itertools\n\n# Define hyperparameter grid\nparam_grid = {\n    'learning_rate': [0.0001, 0.001, 0.01],\n    'batch_size': [16, 32, 64],\n    'dropout': [0.3, 0.5]\n}\n\n# Generate all combinations\nkeys = param_grid.keys()\nvalues = param_grid.values()\ncombinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\nprint(f\"Running {len(combinations)} experiments...\")\n\nbest_accuracy = 0\nbest_params = None\n\nfor params in combinations:\n    with mlflow.start_run(run_name=f\"lr{params['learning_rate']}_bs{params['batch_size']}\"):\n        # Log params\n        mlflow.log_params(params)\n\n        # Train model with these params\n        model = create_model(params)\n        accuracy = train_and_evaluate(model, params)\n\n        # Log result\n        mlflow.log_metric(\"val_accuracy\", accuracy)\n\n        # Track best\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_params = params\n            mlflow.log_param(\"is_best\", True)\n\nprint(f\"\\nBest params: {best_params}\")\nprint(f\"Best accuracy: {best_accuracy:.4f}\")\n</code></pre> <p>Expected: All experiments tracked, best parameters identified</p>"},{"location":"Week6_Day28/#exercise-5-cicd-for-ml-60-min","title":"Exercise 5: CI/CD for ML (60 min)","text":"<p>Create automated testing and deployment workflow:</p> <pre><code># .github/workflows/ml_pipeline.yml\nname: ML Pipeline\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.9\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: |\n        pytest tests/\n\n    - name: Train model\n      run: |\n        python train_pipeline.py --config config.yaml\n\n    - name: Evaluate model\n      run: |\n        python evaluate.py --model model.pth --threshold 0.85\n\n    - name: Upload model\n      if: success()\n      uses: actions/upload-artifact@v2\n      with:\n        name: trained-model\n        path: model.pth\n\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n    - name: Download model\n      uses: actions/download-artifact@v2\n      with:\n        name: trained-model\n\n    - name: Deploy to production\n      run: |\n        # Deploy model to production API\n        echo \"Deploying model...\"\n</code></pre> <pre><code># tests/test_model.py\nimport pytest\nimport torch\n\ndef test_model_forward_pass():\n    \"\"\"Test model forward pass works\"\"\"\n    model = create_model({'num_classes': 2})\n    input_tensor = torch.randn(1, 3, 224, 224)\n    output = model(input_tensor)\n    assert output.shape == (1, 2)\n\ndef test_model_accuracy_threshold():\n    \"\"\"Test model meets minimum accuracy\"\"\"\n    model = load_model('model.pth')\n    accuracy = evaluate(model, test_loader)\n    assert accuracy &gt; 0.80, f\"Model accuracy {accuracy} below threshold\"\n\ndef test_prediction_time():\n    \"\"\"Test inference speed\"\"\"\n    model = load_model('model.pth')\n    start = time.time()\n    for _ in range(100):\n        _ = model(torch.randn(1, 3, 224, 224))\n    avg_time = (time.time() - start) / 100\n    assert avg_time &lt; 0.1, f\"Inference too slow: {avg_time}s\"\n</code></pre> <p>Expected: Automated testing and deployment workflow</p>"},{"location":"Week6_Day28/#exercise-6-monitoring-dashboard-60-min","title":"Exercise 6: Monitoring Dashboard (60 min)","text":"<p>Create monitoring for your deployed model:</p> <pre><code># monitoring.py\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\n\n# Define metrics\nREQUEST_COUNT = Counter('model_predictions_total', 'Total predictions')\nREQUEST_LATENCY = Histogram('model_prediction_latency_seconds', 'Prediction latency')\nMODEL_ACCURACY = Gauge('model_accuracy', 'Current model accuracy')\nPREDICTION_DISTRIBUTION = Counter('prediction_class', 'Prediction class distribution', ['class'])\n\n# Add to FastAPI app\nfrom fastapi import FastAPI\nfrom prometheus_fastapi_instrumentator import Instrumentator\n\napp = FastAPI()\n\n# Instrument app\nInstrumentator().instrument(app).expose(app)\n\n@app.post(\"/predict\")\nasync def predict(data: InputData):\n    # Track request\n    REQUEST_COUNT.inc()\n\n    # Measure latency\n    start = time.time()\n\n    # Make prediction\n    result = model.predict(data)\n\n    # Log latency\n    REQUEST_LATENCY.observe(time.time() - start)\n\n    # Log prediction\n    PREDICTION_DISTRIBUTION.labels(class=result['class']).inc()\n\n    return result\n\n# Start metrics server\nif __name__ == \"__main__\":\n    start_http_server(8001)  # Metrics on port 8001\n    uvicorn.run(app, port=8000)\n</code></pre> <p>Expected: Prometheus metrics exposed for monitoring</p>"},{"location":"Week6_Day28/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review all tracked experiments in MLflow/W&amp;B \u2610 Understand the importance of reproducibility \u2610 Write daily reflection (choose 2-3 prompts)</p>"},{"location":"Week6_Day28/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>How does experiment tracking improve your ML workflow?</li> <li>What benefits does model versioning provide?</li> <li>How would you use these tools in a team setting?</li> <li>What challenges did you face with automation?</li> <li>How does monitoring help in production?</li> <li>What would you add to your ML pipeline?</li> </ul>"},{"location":"Week6_Day28/#summary-checklist","title":"Summary Checklist","text":"<p>By end of day, you should have: - \u2610 Experiment tracking set up (MLflow or W&amp;B) - \u2610 Model versioning implemented - \u2610 Automated training pipeline - \u2610 Hyperparameter sweep tracked - \u2610 Basic CI/CD workflow - \u2610 Monitoring metrics defined</p> <p>Next: Day 29 - Best Practices</p>"},{"location":"Week6_Day29/","title":"Week 6, Day 29: Best Practices - Testing, Documentation, and Code Quality","text":""},{"location":"Week6_Day29/#daily-goals","title":"Daily Goals","text":"<ul> <li>Write unit tests for ML code</li> <li>Create comprehensive documentation</li> <li>Implement code quality tools (linting, formatting)</li> <li>Follow ML best practices and design patterns</li> <li>Build production-ready codebase</li> <li>Prepare for final project deployment</li> </ul>"},{"location":"Week6_Day29/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week6_Day29/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"Week6_Day29/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Testing Machine Learning Code by PyData (30 min)</p> <p>\u2610 Watch: Clean Code in Python by ArjanCodes (30 min)</p> <p>\u2610 Watch: Documentation Best Practices (30 min)</p>"},{"location":"Week6_Day29/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: Python Testing with pytest</p> <p>\u2610 Read: Google Python Style Guide</p>"},{"location":"Week6_Day29/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"Week6_Day29/#exercise-1-unit-testing-60-min","title":"Exercise 1: Unit Testing (60 min)","text":"<p>Write comprehensive tests for your ML pipeline:</p> <pre><code># tests/test_data.py\nimport pytest\nimport torch\nfrom src.data import load_data, preprocess\n\ndef test_data_loading():\n    \"\"\"Test data loads correctly\"\"\"\n    train_loader, val_loader, test_loader = load_data('config.yaml')\n    assert len(train_loader) &gt; 0\n    assert len(val_loader) &gt; 0\n    assert len(test_loader) &gt; 0\n\ndef test_data_shapes():\n    \"\"\"Test data has correct shapes\"\"\"\n    train_loader, _, _ = load_data('config.yaml')\n    batch = next(iter(train_loader))\n    inputs, labels = batch\n    assert inputs.shape[0] &lt;= 32  # batch_size\n    assert inputs.shape[1:] == (3, 224, 224)  # image shape\n\ndef test_preprocessing():\n    \"\"\"Test preprocessing function\"\"\"\n    raw_input = \"This is a test sentence.\"\n    processed = preprocess(raw_input)\n    assert isinstance(processed, torch.Tensor)\n    assert processed.dim() == 1\n\n# tests/test_model.py\ndef test_model_initialization():\n    \"\"\"Test model creates correctly\"\"\"\n    model = create_model({'num_classes': 2})\n    assert model is not None\n    assert sum(p.numel() for p in model.parameters()) &gt; 0\n\ndef test_model_training_step():\n    \"\"\"Test single training step\"\"\"\n    model = create_model({'num_classes': 2})\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # Fake batch\n    inputs = torch.randn(4, 3, 224, 224)\n    labels = torch.randint(0, 2, (4,))\n\n    # Forward\n    outputs = model(inputs)\n    loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n\n    # Backward\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    assert loss.item() &gt; 0\n\ndef test_model_prediction_shape():\n    \"\"\"Test prediction output shape\"\"\"\n    model = create_model({'num_classes': 2})\n    model.eval()\n\n    with torch.no_grad():\n        output = model(torch.randn(1, 3, 224, 224))\n\n    assert output.shape == (1, 2)\n    assert torch.allclose(output.sum(dim=1), torch.ones(1))  # sums to 1 after softmax\n\n# tests/test_api.py\nfrom fastapi.testclient import TestClient\nfrom app import app\n\nclient = TestClient(app)\n\ndef test_health_endpoint():\n    \"\"\"Test health check\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"healthy\"\n\ndef test_prediction_endpoint():\n    \"\"\"Test prediction works\"\"\"\n    # Adjust based on your track\n    data = {\"text\": \"This is great!\"}\n    response = client.post(\"/predict\", json=data)\n    assert response.status_code == 200\n    assert \"sentiment\" in response.json()\n    assert \"confidence\" in response.json()\n\ndef test_invalid_input():\n    \"\"\"Test API handles invalid input\"\"\"\n    response = client.post(\"/predict\", json={})\n    assert response.status_code == 422  # Validation error\n</code></pre> <p>Run tests:</p> <pre><code>pip install pytest pytest-cov\npytest tests/ -v --cov=src\n</code></pre> <p>Expected: All tests passing with &gt;80% code coverage</p>"},{"location":"Week6_Day29/#exercise-2-code-quality-tools-60-min","title":"Exercise 2: Code Quality Tools (60 min)","text":"<p>Set up linting and formatting:</p> <pre><code># Install tools\npip install black flake8 isort mypy pylint\n\n# Format code\nblack src/ tests/\nisort src/ tests/\n\n# Check code quality\nflake8 src/ tests/ --max-line-length=100\npylint src/ tests/\n\n# Type checking\nmypy src/\n</code></pre> <pre><code># setup.cfg\n[flake8]\nmax-line-length = 100\nexclude = .git,__pycache__,venv\nignore = E203, W503\n\n[mypy]\npython_version = 3.9\nwarn_return_any = True\nwarn_unused_configs = True\ndisallow_untyped_defs = True\n\n[isort]\nprofile = black\nline_length = 100\n</code></pre> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.0.0\n    hooks:\n      - id: flake8\n</code></pre> <p>Install pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Expected: Clean, formatted code passing all quality checks</p>"},{"location":"Week6_Day29/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week6_Day29/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"Week6_Day29/#exercise-3-documentation-90-min","title":"Exercise 3: Documentation (90 min)","text":"<p>Create comprehensive documentation:</p> <pre><code># src/model.py\n\"\"\"\nModel architecture definitions and utilities.\n\nThis module contains the neural network architectures used for classification.\nSupports ResNet, custom CNNs, and LSTM-based models.\n\"\"\"\n\nfrom typing import Dict, Any\nimport torch\nimport torch.nn as nn\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"\n    LSTM-based sentiment classification model.\n\n    Architecture:\n        - Embedding layer\n        - Bi-directional LSTM\n        - Attention mechanism\n        - Fully connected layers with dropout\n\n    Args:\n        vocab_size (int): Size of vocabulary\n        embedding_dim (int): Dimension of word embeddings\n        hidden_dim (int): LSTM hidden dimension\n        num_classes (int): Number of output classes\n        dropout (float): Dropout probability\n\n    Example:\n        &gt;&gt;&gt; model = SentimentClassifier(\n        ...     vocab_size=10000,\n        ...     embedding_dim=100,\n        ...     hidden_dim=256,\n        ...     num_classes=2,\n        ...     dropout=0.5\n        ... )\n        &gt;&gt;&gt; output = model(torch.randint(0, 10000, (32, 200)))\n        &gt;&gt;&gt; output.shape\n        torch.Size([32, 2])\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int = 100,\n        hidden_dim: int = 256,\n        num_classes: int = 2,\n        dropout: float = 0.5\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x (torch.Tensor): Input tensor of token indices, shape (batch_size, seq_len)\n\n        Returns:\n            torch.Tensor: Class logits, shape (batch_size, num_classes)\n        \"\"\"\n        embedded = self.dropout(self.embedding(x))\n        lstm_out, _ = self.lstm(embedded)\n        pooled = lstm_out[:, -1, :]  # Take last hidden state\n        output = self.fc(pooled)\n        return output\n\ndef create_model(config: Dict[str, Any]) -&gt; nn.Module:\n    \"\"\"\n    Factory function to create model from config.\n\n    Args:\n        config: Configuration dictionary containing model parameters\n\n    Returns:\n        Initialized model\n\n    Raises:\n        ValueError: If architecture not supported\n    \"\"\"\n    arch = config.get('architecture', 'sentiment_classifier')\n\n    if arch == 'sentiment_classifier':\n        return SentimentClassifier(\n            vocab_size=config['vocab_size'],\n            embedding_dim=config.get('embedding_dim', 100),\n            hidden_dim=config.get('hidden_dim', 256),\n            num_classes=config['num_classes'],\n            dropout=config.get('dropout', 0.5)\n        )\n    else:\n        raise ValueError(f\"Unknown architecture: {arch}\")\n</code></pre> <p>README.md:</p> <pre><code># Sentiment Analysis API\n\nProduction-ready sentiment analysis model with REST API.\n\n## Features\n\n- 87% accuracy on IMDB dataset\n- FastAPI REST API\n- Docker containerization\n- MLflow experiment tracking\n- Comprehensive testing\n- Production monitoring\n\n## Quick Start\n\n### Installation\n```bash\npip install -r requirements.txt\n</code></pre>"},{"location":"Week6_Day29/#training","title":"Training","text":"<pre><code>python train_pipeline.py --config config.yaml\n</code></pre>"},{"location":"Week6_Day29/#api","title":"API","text":"<pre><code># Local\npython app.py\n\n# Docker\ndocker build -t sentiment-api .\ndocker run -p 8000:8000 sentiment-api\n</code></pre>"},{"location":"Week6_Day29/#api-usage","title":"API Usage","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"text\": \"This movie was amazing!\"}\n)\nprint(response.json())\n# {'sentiment': 'POSITIVE', 'confidence': 0.94}\n</code></pre>"},{"location":"Week6_Day29/#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data.py          # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 model.py         # Model architectures\n\u2502   \u251c\u2500\u2500 train.py         # Training logic\n\u2502   \u2514\u2500\u2500 utils.py         # Utilities\n\u251c\u2500\u2500 tests/               # Unit tests\n\u251c\u2500\u2500 app.py               # FastAPI application\n\u251c\u2500\u2500 train_pipeline.py    # Training pipeline\n\u251c\u2500\u2500 config.yaml          # Configuration\n\u251c\u2500\u2500 Dockerfile           # Docker image\n\u251c\u2500\u2500 requirements.txt     # Dependencies\n\u2514\u2500\u2500 README.md            # This file\n</code></pre>"},{"location":"Week6_Day29/#model-performance","title":"Model Performance","text":"Metric Value Accuracy 87.3% Precision 86.8% Recall 87.9% F1 Score 87.3%"},{"location":"Week6_Day29/#development","title":"Development","text":"<p>Run tests:</p> <pre><code>pytest tests/ -v --cov\n</code></pre> <p>Code quality:</p> <pre><code>black src/ tests/\nflake8 src/ tests/\n</code></pre>"},{"location":"Week6_Day29/#license","title":"License","text":"<p>MIT</p> <p>Expected: Professional documentation ready for portfolio</p>"},{"location":"Week6_Day29/#exercise-4-project-structure-refactoring-60-min","title":"Exercise 4: Project Structure Refactoring (60 min)","text":"<p>Organize code into proper structure:</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 dataset.py\n\u2502   \u2502   \u2514\u2500\u2500 preprocessing.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u2514\u2500\u2500 sentiment.py\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 trainer.py\n\u2502   \u2502   \u2514\u2500\u2500 callbacks.py\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 metrics.py\n\u2502       \u2514\u2500\u2500 visualization.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_data.py\n\u2502   \u251c\u2500\u2500 test_model.py\n\u2502   \u2514\u2500\u2500 test_api.py\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 app.py\n\u2502   \u2514\u2500\u2500 schemas.py\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 exploration.ipynb\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 config.yaml\n\u2502   \u2514\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 evaluate.py\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 API.md\n\u2502   \u2514\u2500\u2500 DEPLOYMENT.md\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 ci.yml\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"Week6_Day29/#exercise-5-final-integration-60-min","title":"Exercise 5: Final Integration (60 min)","text":"<p>Put everything together:</p> <pre><code># setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"sentiment-analysis-api\",\n    version=\"1.0.0\",\n    packages=find_packages(),\n    install_requires=[\n        \"torch&gt;=2.0.0\",\n        \"fastapi&gt;=0.104.0\",\n        \"uvicorn&gt;=0.24.0\",\n        \"mlflow&gt;=2.8.0\",\n        \"pytest&gt;=7.4.0\",\n    ],\n    author=\"Your Name\",\n    description=\"Production sentiment analysis API\",\n    python_requires=\"&gt;=3.9\",\n)\n</code></pre> <p>Install as package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"Week6_Day29/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Run all tests and ensure they pass \u2610 Review documentation completeness \u2610 Write daily reflection (choose 2-3 prompts)</p>"},{"location":"Week6_Day29/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>How do tests improve code reliability?</li> <li>What documentation practices will you adopt?</li> <li>How does code quality impact team collaboration?</li> <li>What was most challenging about refactoring?</li> <li>How would you onboard a new team member to your code?</li> <li>What production practices surprised you?</li> </ul>"},{"location":"Week6_Day29/#summary-checklist","title":"Summary Checklist","text":"<p>By end of day, you should have: - \u2610 Comprehensive test suite (&gt;80% coverage) - \u2610 Code quality tools configured - \u2610 Documentation complete (README, docstrings, API docs) - \u2610 Clean project structure - \u2610 Package installable with setup.py - \u2610 Pre-commit hooks working</p> <p>Next: Day 30 - Production Project</p>"},{"location":"Week6_Day30/","title":"Week 6, Day 30: Production Project - Deploy Your Week 5 Project","text":""},{"location":"Week6_Day30/#daily-goals","title":"Daily Goals","text":"<p>Take your Week 5 capstone project and deploy it as a production-ready system with API, monitoring, and professional deployment practices. This is where everything comes together!</p>"},{"location":"Week6_Day30/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"Week6_Day30/#project-assessment-planning-45-min","title":"Project Assessment &amp; Planning (45 min)","text":"<p>Review your Week 5 project and plan the production deployment.</p>"},{"location":"Week6_Day30/#what-youre-building-today","title":"What You're Building Today:","text":"<p>A complete production ML system with: - \u2705 REST API serving predictions - \u2705 Docker containerization - \u2705 Monitoring with Prometheus + Grafana - \u2705 Load testing results - \u2705 Production-ready code with tests - \u2705 Professional deployment documentation</p>"},{"location":"Week6_Day30/#assessment-checklist","title":"Assessment Checklist:","text":"<p>From Week 5, you should have: \u2610 Trained model saved as <code>.pth</code> or <code>.pkl</code> \u2610 Data preprocessing pipeline \u2610 Model architecture code \u2610 Training script \u2610 Basic evaluation script</p> <p>What we'll add today: \u2610 FastAPI application \u2610 Dockerfile and docker-compose.yml \u2610 Prometheus metrics \u2610 Health checks \u2610 API tests \u2610 Deployment documentation</p>"},{"location":"Week6_Day30/#api-development-25-hours","title":"API Development (2.5 hours)","text":"<p>Create <code>app.py</code> - your production FastAPI application.</p>"},{"location":"Week6_Day30/#basic-api-structure-45-min","title":"Basic API Structure (45 min)","text":"<pre><code># app.py\nfrom fastapi import FastAPI, HTTPException, File, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nimport torch\nimport logging\nfrom typing import List, Dict\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"ML Model API\",\n    description=\"Production API for [Your Project Name]\",\n    version=\"1.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Load model on startup\nmodel = None\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model when API starts.\"\"\"\n    global model\n    try:\n        # TODO: Load your model here\n        # model = YourModel()\n        # model.load_state_dict(torch.load('models/final_model.pth'))\n        # model.to(device)\n        # model.eval()\n        logger.info(\"Model loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint with API information.\"\"\"\n    return {\n        \"message\": \"ML Model API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"health\": \"/health\",\n            \"predict\": \"/predict\",\n            \"batch_predict\": \"/batch_predict\",\n            \"metrics\": \"/metrics\"\n        }\n    }\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": model is not None,\n        \"device\": str(device)\n    }\n</code></pre>"},{"location":"Week6_Day30/#track-specific-prediction-endpoints-60-min","title":"Track-Specific Prediction Endpoints (60 min)","text":"<p>Track 1 (Images) - Prediction Endpoint:</p> <pre><code>from PIL import Image\nimport io\nfrom torchvision import transforms\n\n# Define preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\nclass PredictionResponse(BaseModel):\n    prediction: str\n    confidence: float\n    probabilities: Dict[str, float]\n    processing_time_ms: float\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Predict class for uploaded image.\n\n    Args:\n        file: Image file (JPEG, PNG)\n\n    Returns:\n        Prediction with confidence scores\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Read and preprocess image\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        image_tensor = preprocess(image).unsqueeze(0).to(device)\n\n        # Make prediction\n        with torch.no_grad():\n            outputs = model(image_tensor)\n            probabilities = torch.softmax(outputs, dim=1)[0]\n            predicted_class = probabilities.argmax().item()\n            confidence = probabilities[predicted_class].item()\n\n        # Class names (adjust for your project)\n        class_names = ['NORMAL', 'PNEUMONIA']  # Track 1 example\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return PredictionResponse(\n            prediction=class_names[predicted_class],\n            confidence=float(confidence),\n            probabilities={\n                name: float(prob) \n                for name, prob in zip(class_names, probabilities.cpu().numpy())\n            },\n            processing_time_ms=processing_time\n        )\n\n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre> <p>Track 2 (Text) - Prediction Endpoint:</p> <pre><code>class TextInput(BaseModel):\n    text: str\n\nclass PredictionResponse(BaseModel):\n    prediction: str\n    confidence: float\n    probabilities: Dict[str, float]\n    processing_time_ms: float\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(input_data: TextInput):\n    \"\"\"\n    Predict sentiment for input text.\n\n    Args:\n        input_data: Text to classify\n\n    Returns:\n        Prediction with confidence scores\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Preprocess text (use your vocabulary from training)\n        # encoded = vocab.encode(input_data.text, max_len=200)\n        # input_tensor = torch.tensor([encoded]).to(device)\n\n        # Make prediction\n        with torch.no_grad():\n            outputs = model(input_tensor)\n            probabilities = torch.softmax(outputs, dim=1)[0]\n            predicted_class = probabilities.argmax().item()\n            confidence = probabilities[predicted_class].item()\n\n        class_names = ['Negative', 'Positive']\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return PredictionResponse(\n            prediction=class_names[predicted_class],\n            confidence=float(confidence),\n            probabilities={\n                name: float(prob) \n                for name, prob in zip(class_names, probabilities.cpu().numpy())\n            },\n            processing_time_ms=processing_time\n        )\n\n    except Exception as e:\n        logger.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre> <p>Track 3 (Stock) - Prediction Endpoint:</p> <pre><code>class StockInput(BaseModel):\n    features: List[float]  # Technical indicators\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(input_data: StockInput):\n    \"\"\"\n    Predict stock price movement.\n\n    Args:\n        input_data: Technical indicators\n\n    Returns:\n        Prediction (Up/Down) with confidence\n    \"\"\"\n    # Similar structure, adjust for your model input\n    pass\n</code></pre>"},{"location":"Week6_Day30/#add-monitoring-45-min","title":"Add Monitoring (45 min)","text":"<pre><code>from prometheus_client import Counter, Histogram, generate_latest\nfrom fastapi.responses import Response\n\n# Prometheus metrics\nREQUEST_COUNT = Counter(\n    'api_requests_total',\n    'Total API requests',\n    ['endpoint', 'method', 'status']\n)\n\nREQUEST_LATENCY = Histogram(\n    'api_request_duration_seconds',\n    'API request latency',\n    ['endpoint']\n)\n\nPREDICTION_COUNT = Counter(\n    'predictions_total',\n    'Total predictions made',\n    ['predicted_class']\n)\n\n@app.middleware(\"http\")\nasync def add_metrics(request, call_next):\n    \"\"\"Add Prometheus metrics to all requests.\"\"\"\n    start_time = time.time()\n\n    response = await call_next(request)\n\n    # Record metrics\n    duration = time.time() - start_time\n    REQUEST_COUNT.labels(\n        endpoint=request.url.path,\n        method=request.method,\n        status=response.status_code\n    ).inc()\n    REQUEST_LATENCY.labels(endpoint=request.url.path).observe(duration)\n\n    return response\n\n@app.get(\"/metrics\")\nasync def metrics():\n    \"\"\"Prometheus metrics endpoint.\"\"\"\n    return Response(generate_latest(), media_type=\"text/plain\")\n</code></pre> <p>\u2610 FastAPI application created \u2610 Prediction endpoint implemented \u2610 Monitoring added \u2610 API tested locally</p>"},{"location":"Week6_Day30/#testing-your-api-30-min","title":"Testing Your API (30 min)","text":"<p>Create <code>test_api.py</code>:</p> <pre><code>from fastapi.testclient import TestClient\nfrom app import app\n\nclient = TestClient(app)\n\ndef test_root():\n    \"\"\"Test root endpoint.\"\"\"\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert \"message\" in response.json()\n\ndef test_health():\n    \"\"\"Test health check.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"healthy\"\n\ndef test_predict():\n    \"\"\"Test prediction endpoint.\"\"\"\n    # For images:\n    with open(\"test_image.jpg\", \"rb\") as f:\n        response = client.post(\n            \"/predict\",\n            files={\"file\": (\"test.jpg\", f, \"image/jpeg\")}\n        )\n\n    # For text:\n    # response = client.post(\"/predict\", json={\"text\": \"This is a test\"})\n\n    assert response.status_code == 200\n    assert \"prediction\" in response.json()\n    assert \"confidence\" in response.json()\n\nif __name__ == \"__main__\":\n    test_root()\n    test_health()\n    test_predict()\n    print(\"\u2713 All tests passed!\")\n</code></pre> <p>Run tests:</p> <pre><code>python test_api.py\n# or\npytest test_api.py -v\n</code></pre> <p>\u2610 API tests created \u2610 All tests passing  </p>"},{"location":"Week6_Day30/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"Week6_Day30/#docker-containerization-15-hours","title":"Docker Containerization (1.5 hours)","text":""},{"location":"Week6_Day30/#create-dockerfile-30-min","title":"Create Dockerfile (30 min)","text":"<pre><code># Dockerfile\nFROM python:3.9-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app.py .\nCOPY models/ models/\nCOPY src/ src/\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=40s \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Run application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"Week6_Day30/#create-docker-composeyml-30-min","title":"Create docker-compose.yml (30 min)","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./models:/app/models\n    environment:\n      - PYTHONUNBUFFERED=1\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n    restart: unless-stopped\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus-data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n    restart: unless-stopped\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana-data:/var/lib/grafana\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n      - GF_USERS_ALLOW_SIGN_UP=false\n    restart: unless-stopped\n\nvolumes:\n  prometheus-data:\n  grafana-data:\n</code></pre>"},{"location":"Week6_Day30/#create-prometheusyml-15-min","title":"Create prometheus.yml (15 min)","text":"<pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'ml-api'\n    static_configs:\n      - targets: ['api:8000']\n</code></pre>"},{"location":"Week6_Day30/#build-and-test-15-min","title":"Build and Test (15 min)","text":"<pre><code># Build Docker image\ndocker-compose build\n\n# Start services\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f api\n\n# Test API\ncurl http://localhost:8000/health\n\n# Stop services\ndocker-compose down\n</code></pre> <p>\u2610 Dockerfile created \u2610 docker-compose.yml created \u2610 Services running successfully \u2610 API accessible in Docker</p>"},{"location":"Week6_Day30/#load-testing-45-min","title":"Load Testing (45 min)","text":"<p>Create <code>load_test.py</code>:</p> <pre><code>import requests\nimport time\nimport concurrent.futures\nfrom statistics import mean, median\nimport matplotlib.pyplot as plt\n\ndef test_prediction(session, url, test_data):\n    \"\"\"Make a single prediction request.\"\"\"\n    start = time.time()\n    try:\n        response = session.post(url, **test_data)\n        latency = time.time() - start\n        return {\n            'success': response.status_code == 200,\n            'latency': latency,\n            'status': response.status_code\n        }\n    except Exception as e:\n        return {'success': False, 'latency': None, 'status': None}\n\ndef load_test(url, test_data, num_requests=100, num_workers=10):\n    \"\"\"Run load test with concurrent requests.\"\"\"\n    print(f\"Running load test: {num_requests} requests, {num_workers} workers\")\n\n    session = requests.Session()\n    results = []\n\n    start_time = time.time()\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [\n            executor.submit(test_prediction, session, url, test_data)\n            for _ in range(num_requests)\n        ]\n\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    total_time = time.time() - start_time\n\n    # Calculate metrics\n    successful = [r for r in results if r['success']]\n    latencies = [r['latency'] for r in successful]\n\n    print(f\"\\n=== Load Test Results ===\")\n    print(f\"Total requests: {num_requests}\")\n    print(f\"Successful: {len(successful)} ({len(successful)/num_requests*100:.1f}%)\")\n    print(f\"Failed: {num_requests - len(successful)}\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Throughput: {num_requests/total_time:.2f} req/s\")\n    print(f\"\\nLatency:\")\n    print(f\"  Mean: {mean(latencies)*1000:.2f}ms\")\n    print(f\"  Median: {median(latencies)*1000:.2f}ms\")\n    print(f\"  Min: {min(latencies)*1000:.2f}ms\")\n    print(f\"  Max: {max(latencies)*1000:.2f}ms\")\n\n    # Plot latency distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist([l*1000 for l in latencies], bins=30)\n    plt.xlabel('Latency (ms)')\n    plt.ylabel('Frequency')\n    plt.title('Request Latency Distribution')\n    plt.savefig('load_test_results.png')\n    print(f\"\\n\u2713 Results saved to load_test_results.png\")\n\nif __name__ == \"__main__\":\n    # For image API:\n    # with open('test_image.jpg', 'rb') as f:\n    #     test_data = {'files': {'file': ('test.jpg', f, 'image/jpeg')}}\n\n    # For text API:\n    test_data = {'json': {'text': 'This is a test review'}}\n\n    load_test(\n        'http://localhost:8000/predict',\n        test_data,\n        num_requests=100,\n        num_workers=10\n    )\n</code></pre> <p>Run load test:</p> <pre><code>python load_test.py\n</code></pre> <p>\u2610 Load testing completed \u2610 Performance metrics documented \u2610 Identified any bottlenecks</p>"},{"location":"Week6_Day30/#monitoring-setup-45-min","title":"Monitoring Setup (45 min)","text":""},{"location":"Week6_Day30/#configure-grafana-dashboard-30-min","title":"Configure Grafana Dashboard (30 min)","text":"<ol> <li>Access Grafana: http://localhost:3000</li> <li>Login: admin / admin</li> <li>Add Prometheus data source:</li> <li>URL: http://prometheus:9090</li> <li>Create dashboard with panels:</li> <li>Request rate</li> <li>Latency (p50, p95, p99)</li> <li>Error rate</li> <li>Prediction counts by class</li> </ol>"},{"location":"Week6_Day30/#create-monitoring-documentation-15-min","title":"Create Monitoring Documentation (15 min)","text":"<p>Create <code>MONITORING.md</code>:</p> <pre><code># Monitoring Guide\n\n## Accessing Services\n\n- **API**: http://localhost:8000\n- **API Docs**: http://localhost:8000/docs\n- **Prometheus**: http://localhost:9090\n- **Grafana**: http://localhost:3000\n\n## Available Metrics\n\n### API Metrics\n- `api_requests_total`: Total requests by endpoint, method, status\n- `api_request_duration_seconds`: Request latency histogram\n- `predictions_total`: Total predictions by class\n\n### Key Dashboards\n\n**Request Overview**:\n- Requests per second\n- Success rate\n- Error rate\n\n**Performance**:\n- P50, P95, P99 latency\n- Average response time\n- Throughput\n\n**Model Performance**:\n- Predictions by class\n- Confidence distribution\n\n## Alerts\n\nSet up alerts for:\n- Error rate &gt; 5%\n- P95 latency &gt; 500ms\n- Low throughput &lt; 10 req/s\n</code></pre> <p>\u2610 Grafana configured \u2610 Dashboard created \u2610 Monitoring documentation written</p>"},{"location":"Week6_Day30/#deployment-documentation-45-min","title":"Deployment Documentation (45 min)","text":"<p>Create comprehensive <code>DEPLOYMENT.md</code>:</p> <pre><code># Deployment Guide\n\n## Local Development\n\n### Prerequisites\n- Python 3.9+\n- Docker &amp; Docker Compose\n- 4GB+ RAM\n\n### Setup\n\\```bash\n# Clone repository\ngit clone [your-repo]\ncd [your-project]\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run API locally\nuvicorn app:app --reload\n\n# Or with Docker\ndocker-compose up\n\\```\n\n## Production Deployment\n\n### Cloud Deployment Options\n\n**Option 1: AWS ECS/Fargate**\n\\```bash\n# Build and push Docker image\ndocker build -t ml-api:latest .\ndocker tag ml-api:latest [AWS-ACCOUNT].dkr.ecr.[REGION].amazonaws.com/ml-api:latest\ndocker push [AWS-ACCOUNT].dkr.ecr.[REGION].amazonaws.com/ml-api:latest\n\n# Deploy to ECS (configure task definition and service)\n\\```\n\n**Option 2: Google Cloud Run**\n\\```bash\n# Build and deploy\ngcloud builds submit --tag gcr.io/[PROJECT-ID]/ml-api\ngcloud run deploy ml-api --image gcr.io/[PROJECT-ID]/ml-api --platform managed\n\\```\n\n**Option 3: Kubernetes**\n\\```bash\n# Apply Kubernetes manifests\nkubectl apply -f k8s/deployment.yaml\nkubectl apply -f k8s/service.yaml\n\\```\n\n## Environment Variables\n\n\\```bash\nMODEL_PATH=/app/models/final_model.pth\nLOG_LEVEL=INFO\nMAX_WORKERS=4\n\\```\n\n## Health Checks\n\n- Endpoint: `/health`\n- Expected response: `{\"status\": \"healthy\"}`\n- Timeout: 3 seconds\n\n## Scaling\n\n**Horizontal scaling**: Add more replicas/instances\n\\```bash\ndocker-compose up --scale api=3\n\\```\n\n**Vertical scaling**: Increase container resources\n\\```yaml\nresources:\n  limits:\n    memory: 2Gi\n    cpu: 1000m\n\\```\n\n## Troubleshooting\n\n**API not responding**:\n- Check logs: `docker-compose logs api`\n- Verify health: `curl http://localhost:8000/health`\n\n**High latency**:\n- Check resource usage\n- Review Grafana dashboards\n- Consider scaling\n\n**Model loading errors**:\n- Verify model file exists\n- Check model path in code\n- Ensure sufficient memory\n</code></pre> <p>\u2610 Deployment documentation complete \u2610 All deployment options documented \u2610 Troubleshooting guide included</p>"},{"location":"Week6_Day30/#final-project-checklist","title":"Final Project Checklist","text":""},{"location":"Week6_Day30/#code-quality","title":"Code Quality","text":"<p>\u2610 All code follows PEP 8 style \u2610 Docstrings for all functions \u2610 Type hints used throughout \u2610 No hardcoded values (use config) \u2610 Error handling implemented</p>"},{"location":"Week6_Day30/#testing","title":"Testing","text":"<p>\u2610 API tests passing \u2610 Load tests completed \u2610 Performance benchmarked \u2610 Edge cases tested</p>"},{"location":"Week6_Day30/#docker","title":"Docker","text":"<p>\u2610 Dockerfile optimized \u2610 docker-compose.yml complete \u2610 Health checks configured \u2610 Services start successfully</p>"},{"location":"Week6_Day30/#monitoring","title":"Monitoring","text":"<p>\u2610 Prometheus metrics exposed \u2610 Grafana dashboard created \u2610 Logs properly formatted \u2610 Alerts configured</p>"},{"location":"Week6_Day30/#documentation","title":"Documentation","text":"<p>\u2610 README.md comprehensive \u2610 DEPLOYMENT.md detailed \u2610 MONITORING.md clear \u2610 API documentation (FastAPI auto-docs)</p>"},{"location":"Week6_Day30/#deployment","title":"Deployment","text":"<p>\u2610 Runs locally with Docker \u2610 Load tested successfully \u2610 Monitoring working \u2610 Ready for cloud deployment</p>"},{"location":"Week6_Overview/","title":"Week 6 Overview: Production Machine Learning (OPTIONAL)","text":""},{"location":"Week6_Overview/#introduction","title":"Introduction","text":"<p>Week 6 is an optional extension for students who want to learn production ML deployment. This week covers model optimization, API development, MLOps practices, and professional software engineering for ML. Consider this week if you plan to deploy ML systems in production environments.</p>"},{"location":"Week6_Overview/#week-goals","title":"Week Goals","text":"<ul> <li>Optimize models for production (quantization, pruning, ONNX export)</li> <li>Deploy models as REST APIs using FastAPI and Docker</li> <li>Apply MLOps fundamentals (experiment tracking, model versioning, monitoring)</li> <li>Write production-quality code with testing and documentation</li> <li>Build and deploy a complete production ML system</li> <li>Understand the full ML lifecycle from training to deployment</li> </ul>"},{"location":"Week6_Overview/#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Day 26: Model Optimization - Quantization, Pruning, ONNX</li> <li>Day 27: Model Deployment - FastAPI, Docker, Cloud Platforms</li> <li>Day 28: MLOps Fundamentals - Experiment Tracking, Versioning, Pipelines</li> <li>Day 29: Best Practices - Testing, Documentation, Code Quality</li> <li>Day 30: Production Project - Deploy Your Week 5 Project</li> </ul>"},{"location":"Week6_Overview/#prerequisites","title":"Prerequisites","text":"<p>This is an advanced optional week. You should have:</p> <ul> <li>Completed Weeks 1-5 successfully</li> <li>Working ML project from Week 5</li> <li>Strong Python skills</li> <li>Basic familiarity with command line</li> <li>Interest in production ML engineering</li> </ul>"},{"location":"Week6_Overview/#who-should-take-this-week","title":"Who Should Take This Week","text":"<p>This week is ideal if you:</p> <ul> <li>Want to become an ML Engineer (not just data scientist)</li> <li>Plan to deploy models in production</li> <li>Are interested in MLOps and DevOps</li> <li>Want to build end-to-end ML systems</li> <li>Aim for industry ML roles</li> </ul>"},{"location":"Week6_Overview/#who-can-skip-this-week","title":"Who Can Skip This Week","text":"<p>You can skip this week if you:</p> <ul> <li>Are focused purely on research/academia</li> <li>Prefer exploratory data science roles</li> <li>Want to move on to other ML topics</li> <li>Are time-constrained</li> </ul>"},{"location":"Week6_Overview/#success-criteria","title":"Success Criteria","text":"<p>By the end of Week 6, you should have:</p> <ul> <li>Optimized ML model (4x smaller, faster inference)</li> <li>Working REST API serving predictions</li> <li>Dockerized application</li> <li>Monitoring dashboard</li> <li>Production-ready code with tests</li> <li>Deployed system accessible via URL</li> </ul> <p>Ready to start? Begin with Day 26: Model Optimization</p> <p>Want to skip? That's fine! Your Week 5 project is already portfolio-ready.</p>"},{"location":"old/Week3_Day11/","title":"Week 3, Day 11: CNN Theory - Convolutions, Filters, Feature Maps","text":""},{"location":"old/Week3_Day11/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand what convolutions are and why they work for images</li> <li>Learn about filters/kernels and feature detection</li> <li>Master stride, padding, and output size calculations</li> <li>Understand pooling operations</li> <li>Implement 2D convolution from scratch</li> <li>Build first CNN in PyTorch</li> <li>Visualize learned features</li> </ul>"},{"location":"old/Week3_Day11/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week3_Day11/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week3_Day11/#video-learning-2-hours","title":"Video Learning (2 hours)","text":"<p>\u2610 Watch: But what is a convolution? by 3Blue1Brown (20 min) THE essential video for understanding convolutions visually</p> <p>\u2610 Watch: Convolutional Neural Networks (CNNs) explained by deeplizard (15 min) Clear explanation of CNN components</p> <p>\u2610 Watch: CNNs Part 1 - Convolution by StatQuest (20 min) Detailed breakdown of convolution operation</p> <p>\u2610 Watch: CNNs Part 2 - Pooling by StatQuest (15 min) Understanding pooling layers</p> <p>\u2610 Watch: Visualizing Convolutional Networks by Stanford CS231n (20 min) See what CNNs actually learn</p> <p>\u2610 Watch: CNN Architectures by Lex Fridman (15 min) Overview of evolution</p>"},{"location":"old/Week3_Day11/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 7.1 - From Fully Connected to Convolutional</p> <p>\u2610 Read: D2L Chapter 7.2 - Convolutions for Images</p> <p>\u2610 Read: D2L Chapter 7.3 - Padding and Stride</p> <p>\u2610 Read: D2L Chapter 7.4 - Pooling</p>"},{"location":"old/Week3_Day11/#hands-on-coding-part-1-15-hours","title":"Hands-on Coding - Part 1 (1.5 hours)","text":""},{"location":"old/Week3_Day11/#setup-10-min","title":"Setup (10 min)","text":"<pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n# Set random seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Week 3, Day 11: Convolutional Neural Networks\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"old/Week3_Day11/#exercise-1-understanding-convolution-operation-45-min","title":"Exercise 1: Understanding Convolution Operation (45 min)","text":"<p>Learn convolution through manual calculation and visualization:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 1: MANUAL CONVOLUTION\")\nprint(\"=\"*70)\n\n# Simple 1D convolution first\nprint(\"\\n1. 1D Convolution Example\")\nprint(\"-\" * 40)\n\nsignal_1d = np.array([1, 2, 3, 4, 5])\nkernel_1d = np.array([0, 1, 0.5])\n\nprint(f\"Signal: {signal_1d}\")\nprint(f\"Kernel: {kernel_1d}\")\n\n# Manual convolution\ndef convolve_1d_manual(signal, kernel):\n    \"\"\"Manually compute 1D convolution\"\"\"\n    n = len(signal)\n    k = len(kernel)\n    output_size = n - k + 1\n    output = np.zeros(output_size)\n\n    for i in range(output_size):\n        output[i] = np.sum(signal[i:i+k] * kernel)\n\n    return output\n\nresult_manual = convolve_1d_manual(signal_1d, kernel_1d)\nprint(f\"\\nManual result: {result_manual}\")\n\n# Using scipy\nresult_scipy = signal.correlate(signal_1d, kernel_1d, mode='valid')\nprint(f\"SciPy result:  {result_scipy}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].stem(signal_1d)\naxes[0].set_title('Input Signal')\naxes[0].set_xlabel('Position')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].stem(kernel_1d)\naxes[1].set_title('Kernel/Filter')\naxes[1].set_xlabel('Position')\naxes[1].grid(True, alpha=0.3)\n\naxes[2].stem(result_manual)\naxes[2].set_title('Convolution Output')\naxes[2].set_xlabel('Position')\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('1D Convolution', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# 2D convolution - the image case\nprint(\"\\n2. 2D Convolution (Images)\")\nprint(\"-\" * 40)\n\n# Create simple image\nimage = np.array([\n    [1, 2, 3, 0],\n    [0, 1, 2, 3],\n    [3, 0, 1, 2],\n    [2, 3, 0, 1]\n], dtype=float)\n\n# Edge detection kernel\nedge_kernel = np.array([\n    [-1, -1, -1],\n    [-1,  8, -1],\n    [-1, -1, -1]\n], dtype=float)\n\nprint(f\"Image shape: {image.shape}\")\nprint(f\"Kernel shape: {edge_kernel.shape}\")\n\ndef convolve_2d_manual(image, kernel):\n    \"\"\"\n    Manually compute 2D convolution\n\n    Args:\n        image: 2D array (H, W)\n        kernel: 2D array (K, K)\n\n    Returns:\n        output: 2D array (H-K+1, W-K+1)\n    \"\"\"\n    h, w = image.shape\n    kh, kw = kernel.shape\n\n    output_h = h - kh + 1\n    output_w = w - kw + 1\n    output = np.zeros((output_h, output_w))\n\n    print(f\"\\nOutput size: {output.shape}\")\n    print(\"\\nStep-by-step convolution:\")\n\n    for i in range(output_h):\n        for j in range(output_w):\n            # Extract region\n            region = image[i:i+kh, j:j+kw]\n            # Element-wise multiply and sum\n            output[i, j] = np.sum(region * kernel)\n\n            if i == 0 and j == 0:\n                print(f\"\\nPosition (0, 0):\")\n                print(f\"Region:\\n{region}\")\n                print(f\"Kernel:\\n{kernel}\")\n                print(f\"Element-wise product:\\n{region * kernel}\")\n                print(f\"Sum: {output[i, j]}\")\n\n    return output\n\n# Apply convolution\noutput = convolve_2d_manual(image, edge_kernel)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nim0 = axes[0].imshow(image, cmap='gray')\naxes[0].set_title('Input Image (4\u00d74)')\naxes[0].axis('off')\nplt.colorbar(im0, ax=axes[0])\n\nim1 = axes[1].imshow(edge_kernel, cmap='RdBu', vmin=-1, vmax=8)\naxes[1].set_title('Edge Detection Kernel (3\u00d73)')\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1])\n\nim2 = axes[2].imshow(output, cmap='gray')\naxes[2].set_title('Output Feature Map (2\u00d72)')\naxes[2].axis('off')\nplt.colorbar(im2, ax=axes[2])\n\n# Annotate output with values\nfor i in range(output.shape[0]):\n    for j in range(output.shape[1]):\n        axes[2].text(j, i, f'{output[i,j]:.0f}', \n                    ha='center', va='center', color='red', fontsize=12)\n\nplt.suptitle('2D Convolution Step-by-Step', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Manual convolution complete\")\n</code></pre>"},{"location":"old/Week3_Day11/#exercise-2-common-image-filters-45-min","title":"Exercise 2: Common Image Filters (45 min)","text":"<p>Explore different filters and their effects:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: IMAGE FILTERS\")\nprint(\"=\"*70)\n\n# Load a sample image (or create one)\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Download sample image\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/300px-Cat03.jpg\"\nresponse = requests.get(url)\nsample_image = Image.open(BytesIO(response.content)).convert('L')  # Grayscale\nsample_image = sample_image.resize((128, 128))\nimg_array = np.array(sample_image, dtype=float) / 255.0\n\nprint(f\"Image shape: {img_array.shape}\")\n\n# Define common filters\nfilters = {\n    'Identity': np.array([[0, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 0]]),\n\n    'Blur': (1/9) * np.array([[1, 1, 1],\n                              [1, 1, 1],\n                              [1, 1, 1]]),\n\n    'Edge Detection': np.array([[-1, -1, -1],\n                               [-1,  8, -1],\n                               [-1, -1, -1]]),\n\n    'Sharpen': np.array([[ 0, -1,  0],\n                        [-1,  5, -1],\n                        [ 0, -1,  0]]),\n\n    'Horizontal Edge': np.array([[-1, -1, -1],\n                                [ 0,  0,  0],\n                                [ 1,  1,  1]]),\n\n    'Vertical Edge': np.array([[-1, 0, 1],\n                              [-1, 0, 1],\n                              [-1, 0, 1]]),\n}\n\n# Apply filters\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\naxes = axes.flatten()\n\n# Original image\naxes[0].imshow(img_array, cmap='gray')\naxes[0].set_title('Original Image', fontsize=12, fontweight='bold')\naxes[0].axis('off')\n\n# Apply each filter\nfor idx, (name, kernel) in enumerate(filters.items(), 1):\n    # Convolve\n    filtered = signal.correlate2d(img_array, kernel, mode='same', boundary='symm')\n\n    axes[idx].imshow(filtered, cmap='gray')\n    axes[idx].set_title(f'{name}', fontsize=12)\n    axes[idx].axis('off')\n\n    # Show kernel in corner\n    axins = axes[idx].inset_axes([0.7, 0.7, 0.25, 0.25])\n    axins.imshow(kernel, cmap='RdBu', vmin=-2, vmax=5)\n    axins.axis('off')\n\n# Hide extra subplots\nfor idx in range(len(filters) + 1, len(axes)):\n    axes[idx].axis('off')\n\nplt.suptitle('Common Image Filters', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Image filters explored\")\n</code></pre>"},{"location":"old/Week3_Day11/#exercise-3-stride-and-padding-30-min","title":"Exercise 3: Stride and Padding (30 min)","text":"<p>Understand how stride and padding affect output size:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: STRIDE AND PADDING\")\nprint(\"=\"*70)\n\ndef calculate_output_size(input_size, kernel_size, stride, padding):\n    \"\"\"\n    Calculate output size of convolution\n\n    Formula: output_size = floor((input_size + 2*padding - kernel_size) / stride) + 1\n    \"\"\"\n    return ((input_size + 2*padding - kernel_size) // stride) + 1\n\n# Example calculations\nprint(\"\\nOutput Size Calculations:\")\nprint(\"-\" * 60)\nprint(f\"{'Input':&gt;6} | {'Kernel':&gt;6} | {'Stride':&gt;6} | {'Padding':&gt;7} | {'Output':&gt;6}\")\nprint(\"-\" * 60)\n\nconfigs = [\n    (28, 3, 1, 0),  # MNIST with 3\u00d73, no padding\n    (28, 3, 1, 1),  # MNIST with 3\u00d73, padding=1 (same size)\n    (28, 5, 1, 0),  # MNIST with 5\u00d75, no padding\n    (28, 3, 2, 0),  # MNIST with stride=2 (downsampling)\n    (32, 3, 1, 1),  # CIFAR-10 with 3\u00d73, padding=1\n    (224, 7, 2, 3), # ImageNet first layer (ResNet)\n]\n\nfor input_s, kernel_s, stride, padding in configs:\n    output_s = calculate_output_size(input_s, kernel_s, stride, padding)\n    print(f\"{input_s:&gt;6} | {kernel_s:&gt;6} | {stride:&gt;6} | {padding:&gt;7} | {output_s:&gt;6}\")\n\n# Visualize stride effect\nprint(\"\\nVisualizing Stride:\")\nprint(\"-\" * 40)\n\nimage_small = np.random.rand(6, 6)\nkernel_small = np.ones((3, 3)) / 9  # Average filter\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Stride = 1\noutput_s1 = np.zeros((4, 4))\nfor i in range(4):\n    for j in range(4):\n        output_s1[i, j] = np.sum(image_small[i:i+3, j:j+3] * kernel_small)\n\naxes[0].imshow(output_s1, cmap='viridis')\naxes[0].set_title('Stride = 1\\nOutput: 4\u00d74')\naxes[0].axis('off')\n\n# Stride = 2\noutput_s2 = np.zeros((2, 2))\nfor i in range(2):\n    for j in range(2):\n        output_s2[i, j] = np.sum(image_small[i*2:i*2+3, j*2:j*2+3] * kernel_small)\n\naxes[1].imshow(output_s2, cmap='viridis')\naxes[1].set_title('Stride = 2\\nOutput: 2\u00d72')\naxes[1].axis('off')\n\n# Stride = 3\noutput_s3 = np.zeros((2, 2))\ncount = 0\nfor i in range(2):\n    for j in range(2):\n        if i*3+3 &lt;= 6 and j*3+3 &lt;= 6:\n            output_s3[i, j] = np.sum(image_small[i*3:i*3+3, j*3:j*3+3] * kernel_small)\n            count += 1\n\naxes[2].imshow(output_s3, cmap='viridis')\naxes[2].set_title('Stride = 3\\nOutput: 2\u00d72')\naxes[2].axis('off')\n\nplt.suptitle(f'Effect of Stride (Input: 6\u00d76, Kernel: 3\u00d73)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize padding\nprint(\"\\nVisualizing Padding:\")\nprint(\"-\" * 40)\n\nimage_tiny = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]], dtype=float)\n\n# No padding\nimage_nopad = image_tiny\n\n# Padding = 1\nimage_pad1 = np.pad(image_tiny, pad_width=1, mode='constant', constant_values=0)\n\n# Padding = 2\nimage_pad2 = np.pad(image_tiny, pad_width=2, mode='constant', constant_values=0)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(image_nopad, cmap='viridis', vmin=0, vmax=9)\naxes[0].set_title('No Padding\\nSize: 3\u00d73')\nfor i in range(3):\n    for j in range(3):\n        axes[0].text(j, i, f'{image_nopad[i,j]:.0f}', \n                    ha='center', va='center', color='white', fontsize=12)\naxes[0].axis('off')\n\naxes[1].imshow(image_pad1, cmap='viridis', vmin=0, vmax=9)\naxes[1].set_title('Padding = 1\\nSize: 5\u00d75')\naxes[1].axis('off')\n\naxes[2].imshow(image_pad2, cmap='viridis', vmin=0, vmax=9)\naxes[2].set_title('Padding = 2\\nSize: 7\u00d77')\naxes[2].axis('off')\n\nplt.suptitle('Effect of Padding', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key Insights:\")\nprint(\"- Stride &gt; 1: Downsamples the output (reduces spatial dimensions)\")\nprint(\"- Padding: Preserves spatial dimensions and edge information\")\nprint(\"- 'Same' padding: Output size = Input size (when stride=1)\")\nprint(\"- 'Valid' padding: No padding, output shrinks\")\n\nprint(\"\\n\u2713 Stride and padding understood\")\n</code></pre>"},{"location":"old/Week3_Day11/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week3_Day11/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Review: Replay key sections from morning videos as needed</p> <p>\u2610 Watch: Understanding CNNs with practical code review (15 min)</p>"},{"location":"old/Week3_Day11/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week3_Day11/#exercise-4-pooling-operations-40-min","title":"Exercise 4: Pooling Operations (40 min)","text":"<p>Learn how pooling reduces spatial dimensions:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: POOLING OPERATIONS\")\nprint(\"=\"*70)\n\n# Max pooling\ndef max_pool_2d(image, pool_size=2, stride=2):\n    \"\"\"\n    Apply max pooling\n\n    Args:\n        image: 2D array\n        pool_size: size of pooling window\n        stride: stride for pooling\n\n    Returns:\n        pooled: downsampled array\n    \"\"\"\n    h, w = image.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n\n    pooled = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[i*stride:i*stride+pool_size, \n                          j*stride:j*stride+pool_size]\n            pooled[i, j] = np.max(region)\n\n    return pooled\n\n# Average pooling\ndef avg_pool_2d(image, pool_size=2, stride=2):\n    \"\"\"Apply average pooling\"\"\"\n    h, w = image.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n\n    pooled = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[i*stride:i*stride+pool_size,\n                          j*stride:j*stride+pool_size]\n            pooled[i, j] = np.mean(region)\n\n    return pooled\n\n# Test on image\ntest_image = np.array([\n    [1, 3, 2, 4],\n    [5, 6, 1, 3],\n    [2, 1, 4, 2],\n    [3, 5, 2, 1]\n], dtype=float)\n\nmax_pooled = max_pool_2d(test_image, pool_size=2, stride=2)\navg_pooled = avg_pool_2d(test_image, pool_size=2, stride=2)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nim0 = axes[0].imshow(test_image, cmap='viridis')\naxes[0].set_title('Original (4\u00d74)')\nfor i in range(4):\n    for j in range(4):\n        axes[0].text(j, i, f'{test_image[i,j]:.0f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[0].axis('off')\nplt.colorbar(im0, ax=axes[0])\n\n# Draw pooling regions\nfor i in range(0, 4, 2):\n    for j in range(0, 4, 2):\n        rect = plt.Rectangle((j-0.5, i-0.5), 2, 2, \n                             fill=False, edgecolor='red', linewidth=2)\n        axes[0].add_patch(rect)\n\nim1 = axes[1].imshow(max_pooled, cmap='viridis')\naxes[1].set_title('Max Pool 2\u00d72 (2\u00d72)')\nfor i in range(2):\n    for j in range(2):\n        axes[1].text(j, i, f'{max_pooled[i,j]:.0f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1])\n\nim2 = axes[2].imshow(avg_pooled, cmap='viridis')\naxes[2].set_title('Avg Pool 2\u00d72 (2\u00d72)')\nfor i in range(2):\n    for j in range(2):\n        axes[2].text(j, i, f'{avg_pooled[i,j]:.1f}',\n                    ha='center', va='center', color='white', fontsize=14)\naxes[2].axis('off')\nplt.colorbar(im2, ax=axes[2])\n\nplt.suptitle('Pooling Operations', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key Differences:\")\nprint(\"- Max Pool: Takes maximum value (preserves strong features)\")\nprint(\"- Avg Pool: Takes average (smooths features)\")\nprint(\"- Both reduce spatial dimensions \u2192 fewer parameters \u2192 faster\")\nprint(\"- Pooling provides translation invariance\")\n\nprint(\"\\n\u2713 Pooling operations complete\")\n</code></pre>"},{"location":"old/Week3_Day11/#exercise-5-first-cnn-in-pytorch-60-min","title":"Exercise 5: First CNN in PyTorch (60 min)","text":"<p>Build a simple CNN for MNIST:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: FIRST CNN IN PYTORCH\")\nprint(\"=\"*70)\n\n# Define simple CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, \n                               kernel_size=3, stride=1, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n                               kernel_size=3, stride=1, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # Conv block 1\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x = self.pool1(x)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = torch.relu(x)\n        x = self.pool2(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# Create model\nmodel = SimpleCNN()\nprint(\"Simple CNN Architecture:\")\nprint(model)\nprint()\n\n# Calculate parameter count\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\n\n# Trace through the network with a sample input\nprint(\"\\nTracing through the network:\")\nprint(\"-\" * 60)\n\nsample_input = torch.randn(1, 1, 28, 28)\nprint(f\"Input shape: {sample_input.shape} [batch, channels, height, width]\")\n\n# Layer by layer\nx = sample_input\nprint(f\"\\nAfter input: {x.shape}\")\n\nx = model.conv1(x)\nprint(f\"After conv1 (16 filters, 3\u00d73): {x.shape}\")\n\nx = torch.relu(x)\nprint(f\"After ReLU: {x.shape}\")\n\nx = model.pool1(x)\nprint(f\"After pool1 (2\u00d72): {x.shape}\")\n\nx = model.conv2(x)\nprint(f\"After conv2 (32 filters, 3\u00d73): {x.shape}\")\n\nx = torch.relu(x)\nprint(f\"After ReLU: {x.shape}\")\n\nx = model.pool2(x)\nprint(f\"After pool2 (2\u00d72): {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After flatten: {x.shape}\")\n\nx = model.fc1(x)\nprint(f\"After fc1: {x.shape}\")\n\nx = torch.relu(x)\nx = model.dropout(x)\nprint(f\"After ReLU + Dropout: {x.shape}\")\n\nx = model.fc2(x)\nprint(f\"After fc2 (output): {x.shape}\")\n\n# Load MNIST\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"\\nDataset loaded:\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Train CNN\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining CNN on MNIST...\")\nepochs = 5\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test CNN\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_accuracy = correct / total\nprint(f\"\\n\ud83c\udfaf Test Accuracy: {test_accuracy:.4f}\")\n\n# Compare with Week 2's fully connected network\nprint(\"\\nComparison with Week 2:\")\nprint(\"-\" * 60)\nprint(\"Fully Connected (Week 2): ~95-96% accuracy, ~500K parameters\")\nprint(f\"CNN (Week 3):             ~{test_accuracy*100:.1f}% accuracy, {total_params:,} parameters\")\nprint(\"\\n\ud83d\udca1 CNNs achieve similar/better accuracy with MANY fewer parameters!\")\n\nprint(\"\\n\u2713 First CNN complete\")\n</code></pre>"},{"location":"old/Week3_Day11/#exercise-6-visualizing-learned-features-50-min","title":"Exercise 6: Visualizing Learned Features (50 min)","text":"<p>See what the CNN has learned:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 6: VISUALIZING LEARNED FEATURES\")\nprint(\"=\"*70)\n\n# 1. Visualize conv1 filters\nprint(\"\\n1. Visualizing First Layer Filters\")\nprint(\"-\" * 40)\n\n# Get conv1 weights\nconv1_weights = model.conv1.weight.data.cpu()\nprint(f\"Conv1 weights shape: {conv1_weights.shape}\")  # [out_channels, in_channels, h, w]\n\n# Plot first layer filters\nfig, axes = plt.subplots(4, 4, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx in range(16):\n    filter_img = conv1_weights[idx, 0, :, :]  # [3, 3]\n    axes[idx].imshow(filter_img, cmap='gray')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\n\nplt.suptitle('Learned Filters in Conv1', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# 2. Visualize feature maps\nprint(\"\\n2. Visualizing Feature Maps\")\nprint(\"-\" * 40)\n\n# Get a sample image\ndataiter = iter(test_loader)\nimages, labels = next(dataiter)\nsample_image = images[0:1]  # Take first image\nsample_label = labels[0].item()\n\nprint(f\"Sample image label: {sample_label}\")\n\n# Hook to capture intermediate activations\nactivations = {}\n\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks\nmodel.conv1.register_forward_hook(get_activation('conv1'))\nmodel.conv2.register_forward_hook(get_activation('conv2'))\n\n# Forward pass\nmodel.eval()\nwith torch.no_grad():\n    output = model(sample_image)\n    prediction = output.argmax(dim=1).item()\n\nprint(f\"Predicted: {prediction}\")\n\n# Visualize conv1 feature maps\nconv1_features = activations['conv1'][0]  # [16, 28, 28]\nprint(f\"Conv1 feature maps shape: {conv1_features.shape}\")\n\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(16):\n    axes[idx].imshow(conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Conv1 Feature Maps (Label: {sample_label}, Pred: {prediction})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize conv2 feature maps\nconv2_features = activations['conv2'][0]  # [32, 14, 14]\nprint(f\"Conv2 feature maps shape: {conv2_features.shape}\")\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\naxes = axes.flatten()\n\nfor idx in range(32):\n    axes[idx].imshow(conv2_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Filter {idx}', fontsize=8)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Conv2 Feature Maps (Deeper features)', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Observations:\")\nprint(\"- Early layers detect simple features (edges, curves)\")\nprint(\"- Deeper layers combine simple features into complex patterns\")\nprint(\"- Different filters activate for different patterns\")\n\nprint(\"\\n\u2713 Feature visualization complete\")\n</code></pre>"},{"location":"old/Week3_Day11/#mini-challenge-understanding-receptive-fields-40-min","title":"Mini-Challenge: Understanding Receptive Fields (40 min)","text":"<p>Explore what each neuron \"sees\":</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: RECEPTIVE FIELDS\")\nprint(\"=\"*70)\n\ndef calculate_receptive_field(layers_info):\n    \"\"\"\n    Calculate receptive field size\n\n    layers_info: list of (kernel_size, stride) tuples\n    \"\"\"\n    rf = 1\n    stride_prod = 1\n\n    print(\"\\nReceptive Field Calculation:\")\n    print(\"-\" * 60)\n    print(f\"{'Layer':&gt;10} | {'Kernel':&gt;6} | {'Stride':&gt;6} | {'RF':&gt;6} | {'Stride Prod':&gt;12}\")\n    print(\"-\" * 60)\n\n    for i, (k, s) in enumerate(layers_info):\n        rf = rf + (k - 1) * stride_prod\n        stride_prod *= s\n        print(f\"Layer {i+1:&gt;3} | {k:&gt;6} | {s:&gt;6} | {rf:&gt;6} | {stride_prod:&gt;12}\")\n\n    return rf\n\n# Calculate for our SimpleCNN\nprint(\"\\nSimpleCNN Receptive Field:\")\nlayers = [\n    (3, 1),  # conv1: 3\u00d73, stride 1\n    (2, 2),  # pool1: 2\u00d72, stride 2\n    (3, 1),  # conv2: 3\u00d73, stride 1\n    (2, 2),  # pool2: 2\u00d72, stride 2\n]\n\nrf = calculate_receptive_field(layers)\nprint(f\"\\nFinal receptive field: {rf}\u00d7{rf}\")\nprint(f\"This means each neuron in the output 'sees' a {rf}\u00d7{rf} region of the input\")\n\n# Visualize receptive field\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n# Draw input image grid (28\u00d728)\nfor i in range(29):\n    ax.axhline(i, color='gray', linewidth=0.5, alpha=0.3)\n    ax.axvline(i, color='gray', linewidth=0.5, alpha=0.3)\n\n# Highlight receptive field (centered)\ncenter = 14\nhalf_rf = rf // 2\nrect = plt.Rectangle((center - half_rf, center - half_rf), rf, rf,\n                     fill=True, facecolor='blue', alpha=0.3, edgecolor='blue', linewidth=3)\nax.add_patch(rect)\n\nax.set_xlim(0, 28)\nax.set_ylim(28, 0)\nax.set_aspect('equal')\nax.set_title(f'Receptive Field Visualization\\nRF = {rf}\u00d7{rf} pixels', \n             fontsize=14, fontweight='bold')\nax.set_xlabel('Width (pixels)')\nax.set_ylabel('Height (pixels)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key Insights:\")\nprint(f\"- Each output neuron is influenced by a {rf}\u00d7{rf} region of input\")\nprint(\"- Deeper layers have larger receptive fields\")\nprint(\"- This is how CNNs capture hierarchical features\")\n\nprint(\"\\n\u2713 Receptive field analysis complete\")\n</code></pre>"},{"location":"old/Week3_Day11/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review convolution operation thoroughly \u2610 Understand why CNNs work for images \u2610 Write daily reflection (choose 2-3 prompts below) \u2610 Prepare questions for Monday check-in</p>"},{"location":"old/Week3_Day11/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does convolution differ from fully connected layers?</li> <li>Why do CNNs work better for images than fully connected networks?</li> <li>What surprised you about feature visualizations?</li> <li>How does pooling help CNNs?</li> <li>What is the purpose of multiple filters in a convolutional layer?</li> <li>What questions do you still have about CNNs?</li> </ul> <p>Next: Day 12 - Classic Architectures (LeNet, AlexNet)</p>"},{"location":"old/Week3_Day12/","title":"Week 3, Day 12: Classic Architectures - LeNet &amp; AlexNet","text":""},{"location":"old/Week3_Day12/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand LeNet-5 architecture (1998) - the CNN pioneer</li> <li>Study AlexNet (2012) - the ImageNet breakthrough</li> <li>Implement both architectures in PyTorch</li> <li>Compare performance: fully connected vs LeNet vs AlexNet</li> <li>Understand historical context and evolution</li> <li>Visualize architecture differences</li> </ul>"},{"location":"old/Week3_Day12/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week3_Day12/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week3_Day12/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: LeNet-5 Architecture Explained (15 min) Understanding the first successful CNN</p> <p>\u2610 Watch: AlexNet Explained by Yannic Kilcher (25 min) Deep dive into the ImageNet breakthrough</p> <p>\u2610 Watch: History of CNNs - ImageNet Evolution (20 min) Context for why AlexNet mattered</p> <p>\u2610 Watch: CNN Architectures Comparison by Lex Fridman (15 min) Evolution from LeNet to modern CNNs</p> <p>\u2610 Watch: Understanding Deep Learning by 3Blue1Brown review (15 min) Reinforce concepts</p>"},{"location":"old/Week3_Day12/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 8.1 - AlexNet</p> <p>\u2610 Read: D2L Chapter 7.6 - LeNet</p> <p>\u2610 Optional: Original LeNet-5 paper - sections 1-3 (Gradient-Based Learning Applied to Document Recognition)</p>"},{"location":"old/Week3_Day12/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week3_Day12/#exercise-1-implement-lenet-5-60-min","title":"Exercise 1: Implement LeNet-5 (60 min)","text":"<p>Build the classic 1998 architecture:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: LENET-5 IMPLEMENTATION\")\nprint(\"=\"*70)\n\n# LeNet-5 Architecture\nclass LeNet5(nn.Module):\n    \"\"\"\n    LeNet-5 (1998) by Yann LeCun\n\n    Original paper: Gradient-Based Learning Applied to Document Recognition\n\n    Architecture:\n    - Input: 32x32 grayscale image\n    - Conv1: 6 filters, 5x5 kernel\n    - Pool1: 2x2 average pooling\n    - Conv2: 16 filters, 5x5 kernel\n    - Pool2: 2x2 average pooling\n    - FC1: 120 units\n    - FC2: 84 units\n    - Output: 10 classes\n    \"\"\"\n    def __init__(self):\n        super(LeNet5, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Conv block 1\n        x = self.conv1(x)\n        x = torch.tanh(x)  # Original used tanh\n        x = self.pool1(x)\n\n        # Conv block 2\n        x = self.conv2(x)\n        x = torch.tanh(x)\n        x = self.pool2(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected\n        x = self.fc1(x)\n        x = torch.tanh(x)\n        x = self.fc2(x)\n        x = torch.tanh(x)\n        x = self.fc3(x)\n\n        return x\n\n# Create model\nlenet = LeNet5().to(device)\nprint(\"\\nLeNet-5 Architecture:\")\nprint(lenet)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in lenet.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Trace through with sample input\nprint(\"\\nArchitecture Flow:\")\nprint(\"-\" * 60)\n\nsample = torch.randn(1, 1, 32, 32)\nprint(f\"Input: {sample.shape}\")\n\n# Layer by layer\nx = sample\nx = lenet.conv1(x)\nprint(f\"After Conv1 (6@5x5): {x.shape}\")\nx = torch.tanh(x)\nx = lenet.pool1(x)\nprint(f\"After Pool1 (AvgPool 2x2): {x.shape}\")\n\nx = lenet.conv2(x)\nprint(f\"After Conv2 (16@5x5): {x.shape}\")\nx = torch.tanh(x)\nx = lenet.pool2(x)\nprint(f\"After Pool2 (AvgPool 2x2): {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After Flatten: {x.shape}\")\n\nx = lenet.fc1(x)\nprint(f\"After FC1 (120): {x.shape}\")\nx = torch.tanh(x)\n\nx = lenet.fc2(x)\nprint(f\"After FC2 (84): {x.shape}\")\nx = torch.tanh(x)\n\nx = lenet.fc3(x)\nprint(f\"After FC3 (10): {x.shape}\")\n\n# Load MNIST with padding to make it 32x32 (LeNet's expected input)\ntransform_lenet = transforms.Compose([\n    transforms.Pad(2),  # Pad MNIST from 28x28 to 32x32\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                               download=True, transform=transform_lenet)\ntest_dataset = datasets.MNIST(root='./data', train=False,\n                              download=True, transform=transform_lenet)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"\\nDataset: MNIST (padded to 32x32)\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Train LeNet\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters(), lr=0.001)\n\nprint(\"\\nTraining LeNet-5 on MNIST...\")\nepochs = 5\ntrain_losses = []\ntrain_accs = []\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    lenet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = lenet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n    train_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\ntraining_time = time.time() - start_time\n\n# Test LeNet\nlenet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = lenet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nlenet_accuracy = correct / total\nprint(f\"\\nLeNet-5 Test Accuracy: {lenet_accuracy:.4f}\")\nprint(f\"Training time: {training_time:.1f} seconds\")\n\nprint(\"\\nLeNet-5 implementation complete\")\n</code></pre>"},{"location":"old/Week3_Day12/#exercise-2-implement-alexnet-simplified-60-min","title":"Exercise 2: Implement AlexNet (Simplified) (60 min)","text":"<p>Build the 2012 ImageNet winner:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: ALEXNET IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass AlexNet(nn.Module):\n    \"\"\"\n    AlexNet (2012) by Alex Krizhevsky\n\n    Original paper: ImageNet Classification with Deep CNNs\n\n    Simplified version for MNIST (original was for 224x224 ImageNet)\n\n    Key innovations:\n    - ReLU activation (faster than tanh)\n    - Dropout for regularization\n    - Deeper network (8 layers)\n    - Data augmentation\n    - Multiple GPUs (not implemented here)\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n\n        # Convolutional layers\n        self.features = nn.Sequential(\n            # Conv1\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv2\n            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv3\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv4\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv5\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 4 * 4, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create model\nalexnet = AlexNet(num_classes=10).to(device)\nprint(\"\\nAlexNet Architecture:\")\nprint(alexnet)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in alexnet.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Compare with LeNet\nlenet_params = sum(p.numel() for p in lenet.parameters())\nprint(f\"LeNet-5 parameters: {lenet_params:,}\")\nprint(f\"AlexNet is {total_params/lenet_params:.1f}x larger than LeNet\")\n\n# Train AlexNet\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n\nprint(\"\\nTraining AlexNet on MNIST...\")\nepochs = 5\nalexnet_losses = []\nalexnet_accs = []\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    alexnet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = alexnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    alexnet_losses.append(epoch_loss)\n    alexnet_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\nalexnet_time = time.time() - start_time\n\n# Test AlexNet\nalexnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = alexnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nalexnet_accuracy = correct / total\nprint(f\"\\nAlexNet Test Accuracy: {alexnet_accuracy:.4f}\")\nprint(f\"Training time: {alexnet_time:.1f} seconds\")\n\nprint(\"\\nAlexNet implementation complete\")\n</code></pre>"},{"location":"old/Week3_Day12/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week3_Day12/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week3_Day12/#exercise-3-architecture-comparison-50-min","title":"Exercise 3: Architecture Comparison (50 min)","text":"<p>Compare all three approaches comprehensively:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: COMPREHENSIVE COMPARISON\")\nprint(\"=\"*70)\n\n# Load Day 11's SimpleCNN for comparison\nfrom Week3_Day11 import SimpleCNN  # Assuming it's available\n\nsimplecnn = SimpleCNN().to(device)\nsimplecnn_params = sum(p.numel() for p in simplecnn.parameters())\n\n# Create comparison table\nprint(\"\\nModel Comparison:\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Accuracy':&gt;10} | {'Training Time':&gt;14}\")\nprint(\"=\"*80)\nprint(f\"{'SimpleCNN':&lt;15} | {simplecnn_params:&gt;12,} | {'~98.5%':&gt;10} | {'~30s':&gt;14}\")\nprint(f\"{'LeNet-5':&lt;15} | {lenet_params:&gt;12,} | {lenet_accuracy:&gt;10.4f} | {training_time:&gt;13.1f}s\")\nprint(f\"{'AlexNet':&lt;15} | {total_params:&gt;12,} | {alexnet_accuracy:&gt;10.4f} | {alexnet_time:&gt;13.1f}s\")\nprint(\"=\"*80)\n\n# Visualize training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss comparison\naxes[0].plot(train_losses, label='LeNet-5', marker='o')\naxes[0].plot(alexnet_losses, label='AlexNet', marker='s')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy comparison\naxes[1].plot(train_accs, label='LeNet-5', marker='o')\naxes[1].plot(alexnet_accs, label='AlexNet', marker='s')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy Comparison')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('LeNet-5 vs AlexNet Training', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Architecture visualization\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# LeNet-5 diagram\nax = axes[0]\nlayers_lenet = [\n    (\"Input\\n32\u00d732\u00d71\", 0),\n    (\"Conv1\\n28\u00d728\u00d76\", 1),\n    (\"Pool1\\n14\u00d714\u00d76\", 2),\n    (\"Conv2\\n10\u00d710\u00d716\", 3),\n    (\"Pool2\\n5\u00d75\u00d716\", 4),\n    (\"FC1\\n120\", 5),\n    (\"FC2\\n84\", 6),\n    (\"Output\\n10\", 7)\n]\n\nfor i, (label, pos) in enumerate(layers_lenet):\n    ax.add_patch(plt.Rectangle((pos, 0), 0.8, 0.5, \n                                facecolor='lightblue', edgecolor='black', linewidth=2))\n    ax.text(pos + 0.4, 0.25, label, ha='center', va='center', fontsize=9)\n\n    if i &lt; len(layers_lenet) - 1:\n        ax.arrow(pos + 0.8, 0.25, 0.15, 0, head_width=0.1, \n                head_length=0.05, fc='black', ec='black')\n\nax.set_xlim(-0.5, 8)\nax.set_ylim(-0.2, 0.7)\nax.axis('off')\nax.set_title('LeNet-5 Architecture', fontsize=14, fontweight='bold', pad=20)\n\n# AlexNet diagram\nax = axes[1]\nlayers_alexnet = [\n    (\"Input\\n32\u00d732\u00d71\", 0),\n    (\"Conv1\\n32\u00d732\u00d764\", 1),\n    (\"Pool1\\n16\u00d716\u00d764\", 2),\n    (\"Conv2\\n16\u00d716\u00d7192\", 3),\n    (\"Pool2\\n8\u00d78\u00d7192\", 4),\n    (\"Conv3-5\\n8\u00d78\u00d7256\", 5),\n    (\"Pool3\\n4\u00d74\u00d7256\", 6),\n    (\"FC\u00d72\\n4096\", 7),\n    (\"Output\\n10\", 8)\n]\n\nfor i, (label, pos) in enumerate(layers_alexnet):\n    color = 'lightcoral' if 'FC' in label else 'lightgreen'\n    ax.add_patch(plt.Rectangle((pos, 0), 0.8, 0.5,\n                                facecolor=color, edgecolor='black', linewidth=2))\n    ax.text(pos + 0.4, 0.25, label, ha='center', va='center', fontsize=9)\n\n    if i &lt; len(layers_alexnet) - 1:\n        ax.arrow(pos + 0.8, 0.25, 0.15, 0, head_width=0.1,\n                head_length=0.05, fc='black', ec='black')\n\nax.set_xlim(-0.5, 9)\nax.set_ylim(-0.2, 0.7)\nax.axis('off')\nax.set_title('AlexNet Architecture (Simplified)', fontsize=14, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Observations:\")\nprint(\"- LeNet (1998): Small, efficient, tanh activation\")\nprint(\"- AlexNet (2012): Deeper, ReLU, dropout, more parameters\")\nprint(\"- Both work well on MNIST (too easy for modern CNNs)\")\nprint(\"- Real power shows on complex datasets (ImageNet)\")\n\nprint(\"\\nComparison complete\")\n</code></pre>"},{"location":"old/Week3_Day12/#exercise-4-historical-context-analysis-40-min","title":"Exercise 4: Historical Context Analysis (40 min)","text":"<p>Understand the evolution and impact:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: HISTORICAL CONTEXT\")\nprint(\"=\"*70)\n\n# Timeline of CNN evolution\nprint(\"\\nCNN Evolution Timeline:\")\nprint(\"=\"*80)\n\nmilestones = [\n    (\"1989\", \"LeNet-1\", \"Yann LeCun\", \"First successful CNN for digit recognition\"),\n    (\"1998\", \"LeNet-5\", \"LeCun et al.\", \"Refined architecture, deployed in production\"),\n    (\"2012\", \"AlexNet\", \"Krizhevsky\", \"ImageNet breakthrough, 15.3% \u2192 10.9% error\"),\n    (\"2014\", \"VGG\", \"Simonyan &amp; Zisserman\", \"Very deep networks with small filters\"),\n    (\"2015\", \"ResNet\", \"He et al.\", \"Skip connections enable 100+ layer networks\"),\n    (\"2017\", \"DenseNet\", \"Huang et al.\", \"Dense connections between layers\"),\n]\n\nfor year, name, author, achievement in milestones:\n    print(f\"{year}: {name:12s} by {author:20s} - {achievement}\")\n\n# AlexNet innovations\nprint(\"\\n\" + \"=\"*80)\nprint(\"AlexNet's Key Innovations (Why it Changed Everything)\")\nprint(\"=\"*80)\n\ninnovations = {\n    \"ReLU Activation\": {\n        \"Before\": \"tanh/sigmoid (slow training, vanishing gradients)\",\n        \"After\": \"ReLU (6x faster training, better gradients)\",\n        \"Impact\": \"Enabled training of deeper networks\"\n    },\n    \"Dropout\": {\n        \"Before\": \"L2 regularization only\",\n        \"After\": \"Random neuron dropout during training\",\n        \"Impact\": \"Reduced overfitting significantly\"\n    },\n    \"Data Augmentation\": {\n        \"Before\": \"Limited or no augmentation\",\n        \"After\": \"Crops, flips, color jitter\",\n        \"Impact\": \"Effective dataset size increased 2048x\"\n    },\n    \"GPU Training\": {\n        \"Before\": \"CPU-only (slow)\",\n        \"After\": \"Parallel training on 2 GPUs\",\n        \"Impact\": \"Made large-scale training feasible\"\n    },\n    \"Local Response Normalization\": {\n        \"Before\": \"No normalization between filters\",\n        \"After\": \"Normalize activations across channels\",\n        \"Impact\": \"Later replaced by Batch Normalization\"\n    }\n}\n\nfor innovation, details in innovations.items():\n    print(f\"\\n{innovation}:\")\n    print(f\"  Before: {details['Before']}\")\n    print(f\"  After:  {details['After']}\")\n    print(f\"  Impact: {details['Impact']}\")\n\n# ImageNet competition results\nprint(\"\\n\" + \"=\"*80)\nprint(\"ImageNet Competition Results (Top-5 Error Rate)\")\nprint(\"=\"*80)\n\nimagenet_results = [\n    (2010, 28.2, \"Traditional CV (SIFT + Fisher Vectors)\"),\n    (2011, 25.8, \"Traditional CV\"),\n    (2012, 16.4, \"AlexNet (First Deep CNN)\"),\n    (2013, 11.7, \"ZFNet\"),\n    (2014, 7.3, \"VGG &amp; GoogLeNet\"),\n    (2015, 3.57, \"ResNet-152\"),\n    (2017, 2.25, \"SENet\"),\n]\n\nyears = [r[0] for r in imagenet_results]\nerrors = [r[1] for r in imagenet_results]\nnames = [r[2] for r in imagenet_results]\n\nplt.figure(figsize=(12, 7))\nplt.plot(years, errors, marker='o', linewidth=2, markersize=10)\n\n# Annotate key points\nfor year, error, name in imagenet_results:\n    if year in [2011, 2012, 2015]:\n        plt.annotate(name, xy=(year, error), xytext=(10, -20 if year==2012 else 20),\n                    textcoords='offset points', fontsize=9,\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n                    arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3,rad=0'))\n\nplt.axvline(2012, color='red', linestyle='--', alpha=0.5, label='AlexNet Year')\nplt.axhline(5.1, color='green', linestyle='--', alpha=0.5, label='Human Performance (~5%)')\n\nplt.xlabel('Year')\nplt.ylabel('Top-5 Error Rate (%)')\nplt.title('ImageNet Competition: The Deep Learning Revolution', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nWhy AlexNet Was Revolutionary:\")\nprint(\"- First deep learning win in ImageNet competition\")\nprint(\"- Error rate: 25.8% \u2192 16.4% (37% reduction!)\")\nprint(\"- Proved deep learning &gt; traditional computer vision\")\nprint(\"- Sparked the deep learning boom\")\nprint(\"- Made GPUs essential for AI research\")\n\nprint(\"\\nHistorical context understood\")\n</code></pre>"},{"location":"old/Week3_Day12/#exercise-5-feature-visualization-comparison-50-min","title":"Exercise 5: Feature Visualization Comparison (50 min)","text":"<p>Compare what LeNet and AlexNet learn:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: FEATURE VISUALIZATION\")\nprint(\"=\"*70)\n\n# Visualize LeNet filters\nprint(\"\\n1. LeNet-5 First Layer Filters\")\nprint(\"-\" * 40)\n\nlenet_conv1_weights = lenet.conv1.weight.data.cpu()\nprint(f\"LeNet Conv1 filters shape: {lenet_conv1_weights.shape}\")  # [6, 1, 5, 5]\n\nfig, axes = plt.subplots(1, 6, figsize=(15, 3))\nfor idx in range(6):\n    axes[idx].imshow(lenet_conv1_weights[idx, 0], cmap='gray')\n    axes[idx].set_title(f'Filter {idx}')\n    axes[idx].axis('off')\nplt.suptitle('LeNet-5 Conv1 Filters (5\u00d75)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize AlexNet filters\nprint(\"\\n2. AlexNet First Layer Filters\")\nprint(\"-\" * 40)\n\nalexnet_conv1_weights = alexnet.features[0].weight.data.cpu()\nprint(f\"AlexNet Conv1 filters shape: {alexnet_conv1_weights.shape}\")  # [64, 1, 3, 3]\n\n# Show first 16 filters\nfig, axes = plt.subplots(4, 4, figsize=(10, 10))\naxes = axes.flatten()\nfor idx in range(16):\n    axes[idx].imshow(alexnet_conv1_weights[idx, 0], cmap='gray')\n    axes[idx].set_title(f'Filter {idx}', fontsize=9)\n    axes[idx].axis('off')\nplt.suptitle('AlexNet Conv1 Filters (3\u00d73, first 16 of 64)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Get feature maps for comparison\ndataiter = iter(test_loader)\nimages, labels = next(dataiter)\nsample_image = images[0:1]\n\n# LeNet feature maps\nactivations_lenet = {}\ndef get_activation_lenet(name):\n    def hook(model, input, output):\n        activations_lenet[name] = output.detach()\n    return hook\n\nlenet.conv1.register_forward_hook(get_activation_lenet('conv1'))\nlenet.conv2.register_forward_hook(get_activation_lenet('conv2'))\n\nlenet.eval()\nwith torch.no_grad():\n    _ = lenet(sample_image)\n\n# AlexNet feature maps\nactivations_alexnet = {}\ndef get_activation_alexnet(name):\n    def hook(model, input, output):\n        activations_alexnet[name] = output.detach()\n    return hook\n\nalexnet.features[0].register_forward_hook(get_activation_alexnet('conv1'))\nalexnet.features[3].register_forward_hook(get_activation_alexnet('conv2'))\n\nalexnet.eval()\nwith torch.no_grad():\n    _ = alexnet(sample_image)\n\n# Visualize LeNet feature maps\nlenet_conv1_features = activations_lenet['conv1'][0]\nprint(f\"\\nLeNet Conv1 feature maps: {lenet_conv1_features.shape}\")\n\nfig, axes = plt.subplots(1, 6, figsize=(15, 3))\nfor idx in range(6):\n    axes[idx].imshow(lenet_conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Map {idx}', fontsize=10)\n    axes[idx].axis('off')\nplt.suptitle(f'LeNet-5 Conv1 Feature Maps (Label: {labels[0].item()})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Visualize AlexNet feature maps\nalexnet_conv1_features = activations_alexnet['conv1'][0]\nprint(f\"AlexNet Conv1 feature maps: {alexnet_conv1_features.shape}\")\n\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.flatten()\nfor idx in range(16):\n    axes[idx].imshow(alexnet_conv1_features[idx].cpu(), cmap='viridis')\n    axes[idx].set_title(f'Map {idx}', fontsize=9)\n    axes[idx].axis('off')\nplt.suptitle(f'AlexNet Conv1 Feature Maps (first 16 of 64, Label: {labels[0].item()})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"- LeNet: 6 filters, captures basic edges and patterns\")\nprint(\"- AlexNet: 64 filters, more diverse feature detection\")\nprint(\"- More filters \u2192 more capacity to learn complex features\")\nprint(\"- Both learn hierarchical representations\")\n\nprint(\"\\nFeature visualization complete\")\n</code></pre>"},{"location":"old/Week3_Day12/#mini-challenge-design-your-own-architecture-60-min","title":"Mini-Challenge: Design Your Own Architecture (60 min)","text":"<p>Create a custom CNN:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: CUSTOM CNN DESIGN\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nYour task: Design a CNN better than LeNet but smaller than AlexNet\n\nRequirements:\n- Input: 32\u00d732\u00d71 (MNIST with padding)\n- Output: 10 classes\n- Target: &gt;99% accuracy\n- Constraint: &lt;500K parameters\n- Use modern techniques: ReLU, BatchNorm, Dropout\n\nDesign Considerations:\n1. How many conv layers?\n2. What kernel sizes?\n3. When to pool?\n4. How much dropout?\n5. FC layer sizes?\n\nTry different designs and compare!\n\"\"\")\n\nclass CustomCNN(nn.Module):\n    \"\"\"Your custom architecture here\"\"\"\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n\n        # TODO: Design your architecture\n        # Hint: Start with 3-4 conv layers\n        # Use BatchNorm after convolutions\n        # Use ReLU activation\n        # Add dropout before FC layers\n\n        # Example starter:\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 16\u00d716\n\n            # Block 2\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 8\u00d78\n\n            # Block 3\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 4\u00d74\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(256, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create and test your model\ncustom_model = CustomCNN().to(device)\ncustom_params = sum(p.numel() for p in custom_model.parameters())\n\nprint(f\"\\nYour Custom CNN:\")\nprint(custom_model)\nprint(f\"\\nParameters: {custom_params:,}\")\nprint(f\"Within budget: {'Yes' if custom_params &lt; 500000 else 'No'}\")\n\n# Train it\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(custom_model.parameters(), lr=0.001)\n\nprint(\"\\nTraining your custom CNN...\")\nepochs = 5\n\nfor epoch in range(epochs):\n    custom_model.train()\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = custom_model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_acc = correct / total\n    print(f\"Epoch {epoch+1}/{epochs}: Accuracy = {epoch_acc:.4f}\")\n\n# Test\ncustom_model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = custom_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ncustom_accuracy = correct / total\nprint(f\"\\nYour Custom CNN Test Accuracy: {custom_accuracy:.4f}\")\n\n# Final comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL COMPARISON\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Accuracy':&gt;10}\")\nprint(\"=\"*80)\nprint(f\"{'LeNet-5':&lt;15} | {lenet_params:&gt;12,} | {lenet_accuracy:&gt;10.4f}\")\nprint(f\"{'AlexNet':&lt;15} | {total_params:&gt;12,} | {alexnet_accuracy:&gt;10.4f}\")\nprint(f\"{'Your CNN':&lt;15} | {custom_params:&gt;12,} | {custom_accuracy:&gt;10.4f}\")\nprint(\"=\"*80)\n\nprint(\"\\nCustom architecture complete!\")\n</code></pre>"},{"location":"old/Week3_Day12/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review LeNet and AlexNet architectures \u2610 Understand historical significance \u2610 Write daily reflection (choose 2-3 prompts below)</p>"},{"location":"old/Week3_Day12/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How did CNNs evolve from LeNet to AlexNet?</li> <li>Why was AlexNet's 2012 win so significant?</li> <li>What innovations from AlexNet are still used today?</li> <li>How does your custom CNN compare to the classics?</li> <li>What design choices matter most in CNN architecture?</li> <li>What questions do you have about CNN design?</li> </ul> <p>Next: Day 13 - Modern Architectures (VGG, ResNet)</p>"},{"location":"old/Week3_Day13/","title":"Week 3, Day 13: Modern Architectures - VGG &amp; ResNet","text":""},{"location":"old/Week3_Day13/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand VGG philosophy: deeper networks with small filters</li> <li>Learn ResNet and skip connections</li> <li>Understand vanishing gradient problem and how ResNet solves it</li> <li>Implement VGG-style blocks and ResNet blocks</li> <li>Compare network depth effects</li> <li>Visualize gradient flow</li> </ul>"},{"location":"old/Week3_Day13/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week3_Day13/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week3_Day13/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: VGG Networks Explained (15 min) Understanding the philosophy of very deep networks</p> <p>\u2610 Watch: ResNet Explained by Yannic Kilcher (30 min) Deep dive into residual learning</p> <p>\u2610 Watch: ResNet: Why it works (15 min) Understanding skip connections</p> <p>\u2610 Watch: Batch Normalization Explained by StatQuest (15 min) Critical for training deep networks</p> <p>\u2610 Watch: Vanishing Gradients Problem (15 min) Why deep networks were hard to train</p>"},{"location":"old/Week3_Day13/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 8.6 - VGG</p> <p>\u2610 Read: D2L Chapter 8.7 - ResNet</p> <p>\u2610 Optional: Original ResNet paper introduction (Deep Residual Learning for Image Recognition)</p>"},{"location":"old/Week3_Day13/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week3_Day13/#exercise-1-vgg-blocks-and-architecture-60-min","title":"Exercise 1: VGG Blocks and Architecture (60 min)","text":"<p>Understand VGG's modular design:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: VGG ARCHITECTURE\")\nprint(\"=\"*70)\n\ndef vgg_block(num_convs, in_channels, out_channels):\n    \"\"\"\n    Create a VGG block\n\n    VGG philosophy: Stack multiple 3\u00d73 convolutions\n    Benefit: Two 3\u00d73 convs have same receptive field as one 5\u00d75\n            but fewer parameters and more non-linearity\n\n    Args:\n        num_convs: number of conv layers in block\n        in_channels: input channels\n        out_channels: output channels\n    \"\"\"\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n        in_channels = out_channels\n\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    return nn.Sequential(*layers)\n\n# Demonstrate receptive field advantage\nprint(\"\\nVGG Insight: Why 3\u00d73 convolutions?\")\nprint(\"-\" * 60)\nprint(\"Two 3\u00d73 convs:\")\nprint(\"  - Receptive field: 5\u00d75\")\nprint(\"  - Parameters: 2 \u00d7 (3\u00d73) = 18 per channel\")\nprint(\"  - Non-linearities: 2 (ReLU after each)\")\nprint()\nprint(\"One 5\u00d75 conv:\")\nprint(\"  - Receptive field: 5\u00d75\")\nprint(\"  - Parameters: 1 \u00d7 (5\u00d75) = 25 per channel\")\nprint(\"  - Non-linearities: 1\")\nprint()\nprint(\"Result: 28% fewer parameters + more non-linearity!\")\n\nclass VGG16_MNIST(nn.Module):\n    \"\"\"\n    VGG-16 architecture adapted for MNIST\n\n    Original VGG-16 was designed for ImageNet (224\u00d7224 RGB)\n    This is a simplified version for MNIST (32\u00d732 grayscale)\n\n    Architecture:\n    - 5 VGG blocks with increasing channels: 64, 128, 256, 512, 512\n    - Each block has 2-3 conv layers\n    - FC layers at the end\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(VGG16_MNIST, self).__init__()\n\n        self.features = nn.Sequential(\n            # Block 1: 32\u00d732 \u2192 16\u00d716\n            vgg_block(2, 1, 64),\n\n            # Block 2: 16\u00d716 \u2192 8\u00d78\n            vgg_block(2, 64, 128),\n\n            # Block 3: 8\u00d78 \u2192 4\u00d74\n            vgg_block(3, 128, 256),\n\n            # Block 4: 4\u00d74 \u2192 2\u00d72\n            vgg_block(3, 256, 512),\n\n            # Block 5: 2\u00d72 \u2192 1\u00d71\n            vgg_block(3, 512, 512),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 1 * 1, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Create model\nvgg = VGG16_MNIST().to(device)\nprint(\"\\nVGG-16 Architecture (MNIST version):\")\nprint(vgg)\n\n# Count parameters\nvgg_params = sum(p.numel() for p in vgg.parameters())\nprint(f\"\\nTotal parameters: {vgg_params:,}\")\n\n# Analyze each VGG block\nprint(\"\\nVGG Block Analysis:\")\nprint(\"-\" * 60)\nsample = torch.randn(1, 1, 32, 32)\nx = sample\n\nblock_num = 1\nfor module in vgg.features:\n    if isinstance(module, nn.Sequential):\n        x = module(x)\n        print(f\"After Block {block_num}: {x.shape}\")\n        block_num += 1\n\nprint(f\"After Flatten: {x.view(x.size(0), -1).shape}\")\n\n# Load data\ntransform = transforms.Compose([\n    transforms.Pad(2),  # 28\u00d728 \u2192 32\u00d732\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Train VGG (just a few epochs due to size)\nprint(\"\\nTraining VGG-16 on MNIST (3 epochs)...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vgg.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    vgg.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        outputs = vgg(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        if (batch_idx + 1) % 200 == 0:\n            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: \"\n                  f\"Loss = {running_loss/(batch_idx+1):.4f}\")\n\n    epoch_acc = correct / total\n    print(f\"Epoch {epoch+1}/{epochs}: Accuracy = {epoch_acc:.4f}\")\n\n# Test\nvgg.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = vgg(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nvgg_accuracy = correct / total\nprint(f\"\\nVGG-16 Test Accuracy: {vgg_accuracy:.4f}\")\nprint(f\"Parameters: {vgg_params:,}\")\n\nprint(\"\\nVGG implementation complete\")\n</code></pre>"},{"location":"old/Week3_Day13/#exercise-2-understanding-residual-blocks-60-min","title":"Exercise 2: Understanding Residual Blocks (60 min)","text":"<p>Learn the breakthrough idea of ResNet:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: RESIDUAL BLOCKS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nThe Vanishing Gradient Problem:\n- Deep networks hard to train (gradients disappear)\n- Adding layers made performance WORSE (degradation problem)\n- Not overfitting - training error also increased!\n\nResNet's Solution: Skip Connections\n- Instead of learning H(x), learn F(x) = H(x) - x\n- Output: H(x) = F(x) + x\n- If optimal is identity, just learn F(x) = 0 (easy!)\n- Gradients flow directly through skip connections\n\"\"\")\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    Basic Residual Block\n\n    Two 3\u00d73 convolutions with skip connection\n\n           x\n           |\n       [Conv-BN-ReLU]\n           |\n       [Conv-BN]\n           |\n          (+)  \u2190 x (skip connection)\n           |\n         [ReLU]\n           |\n          out\n    \"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut connection\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            # Need to match dimensions\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels,\n                         kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n\n        # Main path\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = torch.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # Add skip connection\n        out += self.shortcut(identity)\n        out = torch.relu(out)\n\n        return out\n\n# Visualize the difference\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARING: Plain Network vs Residual Network\")\nprint(\"=\"*60)\n\nclass PlainBlock(nn.Module):\n    \"\"\"Plain block without skip connection\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(PlainBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = torch.relu(self.bn2(self.conv2(out)))\n        return out\n\n# Test gradient flow\nplain_block = PlainBlock(64, 64)\nres_block = ResidualBlock(64, 64)\n\n# Create input\nx = torch.randn(1, 64, 32, 32, requires_grad=True)\n\n# Forward pass\nplain_out = plain_block(x)\nres_out = res_block(x)\n\n# Backward pass\nplain_loss = plain_out.sum()\nres_loss = res_out.sum()\n\nplain_loss.backward()\nplain_grad = x.grad.clone()\n\nx.grad.zero_()\n\nres_loss.backward()\nres_grad = x.grad.clone()\n\n# Compare gradient magnitudes\nprint(f\"\\nGradient magnitude comparison:\")\nprint(f\"Plain block: {plain_grad.abs().mean().item():.6f}\")\nprint(f\"Residual block: {res_grad.abs().mean().item():.6f}\")\nprint(f\"Ratio: {(res_grad.abs().mean() / plain_grad.abs().mean()).item():.2f}x\")\n\nprint(\"\\nSkip connections maintain gradient flow!\")\n\nprint(\"\\nResidual blocks understood\")\n</code></pre>"},{"location":"old/Week3_Day13/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week3_Day13/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week3_Day13/#exercise-3-implement-resnet-18-70-min","title":"Exercise 3: Implement ResNet-18 (70 min)","text":"<p>Build a complete ResNet architecture:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: RESNET-18 IMPLEMENTATION\")\nprint(\"=\"*70)\n\nclass ResNet18(nn.Module):\n    \"\"\"\n    ResNet-18 architecture\n\n    Structure:\n    - Initial conv layer (7\u00d77 or 3\u00d73 for small images)\n    - 4 residual stages with [2,2,2,2] blocks\n    - Average pooling\n    - FC layer\n\n    Channels: 64 \u2192 128 \u2192 256 \u2192 512\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super(ResNet18, self).__init__()\n\n        # Initial convolution\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Residual stages\n        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n\n        # Global average pooling\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Classifier\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        \"\"\"Create a residual stage\"\"\"\n        layers = []\n\n        # First block (may downsample)\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n\n        # Remaining blocks\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # Initial conv\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n\n        # Residual stages\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # Global pooling\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n\n        # Classifier\n        x = self.fc(x)\n\n        return x\n\n# Create ResNet-18\nresnet18 = ResNet18(num_classes=10)\nprint(\"\\nResNet-18 Architecture:\")\nprint(resnet18)\n\n# Count parameters\nresnet_params = sum(p.numel() for p in resnet18.parameters())\nprint(f\"\\nTotal parameters: {resnet_params:,}\")\n\n# Trace through architecture\nprint(\"\\nArchitecture flow:\")\nprint(\"-\" * 60)\nsample = torch.randn(1, 1, 32, 32)\nprint(f\"Input: {sample.shape}\")\n\nx = sample\nx = resnet18.conv1(x)\nx = resnet18.bn1(x)\nx = torch.relu(x)\nprint(f\"After initial conv: {x.shape}\")\n\nx = resnet18.layer1(x)\nprint(f\"After layer1 (64 channels): {x.shape}\")\n\nx = resnet18.layer2(x)\nprint(f\"After layer2 (128 channels): {x.shape}\")\n\nx = resnet18.layer3(x)\nprint(f\"After layer3 (256 channels): {x.shape}\")\n\nx = resnet18.layer4(x)\nprint(f\"After layer4 (512 channels): {x.shape}\")\n\nx = resnet18.avg_pool(x)\nprint(f\"After global avg pool: {x.shape}\")\n\nx = x.view(x.size(0), -1)\nprint(f\"After flatten: {x.shape}\")\n\n# Train ResNet-18\nprint(\"\\nTraining ResNet-18 on MNIST...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n\nepochs = 5\nresnet_losses = []\nresnet_accs = []\n\nfor epoch in range(epochs):\n    resnet18.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = resnet18(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    resnet_losses.append(epoch_loss)\n    resnet_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test ResNet-18\nresnet18.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet18(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nresnet_accuracy = correct / total\nprint(f\"\\nResNet-18 Test Accuracy: {resnet_accuracy:.4f}\")\n\nprint(\"\\nResNet-18 implementation complete\")\n</code></pre>"},{"location":"old/Week3_Day13/#exercise-4-compare-plain-vs-residual-networks-60-min","title":"Exercise 4: Compare Plain vs Residual Networks (60 min)","text":"<p>Empirically demonstrate ResNet's advantage:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: PLAIN VS RESIDUAL COMPARISON\")\nprint(\"=\"*70)\n\nclass PlainNet(nn.Module):\n    \"\"\"Plain network (no skip connections)\"\"\"\n    def __init__(self, num_classes=10):\n        super(PlainNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 64, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Same structure as ResNet but NO skip connections\n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n\n        # First block\n        layers.append(nn.Conv2d(in_channels, out_channels, 3, \n                               stride=stride, padding=1, bias=False))\n        layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.ReLU(inplace=True))\n\n        # Remaining blocks\n        for _ in range(1, num_blocks):\n            layers.append(nn.Conv2d(out_channels, out_channels, 3,\n                                   padding=1, bias=False))\n            layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(nn.ReLU(inplace=True))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Create plain network\nplainnet = PlainNet(num_classes=10)\nplainnet_params = sum(p.numel() for p in plainnet.parameters())\n\nprint(f\"\\nPlainNet parameters: {plainnet_params:,}\")\nprint(f\"ResNet-18 parameters: {resnet_params:,}\")\nprint(f\"Difference: {abs(plainnet_params - resnet_params):,} \"\n      f\"({abs(plainnet_params - resnet_params)/resnet_params*100:.1f}%)\")\n\n# Train both networks\nprint(\"\\nTraining PlainNet...\")\ncriterion = nn.CrossEntropyLoss()\nplain_optimizer = optim.Adam(plainnet.parameters(), lr=0.001)\n\nepochs = 5\nplain_losses = []\nplain_accs = []\n\nfor epoch in range(epochs):\n    plainnet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = plainnet(images)\n        loss = criterion(outputs, labels)\n\n        plain_optimizer.zero_grad()\n        loss.backward()\n        plain_optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    plain_losses.append(epoch_loss)\n    plain_accs.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n\n# Test PlainNet\nplainnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = plainnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nplain_accuracy = correct / total\nprint(f\"\\nPlainNet Test Accuracy: {plain_accuracy:.4f}\")\n\n# Comparison visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss comparison\naxes[0].plot(plain_losses, label='PlainNet', marker='o', linewidth=2)\naxes[0].plot(resnet_losses, label='ResNet-18', marker='s', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss: Plain vs Residual')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy comparison\naxes[1].plot(plain_accs, label='PlainNet', marker='o', linewidth=2)\naxes[1].plot(resnet_accs, label='ResNet-18', marker='s', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy: Plain vs Residual')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Final comparison table\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL COMPARISON\")\nprint(\"=\"*80)\nprint(f\"{'Model':&lt;15} | {'Parameters':&gt;12} | {'Test Accuracy':&gt;14} | {'Difference':&gt;12}\")\nprint(\"=\"*80)\nprint(f\"{'PlainNet':&lt;15} | {plainnet_params:&gt;12,} | {plain_accuracy:&gt;14.4f} | {'baseline':&gt;12}\")\nprint(f\"{'ResNet-18':&lt;15} | {resnet_params:&gt;12,} | {resnet_accuracy:&gt;14.4f} | \"\n      f\"{'+' if resnet_accuracy &gt; plain_accuracy else ''}{(resnet_accuracy - plain_accuracy):.4f}:&gt;12}\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83d\udca1 Key Observations:\")\nprint(\"- Skip connections improve training stability\")\nprint(\"- ResNet often converges faster\")\nprint(\"- Performance difference more dramatic on complex datasets\")\nprint(\"- Skip connections enable much deeper networks (50, 101, 152 layers)\")\n\nprint(\"\\n\u2713 Comparison complete\")\n</code></pre>"},{"location":"old/Week3_Day13/#exercise-5-visualize-gradient-flow-40-min","title":"Exercise 5: Visualize Gradient Flow (40 min)","text":"<p>See how skip connections help gradients:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: GRADIENT FLOW VISUALIZATION\")\nprint(\"=\"*70)\n\ndef analyze_gradients(model, model_name):\n    \"\"\"Analyze gradient magnitudes throughout network\"\"\"\n    model.train()\n\n    # Forward pass\n    images, labels = next(iter(train_loader))\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n\n    # Backward pass\n    model.zero_grad()\n    loss.backward()\n\n    # Collect gradients\n    grad_norms = []\n    layer_names = []\n\n    for name, param in model.named_parameters():\n        if param.grad is not None and 'weight' in name:\n            grad_norm = param.grad.norm().item()\n            grad_norms.append(grad_norm)\n            # Simplify layer names\n            simple_name = name.split('.')[0]\n            if simple_name not in layer_names or len(layer_names) &lt; 5:\n                layer_names.append(simple_name)\n            else:\n                layer_names.append('')\n\n    return grad_norms, layer_names\n\n# Analyze both networks\nprint(\"\\nAnalyzing gradient flow...\")\nplain_grads, plain_layers = analyze_gradients(plainnet, \"PlainNet\")\nresnet_grads, resnet_layers = analyze_gradients(resnet18, \"ResNet-18\")\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# PlainNet gradients\naxes[0].bar(range(len(plain_grads)), plain_grads, color='coral', alpha=0.7)\naxes[0].set_xlabel('Layer (deeper \u2192)')\naxes[0].set_ylabel('Gradient Norm')\naxes[0].set_title('PlainNet: Gradient Magnitudes')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3, axis='y')\naxes[0].axhline(y=1e-5, color='red', linestyle='--', label='Very small gradient')\naxes[0].legend()\n\n# ResNet gradients\naxes[1].bar(range(len(resnet_grads)), resnet_grads, color='steelblue', alpha=0.7)\naxes[1].set_xlabel('Layer (deeper \u2192)')\naxes[1].set_ylabel('Gradient Norm')\naxes[1].set_title('ResNet-18: Gradient Magnitudes')\naxes[1].set_yscale('log')\naxes[1].grid(True, alpha=0.3, axis='y')\naxes[1].axhline(y=1e-5, color='red', linestyle='--', label='Very small gradient')\naxes[1].legend()\n\nplt.suptitle('Gradient Flow Comparison', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Observations:\")\nprint(\"- PlainNet: Gradients get smaller in earlier layers (vanishing gradients)\")\nprint(\"- ResNet: More uniform gradient distribution throughout network\")\nprint(\"- Skip connections create 'highways' for gradient flow\")\nprint(\"- This is why ResNet can go 100+ layers deep\")\n\nprint(\"\\n\u2713 Gradient visualization complete\")\n</code></pre>"},{"location":"old/Week3_Day13/#mini-challenge-design-deep-resnet-50-min","title":"Mini-Challenge: Design Deep ResNet (50 min)","text":"<p>Apply residual learning principles:</p> <pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: DEEPER RESNET\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nYour challenge: Design a deeper ResNet variant\n\nOptions to explore:\n1. ResNet-34: [3,4,6,3] blocks in each stage\n2. Add more channels: 64 \u2192 128 \u2192 256 \u2192 512 \u2192 1024\n3. Different block structures (bottleneck blocks)\n4. Experiment with initial conv size\n5. Try different downsampling strategies\n\nGoal: Beat your Day 12 custom CNN!\n\"\"\")\n\nclass DeepResNet(nn.Module):\n    \"\"\"Your deeper ResNet design\"\"\"\n    def __init__(self, num_classes=10):\n        super(DeepResNet, self).__init__()\n\n        # TODO: Design your architecture\n        # Consider: How many stages? How many blocks per stage?\n        #          What channel progression?\n\n        self.conv1 = nn.Conv2d(1, 64, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Your layers here\n        self.layer1 = self._make_layer(64, 64, 3, stride=1)\n        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\ndeep_resnet = DeepResNet()\ndeep_params = sum(p.numel() for p in deep_resnet.parameters())\n\nprint(f\"\\nYour Deep ResNet:\")\nprint(f\"Parameters: {deep_params:,}\")\nprint(f\"Architecture: [3,4,6,3] blocks (ResNet-34 style)\")\n\n# Quick training\nprint(\"\\nTraining your Deep ResNet (3 epochs)...\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(deep_resnet.parameters(), lr=0.001)\n\nfor epoch in range(3):\n    deep_resnet.train()\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        outputs = deep_resnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f\"Epoch {epoch+1}/3: Accuracy = {correct/total:.4f}\")\n\n# Test\ndeep_resnet.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = deep_resnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ndeep_accuracy = correct / total\nprint(f\"\\n\ud83c\udfaf Your Deep ResNet Accuracy: {deep_accuracy:.4f}\")\n\nprint(\"\\n\u2713 Deep ResNet challenge complete!\")\n</code></pre>"},{"location":"old/Week3_Day13/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review VGG and ResNet architectures \u2610 Understand skip connections deeply \u2610 Write daily reflection (choose 2-3 prompts below)</p>"},{"location":"old/Week3_Day13/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>Why do skip connections solve the vanishing gradient problem?</li> <li>How does VGG's design philosophy differ from AlexNet?</li> <li>What makes ResNet such a breakthrough?</li> <li>How do gradients flow differently in plain vs residual networks?</li> <li>What would you consider when choosing network depth?</li> <li>What questions do you have about modern architectures?</li> </ul> <p>Next: Day 14 - Transfer Learning &amp; Data Augmentation</p>"},{"location":"old/Week3_Day14/","title":"Week 3, Day 14: Transfer Learning &amp; Data Augmentation","text":""},{"location":"old/Week3_Day14/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand transfer learning and when to use it</li> <li>Learn feature extraction vs fine-tuning</li> <li>Master data augmentation techniques</li> <li>Use pretrained models from torchvision</li> <li>Apply transfer learning to new datasets</li> <li>Design effective augmentation strategies</li> </ul>"},{"location":"old/Week3_Day14/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week3_Day14/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week3_Day14/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Transfer Learning Explained (20 min) Core concepts and when to use transfer learning</p> <p>\u2610 Watch: Fine-Tuning Neural Networks (15 min) Feature extraction vs full fine-tuning</p> <p>\u2610 Watch: Data Augmentation Techniques (20 min) How to increase effective dataset size</p> <p>\u2610 Watch: PyTorch Transfer Learning Tutorial (reading + 20 min) Practical implementation patterns</p>"},{"location":"old/Week3_Day14/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 14.2 - Fine-Tuning</p> <p>\u2610 Read: PyTorch torchvision.models docs</p> <p>\u2610 Read: PyTorch transforms docs</p>"},{"location":"old/Week3_Day14/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copy\n\nprint(\"=\"*70)\nprint(\"DAY 14: TRANSFER LEARNING &amp; DATA AUGMENTATION\")\nprint(\"=\"*70)\n\n#### Exercise 1: Understanding Pretrained Models (45 min)\n\nprint(\"\\nEXERCISE 1: PRETRAINED MODELS\")\nprint(\"-\" * 60)\n\n# Load pretrained ResNet18\nresnet_pretrained = models.resnet18(pretrained=True)\nprint(\"\\nPretrained ResNet-18 (trained on ImageNet):\")\nprint(f\"Total parameters: {sum(p.numel() for p in resnet_pretrained.parameters()):,}\")\n\n# Examine architecture\nprint(\"\\nArchitecture summary:\")\nfor name, module in resnet_pretrained.named_children():\n    if hasattr(module, '__len__'):\n        print(f\"{name}: {len(module)} sub-modules\")\n    else:\n        print(f\"{name}: {module.__class__.__name__}\")\n\n# Look at final layer\nprint(f\"\\nOriginal classifier (for 1000 ImageNet classes):\")\nprint(resnet_pretrained.fc)\n\n# Modify for our task (10 classes)\nnum_features = resnet_pretrained.fc.in_features\nresnet_pretrained.fc = nn.Linear(num_features, 10)\nprint(f\"\\nModified classifier (for 10 classes):\")\nprint(resnet_pretrained.fc)\n\nprint(\"\\n\ud83d\udca1 Transfer Learning Strategy:\")\nprint(\"1. Load pretrained model (learned features from ImageNet)\")\nprint(\"2. Replace final layer (for new task)\")\nprint(\"3. Choose: freeze backbone OR fine-tune all layers\")\n\n#### Exercise 2: Feature Extraction vs Fine-Tuning (50 min)\n\nprint(\"\\n\\nEXERCISE 2: FEATURE EXTRACTION VS FINE-TUNING\")\nprint(\"-\" * 60)\n\n# Prepare data (CIFAR-10 for more challenging task)\nprint(\"\\nLoading CIFAR-10...\")\n\n# Transforms for pretrained models (expecting ImageNet statistics)\ntransform_pretrained = transforms.Compose([\n    transforms.Resize(32),  # Ensure 32x32\n    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])  # ImageNet stats\n])\n\n# For MNIST (grayscale to RGB)\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                              download=True, transform=transform_pretrained)\ntest_dataset = datasets.MNIST(root='./data', train=False,\n                             download=True, transform=transform_pretrained)\n\n# Split train into train/val\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_data, val_data = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"Training: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_dataset)}\")\n\n# Strategy 1: Feature Extraction (freeze backbone)\nprint(\"\\n1. Feature Extraction (Freeze Backbone)\")\nprint(\"-\" * 60)\n\nmodel_feature_extract = models.resnet18(pretrained=True)\n# Modify final layer\nmodel_feature_extract.fc = nn.Linear(model_feature_extract.fc.in_features, 10)\n\n# Freeze all layers except final\nfor param in model_feature_extract.parameters():\n    param.requires_grad = False\n# Unfreeze final layer\nfor param in model_feature_extract.fc.parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in model_feature_extract.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,} (only final layer)\")\n\n# Train\ndef train_model(model, train_loader, val_loader, epochs=3, lr=0.001, name=\"Model\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n\n        for images, labels in train_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"{name} Epoch {epoch+1}/{epochs}: \"\n              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return history\n\nprint(\"\\nTraining feature extraction model...\")\nhistory_extract = train_model(model_feature_extract, train_loader, val_loader, \n                              epochs=3, lr=0.001, name=\"Feature Extract\")\n\n# Strategy 2: Fine-Tuning (train all layers)\nprint(\"\\n\\n2. Fine-Tuning (Unfreeze All Layers)\")\nprint(\"-\" * 60)\n\nmodel_finetune = models.resnet18(pretrained=True)\nmodel_finetune.fc = nn.Linear(model_finetune.fc.in_features, 10)\n\n# All parameters trainable\nfor param in model_finetune.parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in model_finetune.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,} (all layers)\")\n\nprint(\"\\nTraining fine-tuned model...\")\nhistory_finetune = train_model(model_finetune, train_loader, val_loader,\n                               epochs=3, lr=0.0001, name=\"Fine-Tune\")  # Lower LR!\n\n# Compare\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"COMPARISON: Feature Extraction vs Fine-Tuning\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(history_extract['val_acc'])+1)\n\naxes[0].plot(epochs_range, history_extract['val_loss'], label='Feature Extract', marker='o')\naxes[0].plot(epochs_range, history_finetune['val_loss'], label='Fine-Tune', marker='s')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Validation Loss')\naxes[0].set_title('Validation Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(epochs_range, history_extract['val_acc'], label='Feature Extract', marker='o')\naxes[1].plot(epochs_range, history_finetune['val_acc'], label='Fine-Tune', marker='s')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('Validation Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal validation accuracy:\")\nprint(f\"  Feature Extraction: {history_extract['val_acc'][-1]:.4f}\")\nprint(f\"  Fine-Tuning: {history_finetune['val_acc'][-1]:.4f}\")\n\nprint(\"\\n\ud83d\udca1 When to use which:\")\nprint(\"Feature Extraction:\")\nprint(\"  + Faster training (fewer parameters)\")\nprint(\"  + Less risk of overfitting (frozen weights)\")\nprint(\"  - Limited adaptation to new domain\")\nprint(\"\\nFine-Tuning:\")\nprint(\"  + Better adaptation to new domain\")\nprint(\"  + Can achieve higher accuracy\")\nprint(\"  - Requires more data to avoid overfitting\")\nprint(\"  - Slower training\")\n\n#### Exercise 3: Data Augmentation (45 min)\n\nprint(\"\\n\\nEXERCISE 3: DATA AUGMENTATION\")\nprint(\"-\" * 60)\n\n# Show augmentation effects\naugmentations = {\n    'Original': transforms.Compose([\n        transforms.ToTensor(),\n    ]),\n    'Random Crop': transforms.Compose([\n        transforms.RandomCrop(28, padding=4),\n        transforms.ToTensor(),\n    ]),\n    'Horizontal Flip': transforms.Compose([\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.ToTensor(),\n    ]),\n    'Rotation': transforms.Compose([\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n    ]),\n    'Color Jitter': transforms.Compose([\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n    ]),\n    'Random Affine': transforms.Compose([\n        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ToTensor(),\n    ]),\n}\n\n# Get sample image\nmnist_raw = datasets.MNIST(root='./data', train=True, download=True)\nsample_img, label = mnist_raw[0]\n\n# Apply augmentations\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor idx, (name, transform) in enumerate(augmentations.items()):\n    augmented = transform(sample_img)\n    axes[idx].imshow(augmented.squeeze(), cmap='gray')\n    axes[idx].set_title(name)\n    axes[idx].axis('off')\n\nplt.suptitle(f'Data Augmentation Examples (Original Label: {label})', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Train with strong augmentation\nprint(\"\\nTraining with strong augmentation...\")\n\ntransform_augmented = transforms.Compose([\n    transforms.RandomCrop(28, padding=4),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_augmented = datasets.MNIST(root='./data', train=True, \n                                 download=True, transform=transform_augmented)\ntrain_aug_loader = DataLoader(train_augmented, batch_size=128, shuffle=True)\n\n# Simple CNN to test augmentation effect\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)\n        x = torch.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Train without augmentation\nmodel_no_aug = SimpleCNN()\nprint(\"Training WITHOUT augmentation...\")\nhistory_no_aug = train_model(model_no_aug, train_loader, val_loader, \n                             epochs=5, name=\"No Aug\")\n\n# Train with augmentation  \nmodel_with_aug = SimpleCNN()\nprint(\"\\nTraining WITH augmentation...\")\nhistory_with_aug = train_model(model_with_aug, train_aug_loader, val_loader,\n                               epochs=5, name=\"With Aug\")\n\n# Compare\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nepochs_range = range(1, 6)\nax.plot(epochs_range, history_no_aug['val_acc'], label='No Augmentation', marker='o')\nax.plot(epochs_range, history_with_aug['val_acc'], label='With Augmentation', marker='s')\nax.set_xlabel('Epoch')\nax.set_ylabel('Validation Accuracy')\nax.set_title('Effect of Data Augmentation')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal validation accuracy:\")\nprint(f\"  Without augmentation: {history_no_aug['val_acc'][-1]:.4f}\")\nprint(f\"  With augmentation: {history_with_aug['val_acc'][-1]:.4f}\")\nprint(f\"  Improvement: {history_with_aug['val_acc'][-1] - history_no_aug['val_acc'][-1]:.4f}\")\n\nprint(\"\\n\u2713 Exercises 1-3 complete\")\n</code></pre>"},{"location":"old/Week3_Day14/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week3_Day14/#mini-challenge-complete-transfer-learning-pipeline-35-hours","title":"Mini-Challenge: Complete Transfer Learning Pipeline (3.5 hours)","text":"<pre><code>print(\"\\n\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: COMPLETE TRANSFER LEARNING PROJECT\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nYour challenge: Build the best possible model for CIFAR-10\n\nSteps:\n1. Design augmentation strategy\n2. Choose pretrained model (ResNet18, ResNet50, VGG, etc.)\n3. Decide: feature extraction or fine-tuning?\n4. Train and evaluate\n5. Document your choices\n\nTarget: &gt;90% accuracy on CIFAR-10\n\"\"\")\n\n# Load CIFAR-10\nprint(\"\\nLoading CIFAR-10 (color images, 10 classes)...\")\n\n# Design your augmentation\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ncifar_train = datasets.CIFAR10(root='./data', train=True, \n                               download=True, transform=transform_train)\ncifar_test = datasets.CIFAR10(root='./data', train=False,\n                              download=True, transform=transform_test)\n\n# Split train into train/val\ntrain_size = int(0.9 * len(cifar_train))\nval_size = len(cifar_train) - train_size\ncifar_train_data, cifar_val_data = random_split(cifar_train, [train_size, val_size])\n\ncifar_train_loader = DataLoader(cifar_train_data, batch_size=128, shuffle=True, num_workers=2)\ncifar_val_loader = DataLoader(cifar_val_data, batch_size=128, shuffle=False, num_workers=2)\ncifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)\n\nprint(f\"CIFAR-10 loaded:\")\nprint(f\"  Training: {len(cifar_train_data)}\")\nprint(f\"  Validation: {len(cifar_val_data)}\")\nprint(f\"  Test: {len(cifar_test)}\")\n\n# Choose and modify pretrained model\nprint(\"\\nUsing pretrained ResNet18...\")\nmodel = models.resnet18(pretrained=True)\n\n# Modify for CIFAR-10 (10 classes)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)\n\n# Strategy: Fine-tune all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# Train with learning rate scheduling\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nprint(\"\\nTraining on CIFAR-10...\")\nepochs = 10\nbest_val_acc = 0\nhistory = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n\nfor epoch in range(epochs):\n    # Train\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in cifar_train_loader:\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(cifar_train_loader)\n    train_acc = correct / total\n\n    # Validate\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in cifar_val_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_acc = correct / total\n\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}: \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n          f\"Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n\n    # Save best model\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_cifar10_model.pth')\n\n    scheduler.step()\n\n# Load best model and test\nmodel.load_state_dict(torch.load('best_cifar10_model.pth'))\nmodel.eval()\n\ncorrect = 0\ntotal = 0\nclass_correct = [0] * 10\nclass_total = [0] * 10\n\nwith torch.no_grad():\n    for images, labels in cifar_test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        # Per-class accuracy\n        for label, prediction in zip(labels, predicted):\n            if label == prediction:\n                class_correct[label] += 1\n            class_total[label] += 1\n\ntest_acc = correct / total\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS\")\nprint(\"=\"*70)\nprint(f\"Best validation accuracy: {best_val_acc:.4f}\")\nprint(f\"Test accuracy: {test_acc:.4f}\")\n\n# Per-class accuracy\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nprint(\"\\nPer-class accuracy:\")\nfor i, class_name in enumerate(classes):\n    acc = class_correct[i] / class_total[i] if class_total[i] &gt; 0 else 0\n    print(f\"  {class_name:8s}: {acc:.4f} ({class_correct[i]}/{class_total[i]})\")\n\n# Visualize training\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(history['train_acc'])+1)\n\naxes[0].plot(epochs_range, history['train_loss'])\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(epochs_range, history['train_acc'], label='Train')\naxes[1].plot(epochs_range, history['val_acc'], label='Validation')\naxes[1].axhline(y=test_acc, color='r', linestyle='--', label=f'Test: {test_acc:.4f}')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83c\udf89 Challenge complete!\")\nprint(\"\\n\u2713 All Day 14 exercises complete\")\n</code></pre>"},{"location":"old/Week3_Day14/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review transfer learning concepts \u2610 Understand when to freeze vs fine-tune \u2610 Document augmentation strategies \u2610 Write daily reflection</p>"},{"location":"old/Week3_Day14/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept learned today?</li> <li>When should you use feature extraction vs fine-tuning?</li> <li>How does data augmentation improve models?</li> <li>What augmentations are appropriate for your domain?</li> <li>How did transfer learning compare to training from scratch?</li> <li>What would you do differently on your next transfer learning project?</li> </ul> <p>Next: Day 15 - CIFAR-10 Project (Capstone)</p>"},{"location":"old/Week3_Day15/","title":"Week 3, Day 15: CIFAR-10 Project - Color Image Classification","text":""},{"location":"old/Week3_Day15/#daily-goals","title":"Daily Goals","text":"<ul> <li>Complete end-to-end CIFAR-10 classification project</li> <li>Apply all Week 3 concepts (CNNs, architectures, transfer learning, augmentation)</li> <li>Achieve &gt;85% accuracy (target: &gt;90%)</li> <li>Create professional documentation</li> <li>Build portfolio-ready computer vision project</li> </ul>"},{"location":"old/Week3_Day15/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week3_Day15/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week3_Day15/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: CIFAR-10 Image Classification walkthrough (15 min)</p> <p>\u2610 Optional Review: Any Week 3 videos as needed (15 min)</p>"},{"location":"old/Week3_Day15/#project-briefing-30-min","title":"Project Briefing (30 min)","text":"<pre><code>\"\"\"\nCIFAR-10 IMAGE CLASSIFICATION PROJECT\n\nDataset: CIFAR-10\n- 60,000 color images (32\u00d732 RGB)\n- 10 classes: plane, car, bird, cat, deer, dog, frog, horse, ship, truck\n- 50,000 training, 10,000 test\n- More challenging than MNIST!\n\nGoal: Build CNN achieving &gt;85% accuracy (target: &gt;90%)\n\nProject Structure:\nPhase 1: Data Exploration (30 min)\nPhase 2: Baseline Model (45 min)  \nPhase 3: Improved Architecture (60 min)\nPhase 4: Transfer Learning (60 min)\nPhase 5: Analysis &amp; Documentation (45 min)\n\nSuccess Criteria:\n- Minimum: &gt;70% test accuracy\n- Target: &gt;85% test accuracy\n- Stretch: &gt;90% test accuracy\n- Professional documentation\n- Clear visualizations\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport time\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"=\"*70)\nprint(\"CIFAR-10 IMAGE CLASSIFICATION PROJECT\")\nprint(\"Week 3, Day 15 Capstone\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"old/Week3_Day15/#phase-1-data-exploration-30-min","title":"Phase 1: Data Exploration (30 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: DATA EXPLORATION\")\nprint(\"=\"*70)\n\n# Load CIFAR-10\ntransform_basic = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\n\ncifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_basic)\ncifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_basic)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"Training samples: {len(cifar_train)}\")\nprint(f\"Test samples: {len(cifar_test)}\")\nprint(f\"Image shape: {cifar_train[0][0].shape}\")  # [C, H, W]\nprint(f\"Number of classes: 10\")\n\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n# Visualize samples\nfig, axes = plt.subplots(5, 10, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i in range(50):\n    img, label = cifar_train[i]\n    # Denormalize for visualization\n    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\n    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\n    img = torch.clamp(img, 0, 1)\n\n    axes[i].imshow(img.permute(1, 2, 0))\n    axes[i].set_title(classes[label], fontsize=8)\n    axes[i].axis('off')\n\nplt.suptitle('CIFAR-10 Dataset Samples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Class distribution\nlabels = [cifar_train[i][1] for i in range(len(cifar_train))]\nunique, counts = np.unique(labels, return_counts=True)\n\nplt.figure(figsize=(12, 6))\nplt.bar([classes[i] for i in unique], counts, color='steelblue', edgecolor='black')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('CIFAR-10 Class Distribution (Training Set)')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3, axis='y')\nfor i, (cls, count) in enumerate(zip(unique, counts)):\n    plt.text(i, count + 50, str(count), ha='center', fontsize=10)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nClass distribution (perfectly balanced):\")\nfor cls_idx, count in zip(unique, counts):\n    print(f\"  {classes[cls_idx]:8s}: {count} samples\")\n\nprint(\"\\n\u2713 Data exploration complete\")\n</code></pre>"},{"location":"old/Week3_Day15/#phase-2-baseline-model-45-min","title":"Phase 2: Baseline Model (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: BASELINE MODEL\")\nprint(\"=\"*70)\n\n# Split train into train/val\ntrain_size = int(0.9 * len(cifar_train))\nval_size = len(cifar_train) - train_size\ntrain_data, val_data = random_split(cifar_train, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\ntest_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)\n\nprint(f\"Data split:\")\nprint(f\"  Training: {len(train_data)}\")\nprint(f\"  Validation: {len(val_data)}\")\nprint(f\"  Test: {len(cifar_test)}\")\n\n# Simple baseline CNN\nclass BaselineCNN(nn.Module):\n    def __init__(self):\n        super(BaselineCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, 10)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)  # 16x16\n        x = torch.relu(self.conv2(x))\n        x = self.pool(x)  # 8x8\n        x = torch.relu(self.conv3(x))\n        x = self.pool(x)  # 4x4\n        x = x.view(-1, 128 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nbaseline = BaselineCNN()\nprint(\"\\nBaseline CNN:\")\nprint(baseline)\nprint(f\"Parameters: {sum(p.numel() for p in baseline.parameters()):,}\")\n\n# Training function\ndef train_model(model, train_loader, val_loader, epochs, lr, model_name=\"Model\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for images, labels in train_loader:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n\n        # Validate\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        train_acc = train_correct / train_total\n        val_acc = val_correct / val_total\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        print(f\"{model_name} Epoch {epoch+1}/{epochs}: \"\n              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return history\n\nprint(\"\\nTraining baseline...\")\nbaseline_history = train_model(baseline, train_loader, val_loader, \n                               epochs=10, lr=0.001, model_name=\"Baseline\")\n\n# Test baseline\nbaseline.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = baseline(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nbaseline_acc = correct / total\nprint(f\"\\n\ud83c\udfaf Baseline Test Accuracy: {baseline_acc:.4f}\")\n\nprint(\"\\n\u2713 Baseline model complete\")\n</code></pre>"},{"location":"old/Week3_Day15/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week3_Day15/#phase-3-improved-architecture-60-min","title":"Phase 3: Improved Architecture (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: IMPROVED ARCHITECTURE WITH MODERN TECHNIQUES\")\nprint(\"=\"*70)\n\n# Add data augmentation\ntransform_augmented = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\n\ntrain_augmented = datasets.CIFAR10(root='./data', train=True, \n                                   download=True, transform=transform_augmented)\ntrain_aug_data, _ = random_split(train_augmented, [train_size, val_size])\ntrain_aug_loader = DataLoader(train_aug_data, batch_size=128, shuffle=True, num_workers=2)\n\n# Improved CNN with batch norm and better architecture\nclass ImprovedCNN(nn.Module):\n    def __init__(self):\n        super(ImprovedCNN, self).__init__()\n\n        # Block 1\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        # Block 2\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n\n        # Block 3\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.3)\n\n        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # Block 1\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)  # 16x16\n        x = self.dropout(x)\n\n        # Block 2\n        x = torch.relu(self.bn3(self.conv3(x)))\n        x = torch.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)  # 8x8\n        x = self.dropout(x)\n\n        # Block 3\n        x = torch.relu(self.bn5(self.conv5(x)))\n        x = torch.relu(self.bn6(self.conv6(x)))\n        x = self.pool(x)  # 4x4\n        x = self.dropout(x)\n\n        # Classifier\n        x = x.view(-1, 256 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nimproved = ImprovedCNN()\nprint(\"\\nImproved CNN:\")\nprint(f\"Parameters: {sum(p.numel() for p in improved.parameters()):,}\")\n\nprint(\"\\nTraining improved model with augmentation...\")\nimproved_history = train_model(improved, train_aug_loader, val_loader,\n                               epochs=15, lr=0.001, model_name=\"Improved\")\n\n# Test improved\nimproved.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = improved(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nimproved_acc = correct / total\nprint(f\"\\n\ud83c\udfaf Improved Test Accuracy: {improved_acc:.4f}\")\n\nprint(\"\\n\u2713 Improved architecture complete\")\n</code></pre>"},{"location":"old/Week3_Day15/#phase-4-transfer-learning-60-min","title":"Phase 4: Transfer Learning (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: TRANSFER LEARNING WITH RESNET18\")\nprint(\"=\"*70)\n\n# Load pretrained ResNet18\nresnet = models.resnet18(pretrained=True)\n\n# Modify for CIFAR-10\nnum_features = resnet.fc.in_features\nresnet.fc = nn.Linear(num_features, 10)\n\n# Fine-tune all layers\nfor param in resnet.parameters():\n    param.requires_grad = True\n\nprint(f\"\\nResNet-18 (pretrained on ImageNet):\")\nprint(f\"Total parameters: {sum(p.numel() for p in resnet.parameters()):,}\")\n\n# Train with learning rate scheduling\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\nprint(\"\\nTraining ResNet-18 with transfer learning...\")\nepochs = 20\nresnet_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\nbest_val_acc = 0\n\nfor epoch in range(epochs):\n    # Train\n    resnet.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n\n    for images, labels in train_aug_loader:\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    # Validate\n    resnet.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = resnet(images)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    train_loss /= len(train_aug_loader)\n    val_loss /= len(val_loader)\n    train_acc = train_correct / train_total\n    val_acc = val_correct / val_total\n\n    resnet_history['train_loss'].append(train_loss)\n    resnet_history['val_loss'].append(val_loss)\n    resnet_history['train_acc'].append(train_acc)\n    resnet_history['val_acc'].append(val_acc)\n\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(resnet.state_dict(), 'best_cifar10_resnet.pth')\n\n    print(f\"ResNet Epoch {epoch+1}/{epochs}: \"\n          f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n    scheduler.step()\n\n# Load best and test\nresnet.load_state_dict(torch.load('best_cifar10_resnet.pth'))\nresnet.eval()\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nresnet_acc = correct / total\nprint(f\"\\n\ud83c\udfaf ResNet-18 Test Accuracy: {resnet_acc:.4f}\")\n\nprint(\"\\n\u2713 Transfer learning complete\")\n</code></pre>"},{"location":"old/Week3_Day15/#phase-5-analysis-documentation-45-min","title":"Phase 5: Analysis &amp; Documentation (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: COMPREHENSIVE ANALYSIS\")\nprint(\"=\"*70)\n\n# Model comparison\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Model':&lt;20} | {'Test Accuracy':&gt;14} | {'Improvement':&gt;12}\")\nprint(\"=\"*70)\nprint(f\"{'Baseline CNN':&lt;20} | {baseline_acc:&gt;14.4f} | {'baseline':&gt;12}\")\nprint(f\"{'Improved CNN':&lt;20} | {improved_acc:&gt;14.4f} | {f'+{(improved_acc-baseline_acc):.4f}':&gt;12}\")\nprint(f\"{'ResNet-18 Transfer':&lt;20} | {resnet_acc:&gt;14.4f} | {f'+{(resnet_acc-baseline_acc):.4f}':&gt;12}\")\nprint(\"=\"*70)\n\n# Training curves\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Baseline\naxes[0,0].plot(baseline_history['train_loss'], label='Train')\naxes[0,0].plot(baseline_history['val_loss'], label='Val')\naxes[0,0].set_title('Baseline - Loss')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\naxes[1,0].plot(baseline_history['train_acc'], label='Train')\naxes[1,0].plot(baseline_history['val_acc'], label='Val')\naxes[1,0].axhline(y=baseline_acc, color='r', linestyle='--', label=f'Test: {baseline_acc:.4f}')\naxes[1,0].set_title('Baseline - Accuracy')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Improved\naxes[0,1].plot(improved_history['train_loss'], label='Train')\naxes[0,1].plot(improved_history['val_loss'], label='Val')\naxes[0,1].set_title('Improved - Loss')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\naxes[1,1].plot(improved_history['train_acc'], label='Train')\naxes[1,1].plot(improved_history['val_acc'], label='Val')\naxes[1,1].axhline(y=improved_acc, color='r', linestyle='--', label=f'Test: {improved_acc:.4f}')\naxes[1,1].set_title('Improved - Accuracy')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\n# ResNet\naxes[0,2].plot(resnet_history['train_loss'], label='Train')\naxes[0,2].plot(resnet_history['val_loss'], label='Val')\naxes[0,2].set_title('ResNet-18 - Loss')\naxes[0,2].legend()\naxes[0,2].grid(True, alpha=0.3)\n\naxes[1,2].plot(resnet_history['train_acc'], label='Train')\naxes[1,2].plot(resnet_history['val_acc'], label='Val')\naxes[1,2].axhline(y=resnet_acc, color='r', linestyle='--', label=f'Test: {resnet_acc:.4f}')\naxes[1,2].set_title('ResNet-18 - Accuracy')\naxes[1,2].legend()\naxes[1,2].grid(True, alpha=0.3)\n\nplt.suptitle('Training Comparison', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\nprint(\"\\nGenerating confusion matrix for best model...\")\nresnet.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix - ResNet-18 (Accuracy: {resnet_acc:.4f})')\nplt.tight_layout()\nplt.show()\n\n# Per-class accuracy\nprint(\"\\nPer-class Performance:\")\nprint(\"-\" * 50)\nfor i in range(10):\n    class_correct = cm[i, i]\n    class_total = cm[i, :].sum()\n    class_acc = class_correct / class_total if class_total &gt; 0 else 0\n    print(f\"{classes[i]:8s}: {class_acc:.4f} ({class_correct}/{class_total})\")\n\n# Visualize mistakes\nprint(\"\\nVisualizing mistakes...\")\nresnet.eval()\nmistakes = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        probs = torch.softmax(outputs, dim=1)\n        confidences, predicted = torch.max(probs, 1)\n\n        mask = predicted != labels\n        if mask.sum() &gt; 0:\n            for img, true, pred, conf in zip(images[mask], labels[mask], \n                                             predicted[mask], confidences[mask]):\n                mistakes.append((img, true.item(), pred.item(), conf.item()))\n                if len(mistakes) &gt;= 20:\n                    break\n        if len(mistakes) &gt;= 20:\n            break\n\nfig, axes = plt.subplots(4, 5, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, (img, true_label, pred_label, conf) in enumerate(mistakes[:20]):\n    # Denormalize\n    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\n    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\n    img = torch.clamp(img, 0, 1)\n\n    axes[i].imshow(img.permute(1, 2, 0))\n    axes[i].set_title(f'True: {classes[true_label]}\\nPred: {classes[pred_label]} ({conf:.2f})', \n                     fontsize=9)\n    axes[i].axis('off')\n\nplt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Final report\nreport = f\"\"\"\n{'='*70}\nCIFAR-10 PROJECT FINAL REPORT\n{'='*70}\n\nDATASET\n-------\nTraining samples: 50,000\nTest samples: 10,000\nImage size: 32\u00d732 RGB\nClasses: 10 (balanced)\n\nMODELS EVALUATED\n----------------\n1. Baseline CNN\n   - Simple 3-layer CNN\n   - No augmentation, basic dropout\n   - Test Accuracy: {baseline_acc:.4f}\n\n2. Improved CNN\n   - Deeper (6 conv layers)\n   - Batch normalization\n   - Data augmentation\n   - Test Accuracy: {improved_acc:.4f}\n   - Improvement: +{(improved_acc-baseline_acc):.4f}\n\n3. ResNet-18 (Transfer Learning)\n   - Pretrained on ImageNet\n   - Fine-tuned all layers\n   - Data augmentation + LR scheduling\n   - Test Accuracy: {resnet_acc:.4f}\n   - Improvement: +{(resnet_acc-baseline_acc):.4f}\n\nBEST MODEL: ResNet-18 Transfer Learning\nStatus: {'\u2713 TARGET ACHIEVED (&gt;85%)' if resnet_acc &gt; 0.85 else '\u2717 Below target'}\n        {'\ud83c\udf89 STRETCH GOAL (&gt;90%)!' if resnet_acc &gt; 0.90 else ''}\n\nKEY INSIGHTS\n------------\n1. Data augmentation critical for CIFAR-10\n2. Batch normalization stabilizes training\n3. Transfer learning provides major boost\n4. Depth matters but skip connections help\n\nTECHNIQUES APPLIED\n------------------\n\u2713 Custom CNN architectures\n\u2713 Batch normalization\n\u2713 Dropout regularization\n\u2713 Data augmentation\n\u2713 Transfer learning\n\u2713 Learning rate scheduling\n\u2713 Early stopping\n\n{'='*70}\nWeek 3 Complete! \ud83c\udf89\n{'='*70}\n\"\"\"\n\nprint(report)\n\n# Save report\nwith open('cifar10_project_report.txt', 'w') as f:\n    f.write(report)\n\nprint(\"\\n\ud83d\udcc4 Report saved: cifar10_project_report.txt\")\nprint(\"\ud83d\udcbe Model saved: best_cifar10_resnet.pth\")\n\nprint(\"\\n\u2713 Project complete!\")\n</code></pre>"},{"location":"old/Week3_Day15/#reflection-week-review-30-min","title":"Reflection &amp; Week Review (30 min)","text":"<p>\u2610 Review entire Week 3 journey \u2610 Document key learnings \u2610 Celebrate achievements</p>"},{"location":"old/Week3_Day15/#week-3-reflection-prompts-address-all","title":"Week 3 Reflection Prompts (Address All):","text":"<ul> <li>What was the most valuable thing you learned this week?</li> <li>How did your understanding of CNNs evolve?</li> <li>What was your biggest challenge? How did you overcome it?</li> <li>How does your CIFAR-10 accuracy compare to expectations?</li> <li>What connections did you make between Days 11-15?</li> <li>How confident do you feel about computer vision now?</li> <li>What would you do differently on your next CV project?</li> </ul>"},{"location":"old/Week3_Day15/#week-3-achievement-checklist","title":"Week 3 Achievement Checklist:","text":"<p>\u2610 Understood convolutions and their advantages \u2610 Implemented CNNs from scratch \u2610 Built LeNet, AlexNet, VGG, ResNet \u2610 Mastered transfer learning \u2610 Applied data augmentation effectively \u2610 Completed CIFAR-10 project with &gt;85% accuracy \u2610 Created professional documentation \u2610 Built portfolio-ready CV project</p>"},{"location":"old/Week3_Day15/#week-3-complete","title":"\ud83c\udf89 Week 3 Complete!","text":"<p>Achievements Unlocked: - \u2705 CNN fundamentals mastered - \u2705 Classic and modern architectures - \u2705 Transfer learning proficiency - \u2705 Production CV skills - \u2705 CIFAR-10 portfolio project</p> <p>What You Can Now Do: - Build CNNs for any image classification task - Use pretrained models effectively - Design custom architectures - Understand computer vision research papers - Deploy image classification systems</p> <p>Next Week Preview: Week 4 covers advanced topics: RNNs, LSTMs, Transformers, and GANs!</p> <p>Congratulations on completing Week 3! \ud83d\ude80</p> <p>You now have solid computer vision skills and a portfolio project to show for it.</p>"},{"location":"old/Week4_Day16/","title":"Week 4, Day 16: RNNs &amp; LSTMs - Sequential Data Processing","text":""},{"location":"old/Week4_Day16/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand how RNNs process sequential data</li> <li>Learn about vanishing gradients in RNNs</li> <li>Master LSTM architecture and gates</li> <li>Implement RNN and LSTM from scratch</li> <li>Train models on sequential tasks</li> <li>Visualize hidden state evolution</li> </ul>"},{"location":"old/Week4_Day16/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week4_Day16/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week4_Day16/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Recurrent Neural Networks by StatQuest (20 min) Clear explanation of RNN fundamentals</p> <p>\u2610 Watch: LSTM Networks by StatQuest (15 min) Understanding LSTM gates</p> <p>\u2610 Watch: The Unreasonable Effectiveness of RNNs by Andrej Karpathy (20 min) Motivation and applications</p> <p>\u2610 Watch: Illustrated Guide to LSTM's and GRU's (15 min) Visual walkthrough</p> <p>\u2610 Watch: Sequence Models by Andrew Ng (20 min) Comprehensive overview</p>"},{"location":"old/Week4_Day16/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 9.1-9.3 - RNNs</p> <p>\u2610 Read: D2L Chapter 10.1-10.2 - LSTMs</p> <p>\u2610 Read: Understanding LSTM Networks by Chris Olah</p>"},{"location":"old/Week4_Day16/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week4_Day16/#exercise-1-understanding-rnn-forward-pass-45-min","title":"Exercise 1: Understanding RNN Forward Pass (45 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: RNN FORWARD PASS\")\nprint(\"=\"*70)\n\n# Simple RNN from scratch\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Simple RNN implementation\n\n        At each time step:\n        h_t = tanh(W_ih @ x_t + W_hh @ h_{t-1} + b_h)\n        y_t = W_ho @ h_t + b_o\n        \"\"\"\n        # Initialize weights\n        self.W_ih = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n\n        self.W_ho = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_o = np.zeros((output_size, 1))\n\n        self.hidden_size = hidden_size\n\n    def forward(self, X, h_prev=None):\n        \"\"\"\n        Forward pass through sequence\n\n        Args:\n            X: input sequence (seq_len, input_size)\n            h_prev: previous hidden state (hidden_size, 1)\n\n        Returns:\n            outputs: predictions at each time step\n            hidden_states: hidden states at each time step\n        \"\"\"\n        seq_len = X.shape[0]\n\n        # Initialize hidden state\n        if h_prev is None:\n            h = np.zeros((self.hidden_size, 1))\n        else:\n            h = h_prev\n\n        outputs = []\n        hidden_states = [h.copy()]\n\n        # Process sequence\n        for t in range(seq_len):\n            x_t = X[t].reshape(-1, 1)\n\n            # Update hidden state\n            h = np.tanh(self.W_ih @ x_t + self.W_hh @ h + self.b_h)\n\n            # Compute output\n            y_t = self.W_ho @ h + self.b_o\n\n            outputs.append(y_t)\n            hidden_states.append(h.copy())\n\n        return np.array(outputs), np.array(hidden_states)\n\n# Test RNN\nprint(\"\\nTesting simple RNN:\")\nrnn = SimpleRNN(input_size=1, hidden_size=3, output_size=1)\n\n# Simple sequence: [0.1, 0.2, 0.3, 0.4, 0.5]\nX = np.array([[0.1], [0.2], [0.3], [0.4], [0.5]])\nprint(f\"Input sequence: {X.flatten()}\")\n\noutputs, hidden_states = rnn.forward(X)\n\nprint(f\"\\nOutputs shape: {outputs.shape}\")\nprint(f\"Hidden states shape: {hidden_states.shape}\")\n\nprint(f\"\\nOutputs at each time step:\")\nfor t, out in enumerate(outputs):\n    print(f\"  t={t}: {out[0, 0]:.4f}\")\n\n# Visualize hidden state evolution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Hidden state dimensions over time\nfor i in range(rnn.hidden_size):\n    axes[0].plot([h[i, 0] for h in hidden_states], \n                 label=f'h_{i}', marker='o')\naxes[0].set_xlabel('Time Step')\naxes[0].set_ylabel('Hidden State Value')\naxes[0].set_title('Hidden State Evolution')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Output over time\naxes[1].plot(X.flatten(), label='Input', marker='o')\naxes[1].plot([out[0, 0] for out in outputs], label='Output', marker='s')\naxes[1].set_xlabel('Time Step')\naxes[1].set_ylabel('Value')\naxes[1].set_title('Input vs Output Sequence')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key Insight: RNN maintains hidden state that evolves over time!\")\nprint(\"\\n\u2713 Exercise 1 complete\")\n</code></pre>"},{"location":"old/Week4_Day16/#exercise-2-character-level-prediction-60-min","title":"Exercise 2: Character-Level Prediction (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: CHARACTER-LEVEL PREDICTION\")\nprint(\"=\"*70)\n\n# Create simple character sequence\ntext = \"hello world\"\nchars = sorted(list(set(text)))\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for i, ch in enumerate(chars)}\n\nprint(f\"Text: '{text}'\")\nprint(f\"Unique characters: {chars}\")\nprint(f\"Vocabulary size: {len(chars)}\")\n\n# Convert text to indices\nX_text = [char_to_ix[ch] for ch in text[:-1]]\nY_text = [char_to_ix[ch] for ch in text[1:]]\n\nprint(f\"\\nInput sequence (chars): {[text[i] for i in range(len(text)-1)]}\")\nprint(f\"Target sequence (next char): {[text[i] for i in range(1, len(text))]}\")\n\n# Build RNN in PyTorch\nclass CharRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_size):\n        super(CharRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n\n        # RNN layer\n        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n\n        # Output layer\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        # Embed input\n        embedded = self.embedding(x)\n\n        # RNN forward\n        output, hidden = self.rnn(embedded, hidden)\n\n        # Project to vocabulary\n        output = self.fc(output)\n\n        return output, hidden\n\n    def init_hidden(self, batch_size):\n        return torch.zeros(1, batch_size, self.hidden_size)\n\n# Create model\nvocab_size = len(chars)\nhidden_size = 32\nmodel = CharRNN(vocab_size, hidden_size)\n\nprint(f\"\\nCharRNN Model:\")\nprint(model)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Training setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Convert to tensors\nX_tensor = torch.tensor(X_text).unsqueeze(0)  # (1, seq_len)\nY_tensor = torch.tensor(Y_text).unsqueeze(0)\n\nprint(f\"\\nTraining on sequence: '{text}'\")\nprint(\"Learning to predict next character...\")\n\n# Train\nlosses = []\nfor epoch in range(500):\n    # Forward pass\n    hidden = model.init_hidden(1)\n    output, hidden = model(X_tensor, hidden)\n\n    # Reshape for loss\n    loss = criterion(output.view(-1, vocab_size), Y_tensor.view(-1))\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item())\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/500: Loss = {loss.item():.4f}\")\n\n# Plot training\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Character RNN Training')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Test predictions\nprint(\"\\nTesting predictions:\")\nmodel.eval()\nwith torch.no_grad():\n    hidden = model.init_hidden(1)\n    output, _ = model(X_tensor, hidden)\n    predictions = torch.argmax(output, dim=2).squeeze().numpy()\n\n    print(\"\\nInput \u2192 Predicted:\")\n    for i in range(len(X_text)):\n        input_char = ix_to_char[X_text[i]]\n        true_char = ix_to_char[Y_text[i]]\n        pred_char = ix_to_char[predictions[i]]\n        correct = \"\u2713\" if pred_char == true_char else \"\u2717\"\n        print(f\"  '{input_char}' \u2192 '{pred_char}' (true: '{true_char}') {correct}\")\n\nprint(\"\\n\u2713 Exercise 2 complete\")\n</code></pre>"},{"location":"old/Week4_Day16/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week4_Day16/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week4_Day16/#exercise-3-lstm-implementation-70-min","title":"Exercise 3: LSTM Implementation (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: LSTM IMPLEMENTATION\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nLSTM Gates:\n1. Forget gate: f_t = \u03c3(W_f @ [h_{t-1}, x_t] + b_f)\n   - Decides what to remove from cell state\n\n2. Input gate: i_t = \u03c3(W_i @ [h_{t-1}, x_t] + b_i)\n   - Decides what new information to add\n\n3. Cell gate: C\u0303_t = tanh(W_C @ [h_{t-1}, x_t] + b_C)\n   - Creates new candidate cell state\n\n4. Output gate: o_t = \u03c3(W_o @ [h_{t-1}, x_t] + b_o)\n   - Decides what to output\n\nCell state update:\nC_t = f_t * C_{t-1} + i_t * C\u0303_t\n\nHidden state:\nh_t = o_t * tanh(C_t)\n\"\"\")\n\nclass SimpleLSTM:\n    def __init__(self, input_size, hidden_size):\n        \"\"\"Simple LSTM from scratch\"\"\"\n        self.hidden_size = hidden_size\n\n        # Forget gate\n        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_f = np.zeros((hidden_size, 1))\n\n        # Input gate\n        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_i = np.zeros((hidden_size, 1))\n\n        # Cell gate\n        self.W_C = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_C = np.zeros((hidden_size, 1))\n\n        # Output gate\n        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.b_o = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def forward_step(self, x_t, h_prev, C_prev):\n        \"\"\"Single LSTM step\"\"\"\n        # Concatenate input and hidden state\n        combined = np.vstack([h_prev, x_t])\n\n        # Forget gate\n        f_t = self.sigmoid(self.W_f @ combined + self.b_f)\n\n        # Input gate\n        i_t = self.sigmoid(self.W_i @ combined + self.b_i)\n\n        # Cell gate\n        C_tilde = np.tanh(self.W_C @ combined + self.b_C)\n\n        # Update cell state\n        C_t = f_t * C_prev + i_t * C_tilde\n\n        # Output gate\n        o_t = self.sigmoid(self.W_o @ combined + self.b_o)\n\n        # Update hidden state\n        h_t = o_t * np.tanh(C_t)\n\n        # Return gates for visualization\n        gates = {\n            'forget': f_t,\n            'input': i_t,\n            'cell': C_tilde,\n            'output': o_t\n        }\n\n        return h_t, C_t, gates\n\n    def forward(self, X):\n        \"\"\"Forward pass through sequence\"\"\"\n        seq_len = X.shape[0]\n\n        h = np.zeros((self.hidden_size, 1))\n        C = np.zeros((self.hidden_size, 1))\n\n        hidden_states = []\n        cell_states = []\n        all_gates = []\n\n        for t in range(seq_len):\n            x_t = X[t].reshape(-1, 1)\n            h, C, gates = self.forward_step(x_t, h, C)\n\n            hidden_states.append(h.copy())\n            cell_states.append(C.copy())\n            all_gates.append(gates)\n\n        return hidden_states, cell_states, all_gates\n\n# Test LSTM\nlstm = SimpleLSTM(input_size=1, hidden_size=4)\n\n# Test sequence\nX = np.array([[0.1], [0.5], [0.9], [0.3], [0.7]])\nprint(f\"\\nInput sequence: {X.flatten()}\")\n\nhidden_states, cell_states, all_gates = lstm.forward(X)\n\n# Visualize gates\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\ngate_names = ['forget', 'input', 'cell', 'output']\ngate_titles = ['Forget Gate', 'Input Gate', 'Cell Gate', 'Output Gate']\n\nfor idx, (gate_name, title) in enumerate(zip(gate_names, gate_titles)):\n    ax = axes[idx // 2, idx % 2]\n\n    gate_values = [gates[gate_name] for gates in all_gates]\n    gate_array = np.hstack(gate_values).T\n\n    im = ax.imshow(gate_array, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n    ax.set_xlabel('Hidden Unit')\n    ax.set_ylabel('Time Step')\n    ax.set_title(title)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle('LSTM Gate Activations Over Time', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Compare cell state vs hidden state\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Cell state\ncell_array = np.hstack(cell_states).T\nim = axes[0].imshow(cell_array, cmap='viridis', aspect='auto')\naxes[0].set_xlabel('Hidden Unit')\naxes[0].set_ylabel('Time Step')\naxes[0].set_title('Cell State Evolution')\nplt.colorbar(im, ax=axes[0])\n\n# Hidden state\nhidden_array = np.hstack(hidden_states).T\nim = axes[1].imshow(hidden_array, cmap='viridis', aspect='auto')\naxes[1].set_xlabel('Hidden Unit')\naxes[1].set_ylabel('Time Step')\naxes[1].set_title('Hidden State Evolution')\nplt.colorbar(im, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key Insights:\")\nprint(\"- Forget gate controls what to remove from memory\")\nprint(\"- Input gate controls what new info to add\")\nprint(\"- Cell state is long-term memory\")\nprint(\"- Hidden state is output at each step\")\n\nprint(\"\\n\u2713 Exercise 3 complete\")\n</code></pre>"},{"location":"old/Week4_Day16/#exercise-4-sequence-prediction-with-pytorch-lstm-60-min","title":"Exercise 4: Sequence Prediction with PyTorch LSTM (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: SEQUENCE PREDICTION WITH PYTORCH LSTM\")\nprint(\"=\"*70)\n\n# Generate sine wave data\ndef generate_sine_data(seq_len=100, n_sequences=1000):\n    \"\"\"Generate sine wave sequences for prediction\"\"\"\n    X = []\n    Y = []\n\n    for _ in range(n_sequences):\n        start = np.random.rand() * 2 * np.pi\n        x = np.sin(np.linspace(start, start + 4*np.pi, seq_len + 1))\n        X.append(x[:-1])\n        Y.append(x[1:])\n\n    return np.array(X), np.array(Y)\n\nX_train, Y_train = generate_sine_data(seq_len=50, n_sequences=1000)\nX_test, Y_test = generate_sine_data(seq_len=50, n_sequences=100)\n\nprint(f\"Training data: {X_train.shape}\")\nprint(f\"Test data: {X_test.shape}\")\n\n# Visualize sample\nplt.figure(figsize=(12, 5))\nfor i in range(3):\n    plt.plot(X_train[i], label=f'Sequence {i}', alpha=0.7)\nplt.xlabel('Time Step')\nplt.ylabel('Value')\nplt.title('Sample Training Sequences (Sine Waves)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Build LSTM model\nclass LSTMPredictor(nn.Module):\n    def __init__(self, input_size=1, hidden_size=32, num_layers=2):\n        super(LSTMPredictor, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                            batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        # LSTM forward\n        out, _ = self.lstm(x)\n\n        # Project to output\n        out = self.fc(out)\n\n        return out\n\nmodel = LSTMPredictor(input_size=1, hidden_size=32, num_layers=2)\nprint(f\"\\nLSTM Predictor:\")\nprint(model)\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Prepare data\nX_train_tensor = torch.FloatTensor(X_train).unsqueeze(-1)  # (N, seq, 1)\nY_train_tensor = torch.FloatTensor(Y_train).unsqueeze(-1)\nX_test_tensor = torch.FloatTensor(X_test).unsqueeze(-1)\nY_test_tensor = torch.FloatTensor(Y_test).unsqueeze(-1)\n\n# Training\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining LSTM on sine wave prediction...\")\nepochs = 50\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n\n    # Forward\n    predictions = model(X_train_tensor)\n    loss = criterion(predictions, Y_train_tensor)\n\n    # Backward\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    train_losses.append(loss.item())\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.6f}\")\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    test_predictions = model(X_test_tensor)\n    test_loss = criterion(test_predictions, Y_test_tensor)\n\nprint(f\"\\nTest Loss: {test_loss.item():.6f}\")\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Training loss\naxes[0, 0].plot(train_losses)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Sample predictions\nfor i in range(3):\n    idx = i\n\n    true = Y_test[idx]\n    pred = test_predictions[idx].squeeze().numpy()\n\n    axes[0, 1].plot(true, label=f'True {i}', linestyle='--', alpha=0.7)\n    axes[0, 1].plot(pred, label=f'Pred {i}', alpha=0.7)\n\naxes[0, 1].set_xlabel('Time Step')\naxes[0, 1].set_ylabel('Value')\naxes[0, 1].set_title('Sample Predictions')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Single sequence detail\nidx = 0\naxes[1, 0].plot(X_test[idx], label='Input', marker='o', markersize=3)\naxes[1, 0].plot(Y_test[idx], label='True Next', marker='s', markersize=3)\naxes[1, 0].plot(test_predictions[idx].squeeze().numpy(), \n                label='Predicted Next', marker='^', markersize=3)\naxes[1, 0].set_xlabel('Time Step')\naxes[1, 0].set_ylabel('Value')\naxes[1, 0].set_title('Detailed View: Single Sequence')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Error distribution\nerrors = (Y_test_tensor - test_predictions).squeeze().numpy().flatten()\naxes[1, 1].hist(errors, bins=50, edgecolor='black')\naxes[1, 1].set_xlabel('Prediction Error')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title(f'Error Distribution (Mean: {errors.mean():.4f})')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Exercise 4 complete\")\n</code></pre>"},{"location":"old/Week4_Day16/#mini-challenge-many-to-one-sentiment-50-min","title":"Mini-Challenge: Many-to-One Sentiment (50 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: SIMPLE SENTIMENT CLASSIFICATION\")\nprint(\"=\"*70)\n\n# Create simple sentiment dataset\npositive_words = [\n    \"great\", \"excellent\", \"amazing\", \"wonderful\", \"fantastic\",\n    \"love\", \"best\", \"perfect\", \"awesome\", \"brilliant\"\n]\n\nnegative_words = [\n    \"terrible\", \"awful\", \"horrible\", \"worst\", \"hate\",\n    \"bad\", \"poor\", \"disappointing\", \"pathetic\", \"useless\"\n]\n\n# Generate simple sentences\ndef generate_sentence(words, n_words=5):\n    return ' '.join(np.random.choice(words, size=n_words))\n\n# Create dataset\nn_samples = 500\nsentences = []\nlabels = []\n\nfor _ in range(n_samples // 2):\n    sentences.append(generate_sentence(positive_words))\n    labels.append(1)\n    sentences.append(generate_sentence(negative_words))\n    labels.append(0)\n\nprint(f\"Dataset: {len(sentences)} sentences\")\nprint(f\"\\nExamples:\")\nfor i in range(3):\n    sent = sentences[i]\n    label = \"Positive\" if labels[i] == 1 else \"Negative\"\n    print(f\"  '{sent}' \u2192 {label}\")\n\n# Build vocabulary\nall_words = set(' '.join(sentences).split())\nword_to_ix = {word: i for i, word in enumerate(sorted(all_words))}\nvocab_size = len(word_to_ix)\n\nprint(f\"\\nVocabulary size: {vocab_size}\")\n\n# Convert sentences to indices\ndef sentence_to_indices(sentence, word_to_ix, max_len=10):\n    indices = [word_to_ix[w] for w in sentence.split()[:max_len]]\n    # Pad\n    while len(indices) &lt; max_len:\n        indices.append(0)\n    return indices\n\nX = torch.tensor([sentence_to_indices(s, word_to_ix) for s in sentences])\nY = torch.tensor(labels).float()\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"Y shape: {Y.shape}\")\n\n# Build classifier\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=16, hidden_dim=32):\n        super(SentimentLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)\n\n        # LSTM (use final hidden state)\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Classify\n        out = self.fc(hidden.squeeze(0))\n        out = self.sigmoid(out)\n\n        return out\n\nmodel = SentimentLSTM(vocab_size, embedding_dim=16, hidden_dim=32)\nprint(f\"\\nSentiment Classifier:\")\nprint(model)\n\n# Train\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining...\")\nepochs = 100\n\nfor epoch in range(epochs):\n    model.train()\n\n    predictions = model(X).squeeze()\n    loss = criterion(predictions, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 20 == 0:\n        accuracy = ((predictions &gt; 0.5) == Y).float().mean()\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {accuracy:.4f}\")\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X).squeeze()\n    predicted_labels = (predictions &gt; 0.5).long()\n    accuracy = (predicted_labels == Y).float().mean()\n\nprint(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n\n# Test new sentences\ntest_sentences = [\n    \"amazing wonderful excellent\",\n    \"terrible awful horrible\",\n    \"great best love\",\n    \"worst hate bad\"\n]\n\nprint(\"\\nTesting new sentences:\")\nmodel.eval()\nwith torch.no_grad():\n    for sent in test_sentences:\n        X_test = torch.tensor([sentence_to_indices(sent, word_to_ix)])\n        pred = model(X_test).item()\n        label = \"Positive\" if pred &gt; 0.5 else \"Negative\"\n        print(f\"  '{sent}' \u2192 {label} (confidence: {pred:.4f})\")\n\nprint(\"\\n\u2713 Mini-challenge complete!\")\n</code></pre>"},{"location":"old/Week4_Day16/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review RNN and LSTM architectures \u2610 Understand vanishing gradients problem \u2610 Write daily reflection</p>"},{"location":"old/Week4_Day16/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do RNNs differ from feedforward networks?</li> <li>Why do LSTMs solve the vanishing gradient problem?</li> <li>What role does each LSTM gate play?</li> <li>What challenges did you face with sequential modeling?</li> <li>How might you apply RNNs to your domain?</li> </ul> <p>Next: Day 17 - Attention Mechanisms &amp; Transformers</p>"},{"location":"old/Week4_Day17/","title":"Week 4, Day 17: Attention Mechanisms &amp; Transformers","text":""},{"location":"old/Week4_Day17/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand attention mechanism fundamentals</li> <li>Learn self-attention and multi-head attention</li> <li>Grasp Transformer architecture conceptually</li> <li>Implement attention from scratch</li> <li>Understand position encodings</li> <li>Connect to modern LLMs (GPT, BERT)</li> </ul>"},{"location":"old/Week4_Day17/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week4_Day17/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week4_Day17/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Attention Mechanism by StatQuest (15 min) Clear explanation of attention basics</p> <p>\u2610 Watch: Attention in Neural Networks (20 min) Visual explanation</p> <p>\u2610 Watch: Illustrated Guide to Transformers (15 min) Step-by-step walkthrough</p> <p>\u2610 Watch: Attention is All You Need by Yannic Kilcher (30 min) Deep dive into Transformer paper</p> <p>\u2610 Watch: How GPT Models Work (10 min) Connection to modern LLMs</p>"},{"location":"old/Week4_Day17/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: The Illustrated Transformer by Jay Alammar ESSENTIAL - Best visual explanation</p> <p>\u2610 Read: D2L Chapter 11.1-11.3 - Attention Mechanisms</p> <p>\u2610 Read: The Annotated Transformer (skim) Line-by-line code walkthrough</p>"},{"location":"old/Week4_Day17/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week4_Day17/#exercise-1-simple-attention-mechanism-50-min","title":"Exercise 1: Simple Attention Mechanism (50 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: SIMPLE ATTENTION MECHANISM\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nAttention Intuition:\n- Traditional RNN: Fixed-size context vector for entire sequence\n- Attention: Dynamically focus on relevant parts of input\n- Query: What am I looking for?\n- Key: What do I offer?\n- Value: What do I actually contain?\n\nAttention(Q, K, V) = softmax(QK^T / \u221ad_k) V\n\"\"\")\n\ndef simple_attention(query, keys, values):\n    \"\"\"\n    Simple attention mechanism\n\n    Args:\n        query: (d_k,) - what we're looking for\n        keys: (seq_len, d_k) - what each position offers\n        values: (seq_len, d_v) - actual content at each position\n\n    Returns:\n        output: (d_v,) - weighted sum of values\n        weights: (seq_len,) - attention weights\n    \"\"\"\n    # Compute attention scores\n    scores = np.dot(keys, query)  # (seq_len,)\n\n    # Softmax to get weights\n    weights = np.exp(scores) / np.sum(np.exp(scores))\n\n    # Weighted sum of values\n    output = np.dot(weights, values)  # (d_v,)\n\n    return output, weights\n\n# Example: Looking up information\nprint(\"\\nExample: Simple lookup with attention\")\n\n# Sequence: \"The cat sat on mat\"\nwords = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\nseq_len = len(words)\n\n# Simplified: each word has a key and value (random for demo)\nnp.random.seed(42)\nd_k = 4  # key/query dimension\nd_v = 3  # value dimension\n\nkeys = np.random.randn(seq_len, d_k)\nvalues = np.random.randn(seq_len, d_v)\n\n# Query: looking for \"animal\" concept (similar to \"cat\" key)\nquery = keys[1] + np.random.randn(d_k) * 0.1  # Similar to \"cat\"\n\noutput, weights = simple_attention(query, keys, values)\n\nprint(f\"\\nQuery (looking for animal):\")\nprint(f\"Attention weights:\")\nfor word, weight in zip(words, weights):\n    bar = \"\u2588\" * int(weight * 50)\n    print(f\"  {word:5s}: {weight:.4f} {bar}\")\n\nprint(f\"\\nMost attended word: {words[np.argmax(weights)]}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Attention weights\naxes[0].bar(words, weights, color='steelblue', edgecolor='black')\naxes[0].set_ylabel('Attention Weight')\naxes[0].set_title('Attention Weights')\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Heatmap\naxes[1].imshow(weights.reshape(1, -1), cmap='YlOrRd', aspect='auto')\naxes[1].set_xticks(range(seq_len))\naxes[1].set_xticklabels(words)\naxes[1].set_yticks([0])\naxes[1].set_yticklabels(['Attention'])\naxes[1].set_title('Attention Heatmap')\n\nfor i, weight in enumerate(weights):\n    axes[1].text(i, 0, f'{weight:.2f}', ha='center', va='center', \n                fontsize=10, color='black' if weight &lt; 0.5 else 'white')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Attention allows dynamic focus on relevant information!\")\nprint(\"\\n\u2713 Exercise 1 complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#exercise-2-scaled-dot-product-attention-50-min","title":"Exercise 2: Scaled Dot-Product Attention (50 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: SCALED DOT-PRODUCT ATTENTION\")\nprint(\"=\"*70)\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention from Transformer paper\n\n    Attention(Q, K, V) = softmax(QK^T / \u221ad_k) V\n    \"\"\"\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Args:\n            query: (batch, seq_len, d_k)\n            key: (batch, seq_len, d_k)\n            value: (batch, seq_len, d_v)\n            mask: optional mask\n\n        Returns:\n            output: (batch, seq_len, d_v)\n            attention_weights: (batch, seq_len, seq_len)\n        \"\"\"\n        d_k = query.size(-1)\n\n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n\n        # Apply mask (for padding or future tokens)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        output = torch.matmul(attention_weights, value)\n\n        return output, attention_weights\n\n# Test scaled dot-product attention\nattention = ScaledDotProductAttention()\n\n# Create sample inputs\nbatch_size = 2\nseq_len = 5\nd_model = 8\n\nQ = torch.randn(batch_size, seq_len, d_model)\nK = torch.randn(batch_size, seq_len, d_model)\nV = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"\\nInput shapes:\")\nprint(f\"  Query: {Q.shape}\")\nprint(f\"  Key: {K.shape}\")\nprint(f\"  Value: {V.shape}\")\n\noutput, attn_weights = attention(Q, K, V)\n\nprint(f\"\\nOutput shapes:\")\nprint(f\"  Output: {output.shape}\")\nprint(f\"  Attention weights: {attn_weights.shape}\")\n\n# Visualize attention patterns\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nfor batch_idx in range(2):\n    ax = axes[batch_idx]\n\n    weights = attn_weights[batch_idx].detach().numpy()\n\n    im = ax.imshow(weights, cmap='YlOrRd', aspect='auto')\n    ax.set_xlabel('Key Position')\n    ax.set_ylabel('Query Position')\n    ax.set_title(f'Attention Weights (Batch {batch_idx})')\n\n    # Add values\n    for i in range(seq_len):\n        for j in range(seq_len):\n            text = ax.text(j, i, f'{weights[i, j]:.2f}',\n                          ha=\"center\", va=\"center\", color=\"black\" if weights[i, j] &lt; 0.5 else \"white\",\n                          fontsize=8)\n\n    plt.colorbar(im, ax=ax)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Each position can attend to all other positions!\")\nprint(\"\\n\u2713 Exercise 2 complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#exercise-3-multi-head-attention-60-min","title":"Exercise 3: Multi-Head Attention (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: MULTI-HEAD ATTENTION\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nMulti-Head Attention:\n- Instead of single attention, use multiple \"heads\"\n- Each head learns different aspects/relationships\n- Heads computed in parallel\n- Outputs concatenated and projected\n\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\nwhere head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n\"\"\")\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.attention = ScaledDotProductAttention()\n\n    def split_heads(self, x):\n        \"\"\"Split into multiple heads\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        \"\"\"Combine heads back\"\"\"\n        batch_size, num_heads, seq_len, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Linear projections\n        Q = self.W_q(query)\n        K = self.W_k(key)\n        V = self.W_v(value)\n\n        # Split into heads\n        Q = self.split_heads(Q)  # (batch, heads, seq, d_k)\n        K = self.split_heads(K)\n        V = self.split_heads(V)\n\n        # Apply attention\n        output, attn_weights = self.attention(Q, K, V, mask)\n\n        # Combine heads\n        output = self.combine_heads(output)\n\n        # Final projection\n        output = self.W_o(output)\n\n        return output, attn_weights\n\n# Test multi-head attention\nd_model = 64\nnum_heads = 8\nseq_len = 10\nbatch_size = 2\n\nmha = MultiHeadAttention(d_model, num_heads)\n\nprint(f\"\\nMulti-Head Attention:\")\nprint(f\"  d_model: {d_model}\")\nprint(f\"  num_heads: {num_heads}\")\nprint(f\"  d_k (per head): {d_model // num_heads}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n\n# Test input\nX = torch.randn(batch_size, seq_len, d_model)\nprint(f\"\\nInput: {X.shape}\")\n\noutput, attn_weights = mha(X, X, X)  # Self-attention\n\nprint(f\"Output: {output.shape}\")\nprint(f\"Attention weights: {attn_weights.shape}\")\n\n# Visualize different heads\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\n\nbatch_idx = 0\nweights = attn_weights[batch_idx].detach().numpy()\n\nfor head in range(num_heads):\n    ax = axes[head]\n\n    head_weights = weights[head]\n\n    im = ax.imshow(head_weights, cmap='YlOrRd', aspect='auto')\n    ax.set_title(f'Head {head}')\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Query')\n\nplt.suptitle('Multi-Head Attention Patterns', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Different heads learn different patterns!\")\nprint(\"\\n\u2713 Exercise 3 complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week4_Day17/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week4_Day17/#exercise-4-position-encodings-40-min","title":"Exercise 4: Position Encodings (40 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: POSITION ENCODINGS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nPosition Encodings:\n- Transformers have no recurrence \u2192 no inherent position info\n- Add positional information to embeddings\n- Use sine/cosine functions of different frequencies\n\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\"\"\")\n\ndef get_positional_encoding(seq_len, d_model):\n    \"\"\"Generate positional encodings\"\"\"\n    pe = np.zeros((seq_len, d_model))\n\n    position = np.arange(0, seq_len).reshape(-1, 1)\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n\n    return pe\n\n# Generate position encodings\nseq_len = 50\nd_model = 128\n\npe = get_positional_encoding(seq_len, d_model)\n\nprint(f\"Position encodings shape: {pe.shape}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Full encoding heatmap\nim = axes[0].imshow(pe, cmap='RdBu', aspect='auto')\naxes[0].set_xlabel('Embedding Dimension')\naxes[0].set_ylabel('Position')\naxes[0].set_title('Positional Encodings')\nplt.colorbar(im, ax=axes[0])\n\n# First few dimensions over positions\nfor dim in range(8):\n    axes[1].plot(pe[:, dim], label=f'Dim {dim}', alpha=0.7)\naxes[1].set_xlabel('Position')\naxes[1].set_ylabel('Encoding Value')\naxes[1].set_title('First 8 Dimensions')\naxes[1].legend(ncol=2)\naxes[1].grid(True, alpha=0.3)\n\n# Specific positions across all dimensions\nfor pos in [0, 10, 25, 40]:\n    axes[2].plot(pe[pos], label=f'Pos {pos}', alpha=0.7)\naxes[2].set_xlabel('Dimension')\naxes[2].set_ylabel('Encoding Value')\naxes[2].set_title('Different Positions')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Position encodings are unique for each position!\")\nprint(\"   Sine/cosine allow the model to learn relative positions\")\n\nprint(\"\\n\u2713 Exercise 4 complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#exercise-5-simple-transformer-block-70-min","title":"Exercise 5: Simple Transformer Block (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 5: TRANSFORMER ENCODER BLOCK\")\nprint(\"=\"*70)\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"\n    Single Transformer Encoder Block\n\n    Components:\n    1. Multi-Head Self-Attention\n    2. Add &amp; Norm\n    3. Feed-Forward Network\n    4. Add &amp; Norm\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(TransformerEncoderBlock, self).__init__()\n\n        # Multi-head attention\n        self.attention = MultiHeadAttention(d_model, num_heads)\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # Layer norms\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention with residual\n        attn_output, attn_weights = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Feed-forward with residual\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n\n        return x, attn_weights\n\n# Create encoder block\nd_model = 64\nnum_heads = 8\nd_ff = 256\n\nencoder = TransformerEncoderBlock(d_model, num_heads, d_ff)\n\nprint(f\"Transformer Encoder Block:\")\nprint(f\"  d_model: {d_model}\")\nprint(f\"  num_heads: {num_heads}\")\nprint(f\"  d_ff: {d_ff}\")\nprint(f\"  Parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n\n# Test\nbatch_size = 2\nseq_len = 10\nX = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"\\nInput: {X.shape}\")\n\noutput, attn_weights = encoder(X)\n\nprint(f\"Output: {output.shape}\")\nprint(f\"Attention weights: {attn_weights.shape}\")\n\n# Visualize attention in encoder\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nweights = attn_weights[0, 0].detach().numpy()  # First batch, first head\n\nim = ax.imshow(weights, cmap='YlOrRd', aspect='auto')\nax.set_xlabel('Key Position')\nax.set_ylabel('Query Position')\nax.set_title('Self-Attention in Transformer Encoder')\n\nfor i in range(seq_len):\n    for j in range(seq_len):\n        text = ax.text(j, i, f'{weights[i, j]:.2f}',\n                      ha=\"center\", va=\"center\",\n                      color=\"black\" if weights[i, j] &lt; 0.5 else \"white\",\n                      fontsize=8)\n\nplt.colorbar(im, ax=ax)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Transformer encoder allows each position to attend to all positions!\")\nprint(\"\\n\u2713 Exercise 5 complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#mini-challenge-sequence-classification-with-transformer-70-min","title":"Mini-Challenge: Sequence Classification with Transformer (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: SEQUENCE CLASSIFICATION\")\nprint(\"=\"*70)\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"Simple Transformer for sequence classification\"\"\"\n    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_classes, max_seq_len=100):\n        super(TransformerClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n        # Positional encoding\n        self.register_buffer('pos_encoding', \n                           torch.FloatTensor(get_positional_encoding(max_seq_len, d_model)))\n\n        self.encoder = TransformerEncoderBlock(d_model, num_heads, d_ff)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, num_classes)\n        )\n\n    def forward(self, x):\n        # Embed\n        seq_len = x.size(1)\n        embedded = self.embedding(x)\n\n        # Add positional encoding\n        embedded = embedded + self.pos_encoding[:seq_len, :]\n\n        # Encode\n        encoded, attn_weights = self.encoder(embedded)\n\n        # Use mean pooling for classification\n        pooled = encoded.mean(dim=1)\n\n        # Classify\n        output = self.classifier(pooled)\n\n        return output, attn_weights\n\n# Use previous simple sentiment data\nfrom Week4_Day16 import positive_words, negative_words  # Assume available\n\ndef generate_sentence(words, n_words=5):\n    return ' '.join(np.random.choice(words, size=n_words))\n\n# Generate data\nn_samples = 1000\nsentences = []\nlabels = []\n\nfor _ in range(n_samples // 2):\n    sentences.append(generate_sentence(positive_words))\n    labels.append(1)\n    sentences.append(generate_sentence(negative_words))\n    labels.append(0)\n\n# Build vocabulary\nall_words = set(' '.join(sentences).split())\nword_to_ix = {word: i+1 for i, word in enumerate(sorted(all_words))}  # 0 for padding\nvocab_size = len(word_to_ix) + 1\n\nprint(f\"Dataset: {len(sentences)} sentences\")\nprint(f\"Vocabulary: {vocab_size} words\")\n\n# Convert to tensors\ndef sentence_to_indices(sentence, word_to_ix, max_len=10):\n    indices = [word_to_ix.get(w, 0) for w in sentence.split()[:max_len]]\n    while len(indices) &lt; max_len:\n        indices.append(0)\n    return indices\n\nX = torch.tensor([sentence_to_indices(s, word_to_ix) for s in sentences])\nY = torch.tensor(labels).long()\n\n# Create model\nmodel = TransformerClassifier(\n    vocab_size=vocab_size,\n    d_model=32,\n    num_heads=4,\n    d_ff=128,\n    num_classes=2,\n    max_seq_len=10\n)\n\nprint(f\"\\nTransformer Classifier:\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"\\nTraining...\")\nepochs = 50\n\nfor epoch in range(epochs):\n    model.train()\n\n    outputs, _ = model(X)\n    loss = criterion(outputs, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        with torch.no_grad():\n            predictions = torch.argmax(outputs, dim=1)\n            accuracy = (predictions == Y).float().mean()\n            print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {accuracy:.4f}\")\n\n# Final evaluation\nmodel.eval()\nwith torch.no_grad():\n    outputs, attn_weights = model(X)\n    predictions = torch.argmax(outputs, dim=1)\n    accuracy = (predictions == Y).float().mean()\n\nprint(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n\n# Test sentences\ntest_sentences = [\n    \"amazing excellent wonderful great\",\n    \"terrible horrible awful bad\"\n]\n\nprint(\"\\nTesting:\")\nmodel.eval()\nwith torch.no_grad():\n    for sent in test_sentences:\n        X_test = torch.tensor([sentence_to_indices(sent, word_to_ix)])\n        output, attn = model(X_test)\n        pred = torch.argmax(output, dim=1).item()\n        label = \"Positive\" if pred == 1 else \"Negative\"\n        conf = torch.softmax(output, dim=1)[0, pred].item()\n        print(f\"  '{sent}' \u2192 {label} ({conf:.4f})\")\n\nprint(\"\\n\ud83c\udf89 Transformer classifier complete!\")\nprint(\"\ud83d\udca1 This is a simplified version of BERT's architecture!\")\n\nprint(\"\\n\u2713 Mini-challenge complete\")\n</code></pre>"},{"location":"old/Week4_Day17/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review attention mechanism thoroughly \u2610 Understand Transformer architecture \u2610 Connect to modern LLMs \u2610 Write daily reflection</p>"},{"location":"old/Week4_Day17/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How does attention differ from RNN hidden states?</li> <li>Why are Transformers so powerful?</li> <li>What role do position encodings play?</li> <li>How does multi-head attention help?</li> <li>What connections do you see to GPT/BERT?</li> </ul> <p>Next: Day 18 - NLP Fundamentals</p>"},{"location":"old/Week4_Day18/","title":"Week 4, Day 18: NLP Fundamentals - Text Processing &amp; Embeddings","text":""},{"location":"old/Week4_Day18/#daily-goals","title":"Daily Goals","text":"<ul> <li>Master text preprocessing and tokenization</li> <li>Understand word embeddings (Word2Vec, GloVe)</li> <li>Learn embedding spaces and semantic relationships</li> <li>Process text data for deep learning models</li> <li>Build text classification with embeddings</li> <li>Use pretrained embeddings</li> </ul>"},{"location":"old/Week4_Day18/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week4_Day18/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week4_Day18/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: Word Embeddings by StatQuest (15 min) Clear explanation of embeddings</p> <p>\u2610 Watch: Word2Vec Explained (20 min) Understanding Word2Vec</p> <p>\u2610 Watch: GloVe: Global Vectors for Word Representation (15 min) Another embedding approach</p> <p>\u2610 Watch: Illustrated Word2Vec (20 min) Visual walkthrough</p> <p>\u2610 Watch: NLP Preprocessing (20 min) Text cleaning and preparation</p>"},{"location":"old/Week4_Day18/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 15.1-15.3 - Word Embeddings</p> <p>\u2610 Read: GloVe Paper - introduction</p> <p>\u2610 Read: PyTorch Text Tutorial</p>"},{"location":"old/Week4_Day18/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week4_Day18/#exercise-1-text-preprocessing-pipeline-50-min","title":"Exercise 1: Text Preprocessing Pipeline (50 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: TEXT PREPROCESSING\")\nprint(\"=\"*70)\n\n# Sample text corpus\ncorpus = \"\"\"\nNatural language processing is a subfield of artificial intelligence.\nIt focuses on enabling computers to understand human language.\nText preprocessing is crucial for NLP tasks.\nTokenization splits text into words or subwords.\nWord embeddings represent words as dense vectors.\n\"\"\"\n\nprint(f\"Original corpus:\\n{corpus}\\n\")\n\n# Step 1: Lowercase and clean\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\n\ncleaned = clean_text(corpus)\nprint(f\"After cleaning:\\n{cleaned}\\n\")\n\n# Step 2: Tokenization\ndef tokenize(text):\n    return text.split()\n\ntokens = tokenize(cleaned)\nprint(f\"Tokens: {len(tokens)} words\")\nprint(f\"First 20 tokens: {tokens[:20]}\\n\")\n\n# Step 3: Build vocabulary\ndef build_vocab(tokens, min_freq=1):\n    counter = Counter(tokens)\n    vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}\n\n    for word, freq in counter.items():\n        if freq &gt;= min_freq:\n            vocab[word] = len(vocab)\n\n    return vocab\n\nvocab = build_vocab(tokens)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Sample vocabulary: {list(vocab.items())[:15]}\\n\")\n\n# Step 4: Convert text to indices\ndef text_to_indices(text, vocab):\n    tokens = tokenize(clean_text(text))\n    return [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\n\nsample_text = \"Natural language processing uses embeddings\"\nindices = text_to_indices(sample_text, vocab)\nprint(f\"Sample: '{sample_text}'\")\nprint(f\"Indices: {indices}\\n\")\n\n# Step 5: Padding sequences\ndef pad_sequences(sequences, max_len, pad_value=0):\n    padded = []\n    for seq in sequences:\n        if len(seq) &lt; max_len:\n            seq = seq + [pad_value] * (max_len - len(seq))\n        else:\n            seq = seq[:max_len]\n        padded.append(seq)\n    return padded\n\n# Example sequences\nsequences = [\n    [5, 10, 15],\n    [3, 7, 11, 14, 18],\n    [2, 4]\n]\n\npadded = pad_sequences(sequences, max_len=6)\nprint(f\"Original sequences: {sequences}\")\nprint(f\"Padded sequences: {padded}\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nim = ax.imshow(padded, cmap='viridis', aspect='auto')\nax.set_xlabel('Position')\nax.set_ylabel('Sequence')\nax.set_title('Padded Sequences')\nplt.colorbar(im, ax=ax, label='Token Index')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Exercise 1 complete\")\n</code></pre>"},{"location":"old/Week4_Day18/#exercise-2-word2vec-from-scratch-70-min","title":"Exercise 2: Word2Vec from Scratch (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: WORD2VEC IMPLEMENTATION\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nWord2Vec: Learn word embeddings from context\nTwo architectures:\n1. Skip-gram: Predict context from word\n2. CBOW: Predict word from context\n\nWe'll implement simple Skip-gram\n\"\"\")\n\nclass SkipGram(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SkipGram, self).__init__()\n\n        # Target word embedding\n        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # Context word embedding\n        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, target, context):\n        # Get embeddings\n        target_embed = self.target_embeddings(target)  # (batch, embed_dim)\n        context_embed = self.context_embeddings(context)  # (batch, embed_dim)\n\n        # Dot product (similarity)\n        scores = torch.sum(target_embed * context_embed, dim=1)\n\n        # Sigmoid for binary classification\n        return torch.sigmoid(scores)\n\n# Generate training data\ndef generate_skipgram_data(corpus, vocab, window_size=2):\n    tokens = tokenize(clean_text(corpus))\n    indices = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\n\n    pairs = []\n    for i, target in enumerate(indices):\n        # Context window\n        start = max(0, i - window_size)\n        end = min(len(indices), i + window_size + 1)\n\n        for j in range(start, end):\n            if i != j:\n                pairs.append((target, indices[j], 1))  # Positive pair\n\n        # Negative sampling\n        for _ in range(2):\n            neg_context = np.random.randint(2, len(vocab))  # Random word\n            pairs.append((target, neg_context, 0))  # Negative pair\n\n    return pairs\n\n# Generate data\ntraining_pairs = generate_skipgram_data(corpus, vocab, window_size=2)\nprint(f\"Training pairs: {len(training_pairs)}\")\nprint(f\"Sample pairs: {training_pairs[:5]}\\n\")\n\n# Create model\nvocab_size = len(vocab)\nembedding_dim = 10\n\nmodel = SkipGram(vocab_size, embedding_dim)\nprint(f\"Skip-gram model:\")\nprint(f\"  Vocabulary: {vocab_size}\")\nprint(f\"  Embedding dim: {embedding_dim}\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n\n# Prepare tensors\ntargets = torch.tensor([p[0] for p in training_pairs])\ncontexts = torch.tensor([p[1] for p in training_pairs])\nlabels = torch.tensor([p[2] for p in training_pairs], dtype=torch.float)\n\n# Train\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nprint(\"Training Word2Vec...\")\nepochs = 500\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward\n    predictions = model(targets, contexts)\n    loss = criterion(predictions, labels)\n\n    # Backward\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item())\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}\")\n\n# Plot training\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Word2Vec Training')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Extract embeddings\nembeddings = model.target_embeddings.weight.data.numpy()\nprint(f\"\\nLearned embeddings shape: {embeddings.shape}\")\n\n# Find similar words\ndef find_similar(word, vocab, embeddings, top_k=3):\n    if word not in vocab:\n        return []\n\n    word_idx = vocab[word]\n    word_embed = embeddings[word_idx]\n\n    # Compute cosine similarity\n    similarities = []\n    for idx, embed in enumerate(embeddings):\n        if idx != word_idx and idx &gt; 1:  # Skip PAD, UNK, and self\n            sim = np.dot(word_embed, embed) / (np.linalg.norm(word_embed) * np.linalg.norm(embed))\n            similarities.append((idx, sim))\n\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    idx_to_word = {v: k for k, v in vocab.items()}\n    return [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n\n# Test similarity\ntest_words = ['language', 'text', 'word']\nprint(\"\\nWord similarities:\")\nfor word in test_words:\n    if word in vocab:\n        similar = find_similar(word, vocab, embeddings, top_k=3)\n        print(f\"\\n'{word}':\")\n        for similar_word, sim in similar:\n            print(f\"  {similar_word}: {sim:.4f}\")\n\nprint(\"\\n\u2713 Exercise 2 complete\")\n</code></pre>"},{"location":"old/Week4_Day18/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week4_Day18/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week4_Day18/#exercise-3-using-pretrained-embeddings-60-min","title":"Exercise 3: Using Pretrained Embeddings (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: PRETRAINED EMBEDDINGS\")\nprint(\"=\"*70)\n\n# Simulate GloVe embeddings (in practice, download from Stanford)\nprint(\"Note: In practice, download GloVe from https://nlp.stanford.edu/projects/glove/\")\nprint(\"For this exercise, we'll simulate the concept\\n\")\n\n# Create text classifier with embeddings\nclass EmbeddingClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_classes, pretrained_embeddings=None):\n        super(EmbeddingClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # Load pretrained if provided\n        if pretrained_embeddings is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n            self.embedding.weight.requires_grad = False  # Freeze\n\n        self.lstm = nn.LSTM(embedding_dim, 64, batch_first=True)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)\n\n        # LSTM\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Classify\n        output = self.fc(hidden.squeeze(0))\n        return output\n\n# Create sample classification dataset\npositive_sents = [\n    \"this movie is great\",\n    \"excellent film loved it\",\n    \"amazing performance wonderful\",\n    \"fantastic story brilliant acting\",\n    \"loved every minute best\"\n]\n\nnegative_sents = [\n    \"terrible movie hated it\",\n    \"awful film worst ever\",\n    \"boring plot bad acting\",\n    \"disappointing waste of time\",\n    \"horrible terrible poor\"\n]\n\nsentences = positive_sents + negative_sents\nlabels_list = [1] * len(positive_sents) + [0] * len(negative_sents)\n\n# Build vocab from this dataset\nall_tokens = []\nfor sent in sentences:\n    all_tokens.extend(tokenize(clean_text(sent)))\n\ndataset_vocab = build_vocab(all_tokens)\nprint(f\"Dataset vocabulary: {len(dataset_vocab)} words\\n\")\n\n# Convert to indices and pad\nmax_len = 8\nX_indices = [text_to_indices(sent, dataset_vocab) for sent in sentences]\nX_padded = pad_sequences(X_indices, max_len)\n\nX_tensor = torch.tensor(X_padded)\nY_tensor = torch.tensor(labels_list)\n\nprint(f\"Data shapes:\")\nprint(f\"  X: {X_tensor.shape}\")\nprint(f\"  Y: {Y_tensor.shape}\\n\")\n\n# Train without pretrained embeddings\nmodel_scratch = EmbeddingClassifier(len(dataset_vocab), embedding_dim=16, num_classes=2)\nprint(\"Model from scratch:\")\nprint(f\"  Parameters: {sum(p.numel() for p in model_scratch.parameters()):,}\")\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_scratch.parameters(), lr=0.01)\n\nprint(\"\\nTraining from scratch...\")\nfor epoch in range(100):\n    outputs = model_scratch(X_tensor)\n    loss = criterion(outputs, Y_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 20 == 0:\n        preds = torch.argmax(outputs, dim=1)\n        acc = (preds == Y_tensor).float().mean()\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\n# Compare with using our trained embeddings\npretrained_emb = embeddings[:len(dataset_vocab)]  # Use embeddings from Exercise 2\nmodel_pretrained = EmbeddingClassifier(len(dataset_vocab), embedding_dim=10, num_classes=2,\n                                      pretrained_embeddings=pretrained_emb)\n\nprint(\"\\n\\nModel with pretrained embeddings:\")\nprint(f\"  Parameters: {sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad):,} (trainable)\")\n\noptimizer_pre = torch.optim.Adam(filter(lambda p: p.requires_grad, model_pretrained.parameters()), lr=0.01)\n\nprint(\"\\nTraining with pretrained...\")\nfor epoch in range(100):\n    outputs = model_pretrained(X_tensor)\n    loss = criterion(outputs, Y_tensor)\n\n    optimizer_pre.zero_grad()\n    loss.backward()\n    optimizer_pre.step()\n\n    if (epoch + 1) % 20 == 0:\n        preds = torch.argmax(outputs, dim=1)\n        acc = (preds == Y_tensor).float().mean()\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\nprint(\"\\n\ud83d\udca1 Pretrained embeddings provide better starting point!\")\nprint(\"   Especially valuable with limited training data\")\n\nprint(\"\\n\u2713 Exercise 3 complete\")\n</code></pre>"},{"location":"old/Week4_Day18/#exercise-4-embedding-space-visualization-45-min","title":"Exercise 4: Embedding Space Visualization (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: VISUALIZING EMBEDDING SPACE\")\nprint(\"=\"*70)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Use embeddings from Exercise 2\nprint(f\"Embedding space: {embeddings.shape}\\n\")\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings_2d_pca = pca.fit_transform(embeddings[2:])  # Skip PAD and UNK\n\n# t-SNE to 2D\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d_tsne = tsne.fit_transform(embeddings[2:])\n\n# Get word labels\nidx_to_word = {v: k for k, v in vocab.items()}\nwords = [idx_to_word[i] for i in range(2, len(vocab))]\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# PCA\naxes[0].scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], alpha=0.6)\nfor i, word in enumerate(words):\n    axes[0].annotate(word, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]),\n                    fontsize=9, alpha=0.7)\naxes[0].set_title('Word Embeddings (PCA)')\naxes[0].set_xlabel('PC 1')\naxes[0].set_ylabel('PC 2')\naxes[0].grid(True, alpha=0.3)\n\n# t-SNE\naxes[1].scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], alpha=0.6)\nfor i, word in enumerate(words):\n    axes[1].annotate(word, (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]),\n                    fontsize=9, alpha=0.7)\naxes[1].set_title('Word Embeddings (t-SNE)')\naxes[1].set_xlabel('Dimension 1')\naxes[1].set_ylabel('Dimension 2')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\ud83d\udca1 Similar words cluster together in embedding space!\")\nprint(\"   This is how models understand semantic relationships\")\n\nprint(\"\\n\u2713 Exercise 4 complete\")\n</code></pre>"},{"location":"old/Week4_Day18/#mini-challenge-text-classification-pipeline-70-min","title":"Mini-Challenge: Text Classification Pipeline (70 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: COMPLETE TEXT CLASSIFICATION\")\nprint(\"=\"*70)\n\n# Build complete pipeline\nclass TextClassificationPipeline:\n    def __init__(self, embedding_dim=32, hidden_dim=64):\n        self.vocab = None\n        self.model = None\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.max_len = 50\n\n    def build_vocab(self, texts, min_freq=2):\n        \"\"\"Build vocabulary from texts\"\"\"\n        tokens = []\n        for text in texts:\n            tokens.extend(tokenize(clean_text(text)))\n\n        self.vocab = build_vocab(tokens, min_freq=min_freq)\n        return self.vocab\n\n    def preprocess(self, texts):\n        \"\"\"Convert texts to padded indices\"\"\"\n        sequences = [text_to_indices(text, self.vocab) for text in texts]\n        padded = pad_sequences(sequences, self.max_len)\n        return torch.tensor(padded)\n\n    def build_model(self, num_classes):\n        \"\"\"Create classification model\"\"\"\n        self.model = EmbeddingClassifier(\n            vocab_size=len(self.vocab),\n            embedding_dim=self.embedding_dim,\n            num_classes=num_classes\n        )\n        return self.model\n\n    def train(self, texts, labels, epochs=50, lr=0.001):\n        \"\"\"Train the model\"\"\"\n        X = self.preprocess(texts)\n        Y = torch.tensor(labels)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n\n        history = {'loss': [], 'acc': []}\n\n        for epoch in range(epochs):\n            self.model.train()\n\n            outputs = self.model(X)\n            loss = criterion(outputs, Y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            preds = torch.argmax(outputs, dim=1)\n            acc = (preds == Y).float().mean().item()\n\n            history['loss'].append(loss.item())\n            history['acc'].append(acc)\n\n            if (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}, Acc = {acc:.4f}\")\n\n        return history\n\n    def predict(self, texts):\n        \"\"\"Predict on new texts\"\"\"\n        X = self.preprocess(texts)\n\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(X)\n            predictions = torch.argmax(outputs, dim=1)\n\n        return predictions.numpy()\n\n# Create larger dataset\npositive = [\n    \"excellent product highly recommend\",\n    \"great quality very satisfied\",\n    \"amazing service love it\",\n    \"wonderful experience best purchase\",\n    \"fantastic item exceeded expectations\"\n] * 20\n\nnegative = [\n    \"terrible quality waste money\",\n    \"awful product very disappointed\",\n    \"horrible service never again\",\n    \"worst purchase big mistake\",\n    \"poor quality not recommended\"\n] * 20\n\ntexts = positive + negative\nlabels = [1] * len(positive) + [0] * len(negative)\n\n# Shuffle\nindices = np.random.permutation(len(texts))\ntexts = [texts[i] for i in indices]\nlabels = [labels[i] for i in indices]\n\nprint(f\"Dataset: {len(texts)} samples\\n\")\n\n# Create pipeline\npipeline = TextClassificationPipeline(embedding_dim=32, hidden_dim=64)\n\n# Build vocab\nvocab = pipeline.build_vocab(texts, min_freq=2)\nprint(f\"Vocabulary: {len(vocab)} words\")\n\n# Build model\nmodel = pipeline.build_model(num_classes=2)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n\n# Train\nprint(\"Training...\")\nhistory = pipeline.train(texts, labels, epochs=50, lr=0.001)\n\n# Visualize training\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(history['loss'])\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(history['acc'])\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Training Accuracy')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Test predictions\ntest_texts = [\n    \"amazing quality love this product\",\n    \"terrible experience very unhappy\",\n    \"great purchase highly satisfied\",\n    \"awful quality waste of money\"\n]\n\npredictions = pipeline.predict(test_texts)\nprint(\"\\nTest predictions:\")\nfor text, pred in zip(test_texts, predictions):\n    label = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"  '{text}' \u2192 {label}\")\n\nprint(\"\\n\ud83c\udf89 Complete text classification pipeline!\")\nprint(\"\\n\u2713 Mini-challenge complete\")\n</code></pre>"},{"location":"old/Week4_Day18/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review text preprocessing steps \u2610 Understand word embeddings \u2610 Connect embeddings to Transformers \u2610 Write daily reflection</p>"},{"location":"old/Week4_Day18/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do word embeddings capture semantic meaning?</li> <li>Why are embeddings better than one-hot encoding?</li> <li>What is the difference between Word2Vec and GloVe?</li> <li>How do embeddings relate to Transformer models?</li> <li>What preprocessing steps are most critical?</li> </ul> <p>Next: Day 19 - Generative Adversarial Networks</p>"},{"location":"old/Week4_Day19/","title":"Week 4, Day 19: Generative Adversarial Networks (GANs)","text":""},{"location":"old/Week4_Day19/#daily-goals","title":"Daily Goals","text":"<ul> <li>Understand GAN architecture and training dynamics</li> <li>Learn generator and discriminator roles</li> <li>Implement simple GAN from scratch</li> <li>Train GAN on MNIST</li> <li>Understand mode collapse and solutions</li> <li>Generate new images</li> </ul>"},{"location":"old/Week4_Day19/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week4_Day19/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week4_Day19/#video-learning-90-min","title":"Video Learning (90 min)","text":"<p>\u2610 Watch: GANs Explained by Computerphile (10 min) Simple introduction to GANs</p> <p>\u2610 Watch: Generative Adversarial Networks by StatQuest (15 min) Clear breakdown of GAN components</p> <p>\u2610 Watch: GANs from Scratch by Aladdin Persson (25 min) Implementation walkthrough</p> <p>\u2610 Watch: DCGAN Tutorial (20 min) Deep Convolutional GANs</p> <p>\u2610 Watch: GAN Tips and Tricks (20 min) Making GANs train successfully</p>"},{"location":"old/Week4_Day19/#reference-material-30-min","title":"Reference Material (30 min)","text":"<p>\u2610 Read: D2L Chapter 20.1 - GANs</p> <p>\u2610 Read: Original GAN Paper - introduction</p> <p>\u2610 Read: DCGAN Paper - architecture guidelines</p>"},{"location":"old/Week4_Day19/#hands-on-coding-part-1-2-hours","title":"Hands-on Coding - Part 1 (2 hours)","text":""},{"location":"old/Week4_Day19/#exercise-1-understanding-gan-components-45-min","title":"Exercise 1: Understanding GAN Components (45 min)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"EXERCISE 1: GAN COMPONENTS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nGAN Architecture:\n\nGenerator: Noise \u2192 Fake Data\n- Input: Random noise vector z (latent space)\n- Output: Generated sample (e.g., image)\n- Goal: Fool the discriminator\n\nDiscriminator: Data \u2192 Real/Fake Classification\n- Input: Sample (real or fake)\n- Output: Probability it's real\n- Goal: Distinguish real from fake\n\nTraining: Minimax game\n- Discriminator maximizes: log(D(x)) + log(1 - D(G(z)))\n- Generator minimizes: log(1 - D(G(z)))\n  (or equivalently, maximizes log(D(G(z))))\n\"\"\")\n\n# Simple Generator\nclass SimpleGenerator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(SimpleGenerator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.LeakyReLU(0.2),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, output_dim),\n            nn.Tanh()  # Output in [-1, 1]\n        )\n\n    def forward(self, z):\n        return self.model(z)\n\n# Simple Discriminator\nclass SimpleDiscriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleDiscriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid()  # Output probability\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Test components\nlatent_dim = 100\ndata_dim = 784  # 28x28 images flattened\n\ngenerator = SimpleGenerator(latent_dim, data_dim)\ndiscriminator = SimpleDiscriminator(data_dim)\n\nprint(\"\\nGenerator:\")\nprint(f\"  Input: {latent_dim}-dim noise\")\nprint(f\"  Output: {data_dim}-dim data\")\nprint(f\"  Parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n\nprint(\"\\nDiscriminator:\")\nprint(f\"  Input: {data_dim}-dim data\")\nprint(f\"  Output: 1-dim probability\")\nprint(f\"  Parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n\n# Test forward pass\nz = torch.randn(4, latent_dim)  # Batch of 4 noise vectors\nfake_data = generator(z)\nprint(f\"\\nNoise shape: {z.shape}\")\nprint(f\"Generated data shape: {fake_data.shape}\")\nprint(f\"Generated data range: [{fake_data.min():.2f}, {fake_data.max():.2f}]\")\n\n# Discriminate real vs fake\nreal_data = torch.randn(4, data_dim)\nreal_pred = discriminator(real_data)\nfake_pred = discriminator(fake_data.detach())\n\nprint(f\"\\nReal data predictions: {real_pred.squeeze().tolist()}\")\nprint(f\"Fake data predictions: {fake_pred.squeeze().tolist()}\")\n\nprint(\"\\n\ud83d\udca1 Generator creates data, Discriminator judges it!\")\nprint(\"\\n\u2713 Exercise 1 complete\")\n</code></pre>"},{"location":"old/Week4_Day19/#exercise-2-gan-training-loop-60-min","title":"Exercise 2: GAN Training Loop (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 2: GAN TRAINING LOOP\")\nprint(\"=\"*70)\n\n# Load MNIST\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n])\n\nmnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ndataloader = DataLoader(mnist, batch_size=128, shuffle=True)\n\nprint(f\"MNIST dataset: {len(mnist)} images\")\nprint(f\"Batches: {len(dataloader)}\\n\")\n\n# Initialize models\nlatent_dim = 100\ngenerator = SimpleGenerator(latent_dim, 784)\ndiscriminator = SimpleDiscriminator(784)\n\n# Loss and optimizers\ncriterion = nn.BCELoss()\nlr = 0.0002\nbeta1 = 0.5\n\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\nprint(\"Training GAN on MNIST...\")\nprint(\"(This may take a few minutes)\\n\")\n\nepochs = 10\ng_losses = []\nd_losses = []\nreal_scores = []\nfake_scores = []\n\nfor epoch in range(epochs):\n    epoch_g_loss = 0\n    epoch_d_loss = 0\n    epoch_real_score = 0\n    epoch_fake_score = 0\n\n    for i, (real_images, _) in enumerate(dataloader):\n        batch_size = real_images.size(0)\n        real_images = real_images.view(batch_size, -1)\n\n        # Labels\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        # ---------------------\n        # Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n\n        # Real images\n        real_output = discriminator(real_images)\n        d_real_loss = criterion(real_output, real_labels)\n\n        # Fake images\n        z = torch.randn(batch_size, latent_dim)\n        fake_images = generator(z).detach()\n        fake_output = discriminator(fake_images)\n        d_fake_loss = criterion(fake_output, fake_labels)\n\n        # Total discriminator loss\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        # ---------------------\n        # Train Generator\n        # ---------------------\n        optimizer_G.zero_grad()\n\n        # Generate fake images\n        z = torch.randn(batch_size, latent_dim)\n        fake_images = generator(z)\n\n        # Try to fool discriminator\n        fake_output = discriminator(fake_images)\n        g_loss = criterion(fake_output, real_labels)  # Want D to think they're real\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Track statistics\n        epoch_g_loss += g_loss.item()\n        epoch_d_loss += d_loss.item()\n        epoch_real_score += real_output.mean().item()\n        epoch_fake_score += fake_output.mean().item()\n\n    # Average for epoch\n    avg_g_loss = epoch_g_loss / len(dataloader)\n    avg_d_loss = epoch_d_loss / len(dataloader)\n    avg_real_score = epoch_real_score / len(dataloader)\n    avg_fake_score = epoch_fake_score / len(dataloader)\n\n    g_losses.append(avg_g_loss)\n    d_losses.append(avg_d_loss)\n    real_scores.append(avg_real_score)\n    fake_scores.append(avg_fake_score)\n\n    print(f\"Epoch [{epoch+1}/{epochs}] \"\n          f\"D_loss: {avg_d_loss:.4f} G_loss: {avg_g_loss:.4f} \"\n          f\"D(x): {avg_real_score:.4f} D(G(z)): {avg_fake_score:.4f}\")\n\nprint(\"\\n\u2713 Exercise 2 complete\")\n</code></pre>"},{"location":"old/Week4_Day19/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week4_Day19/#hands-on-coding-part-2-35-hours","title":"Hands-on Coding - Part 2 (3.5 hours)","text":""},{"location":"old/Week4_Day19/#exercise-3-visualize-generated-images-40-min","title":"Exercise 3: Visualize Generated Images (40 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 3: VISUALIZING GENERATED IMAGES\")\nprint(\"=\"*70)\n\n# Generate samples\ngenerator.eval()\nwith torch.no_grad():\n    z = torch.randn(64, latent_dim)\n    fake_images = generator(z)\n    fake_images = fake_images.view(-1, 1, 28, 28)\n    fake_images = (fake_images + 1) / 2  # Scale to [0, 1]\n\n# Plot generated images\nfig, axes = plt.subplots(8, 8, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(64):\n    axes[idx].imshow(fake_images[idx].squeeze(), cmap='gray')\n    axes[idx].axis('off')\n\nplt.suptitle(f'Generated MNIST Digits (After {epochs} Epochs)', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Plot training curves\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Losses\naxes[0].plot(g_losses, label='Generator', linewidth=2)\naxes[0].plot(d_losses, label='Discriminator', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('GAN Training Losses')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Scores\naxes[1].plot(real_scores, label='D(x) - Real', linewidth=2)\naxes[1].plot(fake_scores, label='D(G(z)) - Fake', linewidth=2)\naxes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Target')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Discriminator Output')\naxes[1].set_title('Discriminator Predictions')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Combined\naxes[2].plot(np.array(real_scores) - np.array(fake_scores), \n            label='D(x) - D(G(z))', linewidth=2, color='purple')\naxes[2].axhline(y=0, color='r', linestyle='--', alpha=0.5)\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Score Difference')\naxes[2].set_title('Real vs Fake Gap')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Observations:\")\nprint(\"- D(x) should stay around 0.5-0.7 (recognizes real as real)\")\nprint(\"- D(G(z)) should approach 0.5 (fakes become realistic)\")\nprint(\"- If gap is too large, discriminator is winning\")\nprint(\"- If gap is too small, generator might be winning (mode collapse)\")\n\nprint(\"\\n\u2713 Exercise 3 complete\")\n</code></pre>"},{"location":"old/Week4_Day19/#exercise-4-dcgan-implementation-80-min","title":"Exercise 4: DCGAN Implementation (80 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"EXERCISE 4: DEEP CONVOLUTIONAL GAN (DCGAN)\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nDCGAN Guidelines (from paper):\n1. Replace pooling with strided convolutions (D) and fractional-strided convolutions (G)\n2. Use BatchNorm in both G and D\n3. Remove fully connected hidden layers\n4. Use ReLU in G (except output uses Tanh)\n5. Use LeakyReLU in D\n\"\"\")\n\nclass DCGANGenerator(nn.Module):\n    def __init__(self, latent_dim, channels=1):\n        super(DCGANGenerator, self).__init__()\n\n        self.init_size = 7\n        self.l1 = nn.Sequential(\n            nn.Linear(latent_dim, 128 * self.init_size ** 2)\n        )\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),  # 7 \u2192 14\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),  # 14 \u2192 28\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self, channels=1):\n        super(DCGANDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            block.append(nn.Dropout2d(0.25))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 16, bn=False),  # 28 \u2192 14\n            *discriminator_block(16, 32),  # 14 \u2192 7\n            *discriminator_block(32, 64),  # 7 \u2192 3\n            *discriminator_block(64, 128),  # 3 \u2192 1\n        )\n\n        self.adv_layer = nn.Sequential(\n            nn.Linear(128 * 1 * 1, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        return validity\n\n# Create DCGAN\nlatent_dim = 100\ndcgan_generator = DCGANGenerator(latent_dim, channels=1)\ndcgan_discriminator = DCGANDiscriminator(channels=1)\n\nprint(f\"\\nDCGAN Generator:\")\nprint(f\"  Parameters: {sum(p.numel() for p in dcgan_generator.parameters()):,}\")\n\nprint(f\"\\nDCGAN Discriminator:\")\nprint(f\"  Parameters: {sum(p.numel() for p in dcgan_discriminator.parameters()):,}\")\n\n# Train DCGAN (fewer epochs for demo)\noptimizer_G = optim.Adam(dcgan_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(dcgan_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\nprint(\"\\nTraining DCGAN...\\n\")\nepochs = 5\n\nfor epoch in range(epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        batch_size = imgs.size(0)\n\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n\n        real_output = dcgan_discriminator(imgs)\n        d_real_loss = criterion(real_output, real_labels)\n\n        z = torch.randn(batch_size, latent_dim)\n        fake_imgs = dcgan_generator(z).detach()\n        fake_output = dcgan_discriminator(fake_imgs)\n        d_fake_loss = criterion(fake_output, fake_labels)\n\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n\n        z = torch.randn(batch_size, latent_dim)\n        fake_imgs = dcgan_generator(z)\n        fake_output = dcgan_discriminator(fake_imgs)\n        g_loss = criterion(fake_output, real_labels)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n    print(f\"Epoch [{epoch+1}/{epochs}] \"\n          f\"D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")\n\n# Generate samples with DCGAN\ndcgan_generator.eval()\nwith torch.no_grad():\n    z = torch.randn(64, latent_dim)\n    fake_images = dcgan_generator(z)\n    fake_images = (fake_images + 1) / 2  # Scale to [0, 1]\n\n# Plot\nfig, axes = plt.subplots(8, 8, figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx in range(64):\n    axes[idx].imshow(fake_images[idx].squeeze(), cmap='gray')\n    axes[idx].axis('off')\n\nplt.suptitle(f'DCGAN Generated Digits (After {epochs} Epochs)',\n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 DCGAN produces sharper images than simple GAN!\")\nprint(\"\\n\u2713 Exercise 4 complete\")\n</code></pre>"},{"location":"old/Week4_Day19/#mini-challenge-gan-evaluation-mode-collapse-50-min","title":"Mini-Challenge: GAN Evaluation &amp; Mode Collapse (50 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"MINI-CHALLENGE: GAN EVALUATION\")\nprint(\"=\"*70)\n\n# Evaluate diversity of generated samples\ndef evaluate_diversity(generator, latent_dim, n_samples=1000):\n    \"\"\"Check if generator produces diverse outputs\"\"\"\n    generator.eval()\n\n    generated = []\n    with torch.no_grad():\n        for _ in range(n_samples // 64):\n            z = torch.randn(64, latent_dim)\n            imgs = generator(z)\n            generated.append(imgs)\n\n    generated = torch.cat(generated, dim=0)\n\n    # Compute pairwise distances\n    generated_flat = generated.view(generated.size(0), -1)\n\n    # Random sample for efficiency\n    sample_size = 100\n    indices = np.random.choice(len(generated_flat), sample_size, replace=False)\n    sample = generated_flat[indices]\n\n    distances = []\n    for i in range(len(sample)):\n        for j in range(i+1, len(sample)):\n            dist = torch.norm(sample[i] - sample[j]).item()\n            distances.append(dist)\n\n    return np.mean(distances), np.std(distances)\n\n# Evaluate both generators\nmean_dist_simple, std_dist_simple = evaluate_diversity(generator, latent_dim)\nmean_dist_dcgan, std_dist_dcgan = evaluate_diversity(dcgan_generator, latent_dim)\n\nprint(f\"\\nDiversity Metrics:\")\nprint(f\"Simple GAN:  Mean distance = {mean_dist_simple:.4f}, Std = {std_dist_simple:.4f}\")\nprint(f\"DCGAN:       Mean distance = {mean_dist_dcgan:.4f}, Std = {std_dist_dcgan:.4f}\")\n\n# Check digit distribution\ndef check_digit_distribution(generator, latent_dim, n_samples=100):\n    \"\"\"Visually check if all digits are represented\"\"\"\n    generator.eval()\n\n    with torch.no_grad():\n        z = torch.randn(n_samples, latent_dim)\n        imgs = generator(z)\n        imgs = (imgs + 1) / 2\n\n    return imgs\n\nsimple_samples = check_digit_distribution(generator, latent_dim, 100)\ndcgan_samples = check_digit_distribution(dcgan_generator, latent_dim, 100)\n\n# Plot comparison\nfig, axes = plt.subplots(2, 10, figsize=(15, 4))\n\nfor i in range(10):\n    axes[0, i].imshow(simple_samples[i].squeeze(), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel('Simple GAN', rotation=0, ha='right', va='center')\n\n    axes[1, i].imshow(dcgan_samples[i].squeeze(), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel('DCGAN', rotation=0, ha='right', va='center')\n\nplt.suptitle('Generated Digit Diversity', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Key GAN Challenges:\")\nprint(\"1. Mode Collapse: Generator produces limited variety\")\nprint(\"2. Training Instability: Losses oscillate\")\nprint(\"3. Evaluation: Hard to quantify quality objectively\")\nprint(\"\\n\ud83d\udca1 Solutions:\")\nprint(\"- Use proven architectures (DCGAN)\")\nprint(\"- Careful hyperparameter tuning\")\nprint(\"- Label smoothing, noise injection\")\nprint(\"- Wasserstein GAN loss (more stable)\")\n\nprint(\"\\n\u2713 Mini-challenge complete\")\n</code></pre>"},{"location":"old/Week4_Day19/#reflection-consolidation-30-min","title":"Reflection &amp; Consolidation (30 min)","text":"<p>\u2610 Review GAN architecture and training \u2610 Understand adversarial dynamics \u2610 Note common failure modes \u2610 Write daily reflection</p>"},{"location":"old/Week4_Day19/#daily-reflection-prompts-choose-2-3","title":"Daily Reflection Prompts (Choose 2-3):","text":"<ul> <li>What was the most important concept you learned today?</li> <li>How do Generator and Discriminator interact?</li> <li>Why is GAN training unstable?</li> <li>What is mode collapse and how to detect it?</li> <li>How might you apply GANs to your domain?</li> <li>What's the difference between simple GAN and DCGAN?</li> </ul> <p>Next: Day 20 - Sentiment Analysis Project</p>"},{"location":"old/Week4_Day20/","title":"Week 4, Day 20: Sentiment Analysis Project - Movie Reviews","text":""},{"location":"old/Week4_Day20/#daily-goals","title":"Daily Goals","text":"<ul> <li>Complete end-to-end sentiment analysis project</li> <li>Apply RNN/LSTM, embeddings, and attention concepts</li> <li>Achieve &gt;85% accuracy on movie review classification</li> <li>Build complete NLP pipeline from preprocessing to deployment</li> <li>Create portfolio-ready project with professional documentation</li> </ul>"},{"location":"old/Week4_Day20/#morning-session-4-hours","title":"Morning Session (4 hours)","text":""},{"location":"old/Week4_Day20/#optional-daily-check-in-with-peers-on-teams-15-min","title":"Optional: Daily Check-in with Peers on Teams (15 min)","text":""},{"location":"old/Week4_Day20/#video-learning-30-min","title":"Video Learning (30 min)","text":"<p>\u2610 Watch: Sentiment Analysis Walkthrough (15 min)</p> <p>\u2610 Optional Review: Any Week 4 videos as needed (15 min)</p>"},{"location":"old/Week4_Day20/#project-briefing-30-min","title":"Project Briefing (30 min)","text":"<pre><code>\"\"\"\nSENTIMENT ANALYSIS PROJECT\n\nDataset: IMDB Movie Reviews\n- 50,000 reviews (25,000 train, 25,000 test)\n- Binary classification: Positive (1) or Negative (0)\n- Real text data with variety and complexity\n\nGoal: Build sentiment classifier achieving &gt;85% accuracy\n\nProject Structure:\nPhase 1: Data Loading &amp; Exploration (30 min)\nPhase 2: Text Preprocessing Pipeline (45 min)\nPhase 3: Baseline Model (45 min)\nPhase 4: Advanced Model with Attention (60 min)\nPhase 5: Evaluation &amp; Documentation (45 min)\n\nSuccess Criteria:\n- Minimum: &gt;80% test accuracy\n- Target: &gt;85% test accuracy\n- Stretch: &gt;88% test accuracy\n- Clean, documented code\n- Professional visualizations\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"=\"*70)\nprint(\"SENTIMENT ANALYSIS PROJECT\")\nprint(\"Week 4, Day 20 Capstone\")\nprint(\"=\"*70)\n</code></pre>"},{"location":"old/Week4_Day20/#phase-1-data-loading-exploration-30-min","title":"Phase 1: Data Loading &amp; Exploration (30 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: DATA LOADING &amp; EXPLORATION\")\nprint(\"=\"*70)\n\n# For this exercise, we'll use a subset of IMDB-like reviews\n# In practice, use datasets.IMDB from torchtext or download from Kaggle\n\n# Simulated movie reviews\npositive_reviews = [\n    \"This movie was absolutely fantastic! I loved every minute of it.\",\n    \"Brilliant performance by the cast. Highly recommend watching it.\",\n    \"One of the best films I've seen this year. Amazing storytelling.\",\n    \"Incredible cinematography and a compelling plot. Must watch!\",\n    \"Outstanding direction and great character development throughout.\",\n] * 100  # Duplicate for larger dataset\n\nnegative_reviews = [\n    \"Terrible movie. Complete waste of time and money.\",\n    \"The worst film I've ever seen. Poor acting and boring plot.\",\n    \"Disappointing on every level. Would not recommend to anyone.\",\n    \"Awful screenplay and terrible direction. Avoid at all costs.\",\n    \"Boring and predictable. I fell asleep halfway through.\",\n] * 100\n\n# Combine\nall_reviews = positive_reviews + negative_reviews\nall_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n\nprint(f\"Total reviews: {len(all_reviews)}\")\nprint(f\"Positive: {sum(all_labels)}\")\nprint(f\"Negative: {len(all_labels) - sum(all_labels)}\\n\")\n\n# Shuffle\nindices = np.random.permutation(len(all_reviews))\nall_reviews = [all_reviews[i] for i in indices]\nall_labels = [all_labels[i] for i in indices]\n\n# Split\ntrain_size = int(0.8 * len(all_reviews))\ntrain_reviews = all_reviews[:train_size]\ntrain_labels = all_labels[:train_size]\ntest_reviews = all_reviews[train_size:]\ntest_labels = all_labels[train_size:]\n\nprint(f\"Training set: {len(train_reviews)}\")\nprint(f\"Test set: {len(test_reviews)}\\n\")\n\n# Explore samples\nprint(\"Sample reviews:\")\nfor i in range(3):\n    sentiment = \"Positive\" if train_labels[i] == 1 else \"Negative\"\n    print(f\"\\n{sentiment}: '{train_reviews[i]}'\")\n\n# Analyze review lengths\ntrain_lengths = [len(review.split()) for review in train_reviews]\n\nplt.figure(figsize=(10, 5))\nplt.hist(train_lengths, bins=50, edgecolor='black')\nplt.xlabel('Review Length (words)')\nplt.ylabel('Count')\nplt.title('Distribution of Review Lengths')\nplt.axvline(np.mean(train_lengths), color='r', linestyle='--', \n           label=f'Mean: {np.mean(train_lengths):.1f}')\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.show()\n\nprint(f\"\\nLength statistics:\")\nprint(f\"  Mean: {np.mean(train_lengths):.1f}\")\nprint(f\"  Median: {np.median(train_lengths):.1f}\")\nprint(f\"  Max: {max(train_lengths)}\")\n\nprint(\"\\n\u2713 Phase 1 complete\")\n</code></pre>"},{"location":"old/Week4_Day20/#phase-2-text-preprocessing-pipeline-45-min","title":"Phase 2: Text Preprocessing Pipeline (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: TEXT PREPROCESSING\")\nprint(\"=\"*70)\n\nclass TextPreprocessor:\n    def __init__(self, max_vocab_size=10000, max_len=100):\n        self.max_vocab_size = max_vocab_size\n        self.max_len = max_len\n        self.vocab = None\n        self.word_to_idx = None\n        self.idx_to_word = None\n\n    def clean_text(self, text):\n        \"\"\"Clean and normalize text\"\"\"\n        # Lowercase\n        text = text.lower()\n        # Remove special characters but keep important punctuation\n        text = re.sub(r'[^a-z\\s!?.,]', '', text)\n        return text\n\n    def tokenize(self, text):\n        \"\"\"Split text into tokens\"\"\"\n        return text.split()\n\n    def build_vocab(self, texts):\n        \"\"\"Build vocabulary from texts\"\"\"\n        # Tokenize all texts\n        all_tokens = []\n        for text in texts:\n            tokens = self.tokenize(self.clean_text(text))\n            all_tokens.extend(tokens)\n\n        # Count frequencies\n        word_freq = Counter(all_tokens)\n\n        # Keep most common words\n        most_common = word_freq.most_common(self.max_vocab_size - 2)\n\n        # Build vocab\n        self.vocab = ['&lt;PAD&gt;', '&lt;UNK&gt;'] + [word for word, _ in most_common]\n        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n\n        print(f\"Vocabulary built: {len(self.vocab)} words\")\n        return self.vocab\n\n    def text_to_sequence(self, text):\n        \"\"\"Convert text to sequence of indices\"\"\"\n        tokens = self.tokenize(self.clean_text(text))\n        sequence = [self.word_to_idx.get(token, self.word_to_idx['&lt;UNK&gt;']) \n                   for token in tokens]\n        return sequence\n\n    def pad_sequence(self, sequence):\n        \"\"\"Pad or truncate sequence to fixed length\"\"\"\n        if len(sequence) &lt; self.max_len:\n            sequence = sequence + [self.word_to_idx['&lt;PAD&gt;']] * (self.max_len - len(sequence))\n        else:\n            sequence = sequence[:self.max_len]\n        return sequence\n\n    def preprocess(self, texts):\n        \"\"\"Complete preprocessing pipeline\"\"\"\n        sequences = [self.text_to_sequence(text) for text in texts]\n        padded = [self.pad_sequence(seq) for seq in sequences]\n        return np.array(padded)\n\n# Create preprocessor\npreprocessor = TextPreprocessor(max_vocab_size=5000, max_len=50)\n\n# Build vocabulary\nvocab = preprocessor.build_vocab(train_reviews)\n\nprint(f\"\\nVocabulary sample: {vocab[:20]}\")\n\n# Preprocess data\nX_train = preprocessor.preprocess(train_reviews)\nX_test = preprocessor.preprocess(test_reviews)\n\ny_train = np.array(train_labels)\ny_test = np.array(test_labels)\n\nprint(f\"\\nPreprocessed shapes:\")\nprint(f\"  X_train: {X_train.shape}\")\nprint(f\"  y_train: {y_train.shape}\")\nprint(f\"  X_test: {X_test.shape}\")\nprint(f\"  y_test: {y_test.shape}\")\n\n# Visualize a preprocessed example\nexample_idx = 0\nexample_text = train_reviews[example_idx]\nexample_sequence = X_train[example_idx]\n\nprint(f\"\\nExample preprocessing:\")\nprint(f\"Original: '{example_text}'\")\nprint(f\"Tokens: {[preprocessor.idx_to_word[idx] for idx in example_sequence[:20]]}\")\nprint(f\"Indices: {example_sequence[:20]}\")\n\nprint(\"\\n\u2713 Phase 2 complete\")\n</code></pre>"},{"location":"old/Week4_Day20/#afternoon-session-4-hours","title":"Afternoon Session (4 hours)","text":""},{"location":"old/Week4_Day20/#phase-3-baseline-model-45-min","title":"Phase 3: Baseline Model (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: BASELINE LSTM MODEL\")\nprint(\"=\"*70)\n\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super(SentimentLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n                           batch_first=True, dropout=dropout if n_layers &gt; 1 else 0)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.dropout(self.embedding(x))\n\n        # LSTM (use final hidden state)\n        _, (hidden, _) = self.lstm(embedded)\n\n        # Use last layer's hidden state\n        output = self.fc(self.dropout(hidden[-1]))\n        return self.sigmoid(output)\n\n# Create model\nvocab_size = len(preprocessor.vocab)\nembedding_dim = 128\nhidden_dim = 256\n\nbaseline_model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim)\n\nprint(f\"Baseline Model:\")\nprint(baseline_model)\nprint(f\"\\nParameters: {sum(p.numel() for p in baseline_model.parameters()):,}\")\n\n# Prepare data loaders\ntrain_dataset = torch.utils.data.TensorDataset(\n    torch.LongTensor(X_train),\n    torch.FloatTensor(y_train)\n)\ntest_dataset = torch.utils.data.TensorDataset(\n    torch.LongTensor(X_test),\n    torch.FloatTensor(y_test)\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Training function\ndef train_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n    best_test_acc = 0\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for inputs, labels in train_loader:\n            labels = labels.unsqueeze(1)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            predictions = (outputs &gt; 0.5).float()\n            train_correct += (predictions == labels).sum().item()\n            train_total += labels.size(0)\n\n        # Test\n        model.eval()\n        test_correct = 0\n        test_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                labels = labels.unsqueeze(1)\n                outputs = model(inputs)\n                predictions = (outputs &gt; 0.5).float()\n                test_correct += (predictions == labels).sum().item()\n                test_total += labels.size(0)\n\n        train_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        test_acc = test_correct / test_total\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n\n        if test_acc &gt; best_test_acc:\n            best_test_acc = test_acc\n            torch.save(model.state_dict(), 'best_sentiment_model.pth')\n\n        print(f\"Epoch {epoch+1}/{epochs}: \"\n              f\"Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n    return history, best_test_acc\n\nprint(\"\\nTraining baseline model...\")\nbaseline_history, baseline_best_acc = train_model(baseline_model, train_loader, test_loader, epochs=10)\n\nprint(f\"\\n\ud83c\udfaf Baseline Best Test Accuracy: {baseline_best_acc:.4f}\")\n\nprint(\"\\n\u2713 Phase 3 complete\")\n</code></pre>"},{"location":"old/Week4_Day20/#phase-4-advanced-model-with-attention-60-min","title":"Phase 4: Advanced Model with Attention (60 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: LSTM WITH ATTENTION\")\nprint(\"=\"*70)\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, lstm_output):\n        # lstm_output: (batch, seq_len, hidden_dim)\n\n        # Compute attention scores\n        attention_scores = self.attention(lstm_output)  # (batch, seq_len, 1)\n        attention_weights = torch.softmax(attention_scores, dim=1)\n\n        # Apply attention\n        attended = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_dim)\n\n        return attended, attention_weights\n\nclass SentimentLSTMWithAttention(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super(SentimentLSTMWithAttention, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n                           batch_first=True, dropout=dropout if n_layers &gt; 1 else 0,\n                           bidirectional=False)\n        self.attention = AttentionLayer(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Embed\n        embedded = self.dropout(self.embedding(x))\n\n        # LSTM\n        lstm_out, _ = self.lstm(embedded)\n\n        # Attention\n        attended, attention_weights = self.attention(lstm_out)\n\n        # Classify\n        output = self.fc(self.dropout(attended))\n        return self.sigmoid(output), attention_weights\n\n# Create attention model\nattention_model = SentimentLSTMWithAttention(vocab_size, embedding_dim, hidden_dim)\n\nprint(f\"Attention Model:\")\nprint(attention_model)\nprint(f\"\\nParameters: {sum(p.numel() for p in attention_model.parameters()):,}\")\n\n# Modified training for attention model\ndef train_attention_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n    best_test_acc = 0\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for inputs, labels in train_loader:\n            labels = labels.unsqueeze(1)\n\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            predictions = (outputs &gt; 0.5).float()\n            train_correct += (predictions == labels).sum().item()\n            train_total += labels.size(0)\n\n        # Test\n        model.eval()\n        test_correct = 0\n        test_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                labels = labels.unsqueeze(1)\n                outputs, _ = model(inputs)\n                predictions = (outputs &gt; 0.5).float()\n                test_correct += (predictions == labels).sum().item()\n                test_total += labels.size(0)\n\n        train_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        test_acc = test_correct / test_total\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n\n        if test_acc &gt; best_test_acc:\n            best_test_acc = test_acc\n            torch.save(model.state_dict(), 'best_attention_model.pth')\n\n        print(f\"Epoch {epoch+1}/{epochs}: \"\n              f\"Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n    return history, best_test_acc\n\nprint(\"\\nTraining attention model...\")\nattention_history, attention_best_acc = train_attention_model(attention_model, train_loader, test_loader, epochs=10)\n\nprint(f\"\\n\ud83c\udfaf Attention Model Best Test Accuracy: {attention_best_acc:.4f}\")\nprint(f\"Improvement over baseline: +{(attention_best_acc - baseline_best_acc):.4f}\")\n\nprint(\"\\n\u2713 Phase 4 complete\")\n</code></pre>"},{"location":"old/Week4_Day20/#phase-5-evaluation-documentation-45-min","title":"Phase 5: Evaluation &amp; Documentation (45 min)","text":"<pre><code>print(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: COMPREHENSIVE EVALUATION\")\nprint(\"=\"*70)\n\n# Load best models\nbaseline_model.load_state_dict(torch.load('best_sentiment_model.pth'))\nattention_model.load_state_dict(torch.load('best_attention_model.pth'))\n\n# Compare training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(baseline_history['train_acc'])+1)\n\n# Accuracy comparison\naxes[0].plot(epochs_range, baseline_history['test_acc'], label='Baseline', marker='o')\naxes[0].plot(epochs_range, attention_history['test_acc'], label='With Attention', marker='s')\naxes[0].axhline(y=0.85, color='r', linestyle='--', alpha=0.5, label='Target (85%)')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Test Accuracy')\naxes[0].set_title('Test Accuracy Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Loss comparison\naxes[1].plot(epochs_range, baseline_history['train_loss'], label='Baseline')\naxes[1].plot(epochs_range, attention_history['train_loss'], label='With Attention')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Training Loss')\naxes[1].set_title('Training Loss Comparison')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix for best model\nattention_model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs, _ = attention_model(inputs)\n        predictions = (outputs &gt; 0.5).squeeze().long()\n        all_preds.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n           xticklabels=['Negative', 'Positive'],\n           yticklabels=['Negative', 'Positive'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (Accuracy: {attention_best_acc:.4f})')\nplt.show()\n\n# Visualize attention weights\ndef visualize_attention(model, text, preprocessor):\n    \"\"\"Visualize what the model pays attention to\"\"\"\n    model.eval()\n\n    # Preprocess\n    sequence = preprocessor.text_to_sequence(text)\n    padded = preprocessor.pad_sequence(sequence)\n    input_tensor = torch.LongTensor([padded])\n\n    # Get prediction and attention\n    with torch.no_grad():\n        output, attention_weights = model(input_tensor)\n\n    prediction = \"Positive\" if output.item() &gt; 0.5 else \"Negative\"\n    confidence = output.item() if output.item() &gt; 0.5 else 1 - output.item()\n\n    # Get words (remove padding)\n    words = [preprocessor.idx_to_word[idx] for idx in padded if idx != 0]\n    attention = attention_weights[0, :len(words), 0].cpu().numpy()\n\n    return prediction, confidence, words, attention\n\n# Test on sample reviews\ntest_samples = [\n    \"This movie was absolutely fantastic and amazing!\",\n    \"Terrible film, complete waste of time and money.\",\n    \"Great acting but the plot was somewhat boring.\"\n]\n\nprint(\"\\nAttention Visualization:\")\nfor sample_text in test_samples:\n    pred, conf, words, attn = visualize_attention(attention_model, sample_text, preprocessor)\n\n    print(f\"\\nText: '{sample_text}'\")\n    print(f\"Prediction: {pred} (confidence: {conf:.4f})\")\n    print(\"Attention weights:\")\n\n    # Show top attended words\n    word_attention = list(zip(words, attn))\n    word_attention.sort(key=lambda x: x[1], reverse=True)\n\n    for word, weight in word_attention[:5]:\n        bar = \"\u2588\" * int(weight * 50)\n        print(f\"  {word:15s}: {weight:.4f} {bar}\")\n\n# Final report\nreport = f\"\"\"\n{'='*70}\nSENTIMENT ANALYSIS PROJECT - FINAL REPORT\n{'='*70}\n\nDATASET\n-------\nTraining samples: {len(train_reviews)}\nTest samples: {len(test_reviews)}\nVocabulary size: {len(preprocessor.vocab)}\nMax sequence length: {preprocessor.max_len}\n\nMODELS EVALUATED\n----------------\n1. Baseline LSTM\n   - Architecture: Embedding \u2192 LSTM (2 layers) \u2192 FC\n   - Parameters: {sum(p.numel() for p in baseline_model.parameters()):,}\n   - Best Test Accuracy: {baseline_best_acc:.4f}\n\n2. LSTM with Attention\n   - Architecture: Embedding \u2192 LSTM \u2192 Attention \u2192 FC\n   - Parameters: {sum(p.numel() for p in attention_model.parameters()):,}\n   - Best Test Accuracy: {attention_best_acc:.4f}\n   - Improvement: +{(attention_best_acc - baseline_best_acc):.4f}\n\nBEST MODEL: LSTM with Attention\nStatus: {'\u2713 TARGET ACHIEVED (&gt;85%)' if attention_best_acc &gt; 0.85 else '\u2717 Below target'}\n\nKEY INSIGHTS\n------------\n1. Attention mechanism improves accuracy\n2. Model focuses on sentiment-heavy words\n3. Bidirectional LSTM could improve further\n4. Pretrained embeddings (GloVe) could help\n\nTECHNIQUES APPLIED\n------------------\n\u2713 Text preprocessing pipeline\n\u2713 Custom vocabulary building\n\u2713 LSTM for sequential modeling\n\u2713 Attention mechanism\n\u2713 Proper train/test splitting\n\u2713 Model checkpointing\n\n{'='*70}\nWeek 4 Complete! \ud83c\udf89\n{'='*70}\n\"\"\"\n\nprint(report)\n\n# Save report\nwith open('sentiment_analysis_report.txt', 'w') as f:\n    f.write(report)\n\nprint(\"\\n\ud83d\udcc4 Report saved: sentiment_analysis_report.txt\")\nprint(\"\ud83d\udcbe Models saved: best_sentiment_model.pth, best_attention_model.pth\")\n\nprint(\"\\n\u2713 Project complete!\")\n</code></pre>"},{"location":"old/Week4_Day20/#reflection-week-review-30-min","title":"Reflection &amp; Week Review (30 min)","text":"<p>\u2610 Review entire Week 4 journey \u2610 Document key learnings \u2610 Celebrate achievements</p>"},{"location":"old/Week4_Day20/#week-4-reflection-prompts-address-all","title":"Week 4 Reflection Prompts (Address All):","text":"<ul> <li>What was the most valuable thing you learned this week?</li> <li>How do RNNs differ from CNNs in what they model?</li> <li>What is the significance of attention mechanisms?</li> <li>How do GANs enable generative modeling?</li> <li>What connections do you see to modern LLMs?</li> <li>How confident do you feel about NLP now?</li> <li>What would you explore further in advanced ML?</li> </ul>"},{"location":"old/Week4_Day20/#week-4-achievement-checklist","title":"Week 4 Achievement Checklist:","text":"<p>\u2610 Understood RNNs and LSTMs \u2610 Implemented attention mechanism \u2610 Grasped Transformer architecture conceptually \u2610 Processed text data for NLP \u2610 Built and trained GANs \u2610 Completed sentiment analysis project &gt;85% accuracy \u2610 Created professional documentation</p>"},{"location":"old/Week4_Day20/#week-4-complete","title":"\ud83c\udf89 Week 4 Complete!","text":"<p>Achievements Unlocked: - \u2705 Sequential modeling with RNNs/LSTMs - \u2705 Attention mechanisms understood - \u2705 Transformer architecture grasped - \u2705 NLP pipeline mastery - \u2705 Generative modeling with GANs - \u2705 Portfolio sentiment analysis project</p> <p>What You Can Now Do: - Build sequence models for text and time series - Understand modern language models architecturally - Process text data professionally - Generate images with GANs - Deploy sentiment classifiers - Understand the foundations of ChatGPT/GPT-4</p> <p>Congratulations on completing Week 4! \ud83d\ude80</p> <p>You've mastered advanced deep learning topics and built impressive projects!</p> <p>Next: Week 5 Overview - Model Deployment &amp; Best Practices</p>"}]}